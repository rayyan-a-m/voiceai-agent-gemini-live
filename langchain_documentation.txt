Core components
Agents

Copy page

Agents combine language models with tools to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.
create_agent provides a production-ready agent implementation.
An LLM Agent runs tools in a loop to achieve a goal. An agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.
action

observation

finish

input

model

tools

output

create_agent builds a graph-based agent runtime using LangGraph. A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.
Learn more about the Graph API.
‚Äã
Core components
‚Äã
Model
The model is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.
‚Äã
Static model
Static models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.
To initialize a static model from a model identifier string:
from langchain.agents import create_agent

agent = create_agent(
    "gpt-5",
    tools=tools
)
Model identifier strings support automatic inference (e.g., "gpt-5" will be inferred as "openai:gpt-5"). Refer to the reference to see a full list of model identifier string mappings.
For more control over the model configuration, initialize a model instance directly using the provider package. In this example, we use ChatOpenAI. See Chat models for other available chat model classes.
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI

model = ChatOpenAI(
    model="gpt-5",
    temperature=0.1,
    max_tokens=1000,
    timeout=30
    # ... (other params)
)
agent = create_agent(model, tools=tools)
Model instances give you complete control over configuration. Use them when you need to set specific parameters like temperature, max_tokens, timeouts, base_url, and other provider-specific settings. Refer to the reference to see available params and methods on your model.
‚Äã
Dynamic model
Dynamic models are selected at runtime based on the current state and context. This enables sophisticated routing logic and cost optimization.
To use a dynamic model, create middleware using the @wrap_model_call decorator that modifies the model in the request:
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse


basic_model = ChatOpenAI(model="gpt-4o-mini")
advanced_model = ChatOpenAI(model="gpt-4o")

@wrap_model_call
def dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:
    """Choose model based on conversation complexity."""
    message_count = len(request.state["messages"])

    if message_count > 10:
        # Use an advanced model for longer conversations
        model = advanced_model
    else:
        model = basic_model

    return handler(request.override(model=model))

agent = create_agent(
    model=basic_model,  # Default model
    tools=tools,
    middleware=[dynamic_model_selection]
)
Pre-bound models (models with bind_tools already called) are not supported when using structured output. If you need dynamic model selection with structured output, ensure the models passed to the middleware are not pre-bound.
For model configuration details, see Models. For dynamic model selection patterns, see Dynamic model in middleware.
‚Äã
Tools
Tools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:
Multiple tool calls in sequence (triggered by a single prompt)
Parallel tool calls when appropriate
Dynamic tool selection based on previous results
Tool retry logic and error handling
State persistence across tool calls
For more information, see Tools.
‚Äã
Defining tools
Pass a list of tools to the agent.
Tools can be specified as plain Python functions or coroutines.
The tool decorator can be used to customize tool names, descriptions, argument schemas, and other properties.
from langchain.tools import tool
from langchain.agents import create_agent


@tool
def search(query: str) -> str:
    """Search for information."""
    return f"Results for: {query}"

@tool
def get_weather(location: str) -> str:
    """Get weather information for a location."""
    return f"Weather in {location}: Sunny, 72¬∞F"

agent = create_agent(model, tools=[search, get_weather])
If an empty tool list is provided, the agent will consist of a single LLM node without tool-calling capabilities.
‚Äã
Tool error handling
To customize how tool errors are handled, use the @wrap_tool_call decorator to create middleware:
from langchain.agents import create_agent
from langchain.agents.middleware import wrap_tool_call
from langchain.messages import ToolMessage


@wrap_tool_call
def handle_tool_errors(request, handler):
    """Handle tool execution errors with custom messages."""
    try:
        return handler(request)
    except Exception as e:
        # Return a custom error message to the model
        return ToolMessage(
            content=f"Tool error: Please check your input and try again. ({str(e)})",
            tool_call_id=request.tool_call["id"]
        )

agent = create_agent(
    model="gpt-4o",
    tools=[search, get_weather],
    middleware=[handle_tool_errors]
)
The agent will return a ToolMessage with the custom error message when a tool fails:
[
    ...
    ToolMessage(
        content="Tool error: Please check your input and try again. (division by zero)",
        tool_call_id="..."
    ),
    ...
]
‚Äã
Tool use in the ReAct loop
Agents follow the ReAct (‚ÄúReasoning + Acting‚Äù) pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.
Example of ReAct loop

To learn more about tools, see Tools.
‚Äã
System prompt
You can shape how your agent approaches tasks by providing a prompt. The system_prompt parameter can be provided as a string:
agent = create_agent(
    model,
    tools,
    system_prompt="You are a helpful assistant. Be concise and accurate."
)
When no system_prompt is provided, the agent will infer its task from the messages directly.
The system_prompt parameter accepts either a str or a SystemMessage. Using a SystemMessage gives you more control over the prompt structure, which is useful for provider-specific features like Anthropic‚Äôs prompt caching:
from langchain.agents import create_agent
from langchain.messages import SystemMessage, HumanMessage

literary_agent = create_agent(
    model="anthropic:claude-sonnet-4-5",
    system_prompt=SystemMessage(
        content=[
            {
                "type": "text",
                "text": "You are an AI assistant tasked with analyzing literary works.",
            },
            {
                "type": "text",
                "text": "<the entire contents of 'Pride and Prejudice'>",
                "cache_control": {"type": "ephemeral"}
            }
        ]
    )
)

result = literary_agent.invoke(
    {"messages": [HumanMessage("Analyze the major themes in 'Pride and Prejudice'.")]}
)
The cache_control field with {"type": "ephemeral"} tells Anthropic to cache that content block, reducing latency and costs for repeated requests that use the same system prompt.
‚Äã
Dynamic system prompt
For more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use middleware.
The @dynamic_prompt decorator creates middleware that generates system prompts based on the model request:
from typing import TypedDict

from langchain.agents import create_agent
from langchain.agents.middleware import dynamic_prompt, ModelRequest


class Context(TypedDict):
    user_role: str

@dynamic_prompt
def user_role_prompt(request: ModelRequest) -> str:
    """Generate system prompt based on user role."""
    user_role = request.runtime.context.get("user_role", "user")
    base_prompt = "You are a helpful assistant."

    if user_role == "expert":
        return f"{base_prompt} Provide detailed technical responses."
    elif user_role == "beginner":
        return f"{base_prompt} Explain concepts simply and avoid jargon."

    return base_prompt

agent = create_agent(
    model="gpt-4o",
    tools=[web_search],
    middleware=[user_role_prompt],
    context_schema=Context
)

# The system prompt will be set dynamically based on context
result = agent.invoke(
    {"messages": [{"role": "user", "content": "Explain machine learning"}]},
    context={"user_role": "expert"}
)
For more details on message types and formatting, see Messages. For comprehensive middleware documentation, see Middleware.
‚Äã
Invocation
You can invoke an agent by passing an update to its State. All agents include a sequence of messages in their state; to invoke the agent, pass a new message:
result = agent.invoke(
    {"messages": [{"role": "user", "content": "What's the weather in San Francisco?"}]}
)
For streaming steps and / or tokens from the agent, refer to the streaming guide.
Otherwise, the agent follows the LangGraph Graph API and supports all associated methods, such as stream and invoke.
‚Äã
Advanced concepts
‚Äã
Structured output
In some situations, you may want the agent to return an output in a specific format. LangChain provides strategies for structured output via the response_format parameter.
‚Äã
ToolStrategy
ToolStrategy uses artificial tool calling to generate structured output. This works with any model that supports tool calling:
from pydantic import BaseModel
from langchain.agents import create_agent
from langchain.agents.structured_output import ToolStrategy


class ContactInfo(BaseModel):
    name: str
    email: str
    phone: str

agent = create_agent(
    model="gpt-4o-mini",
    tools=[search_tool],
    response_format=ToolStrategy(ContactInfo)
)

result = agent.invoke({
    "messages": [{"role": "user", "content": "Extract contact info from: John Doe, john@example.com, (555) 123-4567"}]
})

result["structured_response"]
# ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')
‚Äã
ProviderStrategy
ProviderStrategy uses the model provider‚Äôs native structured output generation. This is more reliable but only works with providers that support native structured output (e.g., OpenAI):
from langchain.agents.structured_output import ProviderStrategy

agent = create_agent(
    model="gpt-4o",
    response_format=ProviderStrategy(ContactInfo)
)
As of langchain 1.0, simply passing a schema (e.g., response_format=ContactInfo) is no longer supported. You must explicitly use ToolStrategy or ProviderStrategy.
To learn about structured output, see Structured output.
‚Äã
Memory
Agents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.
Information stored in the state can be thought of as the short-term memory of the agent:
Custom state schemas must extend AgentState as a TypedDict.
There are two ways to define custom state:
Via middleware (preferred)
Via state_schema on create_agent
‚Äã
Defining state via middleware
Use middleware to define custom state when your custom state needs to be accessed by specific middleware hooks and tools attached to said middleware.
from langchain.agents import AgentState
from langchain.agents.middleware import AgentMiddleware
from typing import Any


class CustomState(AgentState):
    user_preferences: dict

class CustomMiddleware(AgentMiddleware):
    state_schema = CustomState
    tools = [tool1, tool2]

    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:
        ...

agent = create_agent(
    model,
    tools=tools,
    middleware=[CustomMiddleware()]
)

# The agent can now track additional state beyond messages
result = agent.invoke({
    "messages": [{"role": "user", "content": "I prefer technical explanations"}],
    "user_preferences": {"style": "technical", "verbosity": "detailed"},
})
‚Äã
Defining state via state_schema
Use the state_schema parameter as a shortcut to define custom state that is only used in tools.
from langchain.agents import AgentState


class CustomState(AgentState):
    user_preferences: dict

agent = create_agent(
    model,
    tools=[tool1, tool2],
    state_schema=CustomState
)
# The agent can now track additional state beyond messages
result = agent.invoke({
    "messages": [{"role": "user", "content": "I prefer technical explanations"}],
    "user_preferences": {"style": "technical", "verbosity": "detailed"},
})
As of langchain 1.0, custom state schemas must be TypedDict types. Pydantic models and dataclasses are no longer supported. See the v1 migration guide for more details.
Defining custom state via middleware is preferred over defining it via state_schema on create_agent because it allows you to keep state extensions conceptually scoped to the relevant middleware and tools.
state_schema is still supported for backwards compatibility on create_agent.
To learn more about memory, see Memory. For information on implementing long-term memory that persists across sessions, see Long-term memory.
‚Äã
Streaming
We‚Äôve seen how the agent can be called with invoke to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.
for chunk in agent.stream({
    "messages": [{"role": "user", "content": "Search for AI news and summarize the findings"}]
}, stream_mode="values"):
    # Each chunk contains the full state at that point
    latest_message = chunk["messages"][-1]
    if latest_message.content:
        print(f"Agent: {latest_message.content}")
    elif latest_message.tool_calls:
        print(f"Calling tools: {[tc['name'] for tc in latest_message.tool_calls]}")
For more details on streaming, see Streaming.
‚Äã
Middleware
Middleware provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:
Process state before the model is called (e.g., message trimming, context injection)
Modify or validate the model‚Äôs response (e.g., guardrails, content filtering)
Handle tool execution errors with custom logic
Implement dynamic model selection based on state or context
Add custom logging, monitoring, or analytics
Middleware integrates seamlessly into the agent‚Äôs execution, allowing you to intercept and modify data flow at key points without changing the core agent logic.
For comprehensive middleware documentation including decorators like @before_model, @after_model, and @wrap_tool_call, see Middleware.
Edit the source of this page on GitHub.
Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.
Was this page helpful?


Yes

No
Ask a question...
....Core components
Models

Copy page

LLMs are powerful AI tools that can interpret and generate text like humans. They‚Äôre versatile enough to write content, translate languages, summarize, and answer questions without needing specialized training for each task.
In addition to text generation, many models support:
 Tool calling - calling external tools (like databases queries or API calls) and use results in their responses.
 Structured output - where the model‚Äôs response is constrained to follow a defined format.
 Multimodality - process and return data other than text, such as images, audio, and video.
 Reasoning - models perform multi-step reasoning to arrive at a conclusion.
Models are the reasoning engine of agents. They drive the agent‚Äôs decision-making process, determining which tools to call, how to interpret results, and when to provide a final answer.
The quality and capabilities of the model you choose directly impact your agent‚Äôs baseline reliability and performance. Different models excel at different tasks - some are better at following complex instructions, others at structured reasoning, and some support larger context windows for handling more information.
LangChain‚Äôs standard model interfaces give you access to many different provider integrations, which makes it easy to experiment with and switch between models to find the best fit for your use case.
For provider-specific integration information and capabilities, see the provider‚Äôs chat model page.
‚Äã
Basic usage
Models can be utilized in two ways:
With agents - Models can be dynamically specified when creating an agent.
Standalone - Models can be called directly (outside of the agent loop) for tasks like text generation, classification, or extraction without the need for an agent framework.
The same model interface works in both contexts, which gives you the flexibility to start simple and scale up to more complex agent-based workflows as needed.
‚Äã
Initialize a model
The easiest way to get started with a standalone model in LangChain is to use init_chat_model to initialize one from a chat model provider of your choice (examples below):
OpenAI
Anthropic
Azure
Google Gemini
AWS Bedrock
üëâ Read the OpenAI chat model integration docs
pip install -U "langchain[openai]"

init_chat_model

Model Class
import os
from langchain.chat_models import init_chat_model

os.environ["OPENAI_API_KEY"] = "sk-..."

model = init_chat_model("gpt-4.1")
response = model.invoke("Why do parrots talk?")
See init_chat_model for more detail, including information on how to pass model parameters.
‚Äã
Key methods
Invoke
The model takes messages as input and outputs messages after generating a complete response.
Stream
Invoke the model, but stream the output as it is generated in real-time.
Batch
Send multiple requests to a model in a batch for more efficient processing.
In addition to chat models, LangChain provides support for other adjacent technologies, such as embedding models and vector stores. See the integrations page for details.
‚Äã
Parameters
A chat model takes parameters that can be used to configure its behavior. The full set of supported parameters varies by model and provider, but standard ones include:
‚Äã
model
stringrequired
The name or identifier of the specific model you want to use with a provider. You can also specify both the model and its provider in a single argument using the ‚Äô:‚Äô format, for example, ‚Äòopenai:o1‚Äô.
‚Äã
api_key
string
The key required for authenticating with the model‚Äôs provider. This is usually issued when you sign up for access to the model. Often accessed by setting an environment variable.
‚Äã
temperature
number
Controls the randomness of the model‚Äôs output. A higher number makes responses more creative; lower ones make them more deterministic.
‚Äã
max_tokens
number
Limits the total number of tokens in the response, effectively controlling how long the output can be.
‚Äã
timeout
number
The maximum time (in seconds) to wait for a response from the model before canceling the request.
‚Äã
max_retries
number
The maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.
Using init_chat_model, pass these parameters as inline **kwargs:
Initialize using model parameters
model = init_chat_model(
    "claude-sonnet-4-5-20250929",
    # Kwargs passed to the model:
    temperature=0.7,
    timeout=30,
    max_tokens=1000,
)
Each chat model integration may have additional params used to control provider-specific functionality.
For example, ChatOpenAI has use_responses_api to dictate whether to use the OpenAI Responses or Completions API.
To find all the parameters supported by a given chat model, head to the chat model integrations page.
‚Äã
Invocation
A chat model must be invoked to generate an output. There are three primary invocation methods, each suited to different use cases.
‚Äã
Invoke
The most straightforward way to call a model is to use invoke() with a single message or a list of messages.
Single message
response = model.invoke("Why do parrots have colorful feathers?")
print(response)
A list of messages can be provided to a chat model to represent conversation history. Each message has a role that models use to indicate who sent the message in the conversation.
See the messages guide for more detail on roles, types, and content.
Dictionary format
conversation = [
    {"role": "system", "content": "You are a helpful assistant that translates English to French."},
    {"role": "user", "content": "Translate: I love programming."},
    {"role": "assistant", "content": "J'adore la programmation."},
    {"role": "user", "content": "Translate: I love building applications."}
]

response = model.invoke(conversation)
print(response)  # AIMessage("J'adore cr√©er des applications.")
Message objects
from langchain.messages import HumanMessage, AIMessage, SystemMessage

conversation = [
    SystemMessage("You are a helpful assistant that translates English to French."),
    HumanMessage("Translate: I love programming."),
    AIMessage("J'adore la programmation."),
    HumanMessage("Translate: I love building applications.")
]

response = model.invoke(conversation)
print(response)  # AIMessage("J'adore cr√©er des applications.")
If the return type of your invocation is a string, ensure that you are using a chat model as opposed to a LLM. Legacy, text-completion LLMs return strings directly. LangChain chat models are prefixed with ‚ÄúChat‚Äù, e.g., ChatOpenAI(/oss/integrations/chat/openai).
‚Äã
Stream
Most models can stream their output content while it is being generated. By displaying output progressively, streaming significantly improves user experience, particularly for longer responses.
Calling stream() returns an iterator that yields output chunks as they are produced. You can use a loop to process each chunk in real-time:

Basic text streaming

Stream tool calls, reasoning, and other content
for chunk in model.stream("Why do parrots have colorful feathers?"):
    print(chunk.text, end="|", flush=True)
As opposed to invoke(), which returns a single AIMessage after the model has finished generating its full response, stream() returns multiple AIMessageChunk objects, each containing a portion of the output text. Importantly, each chunk in a stream is designed to be gathered into a full message via summation:
Construct an AIMessage
full = None  # None | AIMessageChunk
for chunk in model.stream("What color is the sky?"):
    full = chunk if full is None else full + chunk
    print(full.text)

# The
# The sky
# The sky is
# The sky is typically
# The sky is typically blue
# ...

print(full.content_blocks)
# [{"type": "text", "text": "The sky is typically blue..."}]
The resulting message can be treated the same as a message that was generated with invoke() ‚Äì for example, it can be aggregated into a message history and passed back to the model as conversational context.
Streaming only works if all steps in the program know how to process a stream of chunks. For instance, an application that isn‚Äôt streaming-capable would be one that needs to store the entire output in memory before it can be processed.
Advanced streaming topics

‚Äã
Batch
Batching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel:
Batch
responses = model.batch([
    "Why do parrots have colorful feathers?",
    "How do airplanes fly?",
    "What is quantum computing?"
])
for response in responses:
    print(response)
This section describes a chat model method batch(), which parallelizes model calls client-side.
It is distinct from batch APIs supported by inference providers, such as OpenAI or Anthropic.
By default, batch() will only return the final output for the entire batch. If you want to receive the output for each individual input as it finishes generating, you can stream results with batch_as_completed():
Yield batch responses upon completion
for response in model.batch_as_completed([
    "Why do parrots have colorful feathers?",
    "How do airplanes fly?",
    "What is quantum computing?"
]):
    print(response)
When using batch_as_completed(), results may arrive out of order. Each includes the input index for matching to reconstruct the original order as needed.
When processing a large number of inputs using batch() or batch_as_completed(), you may want to control the maximum number of parallel calls. This can be done by setting the max_concurrency attribute in the RunnableConfig dictionary.
Batch with max concurrency
model.batch(
    list_of_inputs,
    config={
        'max_concurrency': 5,  # Limit to 5 parallel calls
    }
)
See the RunnableConfig reference for a full list of supported attributes.
For more details on batching, see the reference.
‚Äã
Tool calling
Models can request to call tools that perform tasks such as fetching data from a database, searching the web, or running code. Tools are pairings of:
A schema, including the name of the tool, a description, and/or argument definitions (often a JSON schema)
A function or coroutine to execute.
You may hear the term ‚Äúfunction calling‚Äù. We use this interchangeably with ‚Äútool calling‚Äù.
Here‚Äôs the basic tool calling flow between a user and a model:
Tools
Model
User
Tools
Model
User
par
[Parallel Tool Calls]
par
[Tool Execution]
"What's the weather in SF and NYC?"
Analyze request & decide tools needed
get_weather("San Francisco")
get_weather("New York")
SF weather data
NYC weather data
Process results & generate response
"SF: 72¬∞F sunny, NYC: 68¬∞F cloudy"
To make tools that you have defined available for use by a model, you must bind them using bind_tools. In subsequent invocations, the model can choose to call any of the bound tools as needed.
Some model providers offer built-in tools that can be enabled via model or invocation parameters (e.g. ChatOpenAI, ChatAnthropic). Check the respective provider reference for details.
See the tools guide for details and other options for creating tools.
Binding user tools
from langchain.tools import tool

@tool
def get_weather(location: str) -> str:
    """Get the weather at a location."""
    return f"It's sunny in {location}."


model_with_tools = model.bind_tools([get_weather])  

response = model_with_tools.invoke("What's the weather like in Boston?")
for tool_call in response.tool_calls:
    # View tool calls made by the model
    print(f"Tool: {tool_call['name']}")
    print(f"Args: {tool_call['args']}")
When binding user-defined tools, the model‚Äôs response includes a request to execute a tool. When using a model separately from an agent, it is up to you to execute the requested tool and return the result back to the model for use in subsequent reasoning. When using an agent, the agent loop will handle the tool execution loop for you.
Below, we show some common ways you can use tool calling.
Tool execution loop

Forcing tool calls

Parallel tool calls

Streaming tool calls

‚Äã
Structured output
Models can be requested to provide their response in a format matching a given schema. This is useful for ensuring the output can be easily parsed and used in subsequent processing. LangChain supports multiple schema types and methods for enforcing structured output.
Pydantic
TypedDict
JSON Schema
Pydantic models provide the richest feature set with field validation, descriptions, and nested structures.
from pydantic import BaseModel, Field

class Movie(BaseModel):
    """A movie with details."""
    title: str = Field(..., description="The title of the movie")
    year: int = Field(..., description="The year the movie was released")
    director: str = Field(..., description="The director of the movie")
    rating: float = Field(..., description="The movie's rating out of 10")

model_with_structure = model.with_structured_output(Movie)
response = model_with_structure.invoke("Provide details about the movie Inception")
print(response)  # Movie(title="Inception", year=2010, director="Christopher Nolan", rating=8.8)
Key considerations for structured output:
Method parameter: Some providers support different methods ('json_schema', 'function_calling', 'json_mode')
'json_schema' typically refers to dedicated structured output features offered by a provider
'function_calling' derives structured output by forcing a tool call following the given schema
'json_mode' is a precursor to 'json_schema' offered by some providers - it generates valid json, but the schema must be described in the prompt
Include raw: Use include_raw=True to get both the parsed output and the raw AI message
Validation: Pydantic models provide automatic validation, while TypedDict and JSON Schema require manual validation
Example: Message output alongside parsed structure

Example: Nested structures

‚Äã
Supported models
LangChain supports all major model providers, including OpenAI, Anthropic, Google, Azure, AWS Bedrock, and more. Each provider offers a variety of models with different capabilities. For a full list of supported models in LangChain, see the integrations page.
‚Äã
Advanced topics
‚Äã
Model profiles
This is a beta feature. The format of model profiles is subject to change.
Model profiles require langchain>=1.1.
LangChain chat models can expose a dictionary of supported features and capabilities through a .profile attribute:
model.profile
# {
#   "max_input_tokens": 400000,
#   "image_inputs": True,
#   "reasoning_output": True,
#   "tool_calling": True,
#   ...
# }
Refer to the full set of fields in the API reference.
Much of the model profile data is powered by the models.dev project, an open source initiative that provides model capability data. These data are augmented with additional fields for purposes of use with LangChain. These augmentations are kept aligned with the upstream project as it evolves.
Model profile data allow applications to work around model capabilities dynamically. For example:
Summarization middleware can trigger summarization based on a model‚Äôs context window size.
Structured output strategies in create_agent can be inferred automatically (e.g., by checking support for native structured output features).
Model inputs can be gated based on supported modalities and maximum input tokens.
Updating or overwriting profile data

‚Äã
Multimodal
Certain models can process and return non-textual data such as images, audio, and video. You can pass non-textual data to a model by providing content blocks.
All LangChain chat models with underlying multimodal capabilities support:
Data in the cross-provider standard format (see our messages guide)
OpenAI chat completions format
Any format that is native to that specific provider (e.g., Anthropic models accept Anthropic native format)
See the multimodal section of the messages guide for details.
Some models can return multimodal data as part of their response. If invoked to do so, the resulting AIMessage will have content blocks with multimodal types.
Multimodal output
response = model.invoke("Create a picture of a cat")
print(response.content_blocks)
# [
#     {"type": "text", "text": "Here's a picture of a cat"},
#     {"type": "image", "base64": "...", "mime_type": "image/jpeg"},
# ]
See the integrations page for details on specific providers.
‚Äã
Reasoning
Many models are capable of performing multi-step reasoning to arrive at a conclusion. This involves breaking down complex problems into smaller, more manageable steps.
If supported by the underlying model, you can surface this reasoning process to better understand how the model arrived at its final answer.

Stream reasoning output

Complete reasoning output
for chunk in model.stream("Why do parrots have colorful feathers?"):
    reasoning_steps = [r for r in chunk.content_blocks if r["type"] == "reasoning"]
    print(reasoning_steps if reasoning_steps else chunk.text)
Depending on the model, you can sometimes specify the level of effort it should put into reasoning. Similarly, you can request that the model turn off reasoning entirely. This may take the form of categorical ‚Äútiers‚Äù of reasoning (e.g., 'low' or 'high') or integer token budgets.
For details, see the integrations page or reference for your respective chat model.
‚Äã
Local models
LangChain supports running models locally on your own hardware. This is useful for scenarios where either data privacy is critical, you want to invoke a custom model, or when you want to avoid the costs incurred when using a cloud-based model.
Ollama is one of the easiest ways to run models locally. See the full list of local integrations on the integrations page.
‚Äã
Prompt caching
Many providers offer prompt caching features to reduce latency and cost on repeat processing of the same tokens. These features can be implicit or explicit:
Implicit prompt caching: providers will automatically pass on cost savings if a request hits a cache. Examples: OpenAI and Gemini.
Explicit caching: providers allow you to manually indicate cache points for greater control or to guarantee cost savings. Examples:
ChatOpenAI (via prompt_cache_key)
Anthropic‚Äôs AnthropicPromptCachingMiddleware
Gemini.
AWS Bedrock
Prompt caching is often only engaged above a minimum input token threshold. See provider pages for details.
Cache usage will be reflected in the usage metadata of the model response.
‚Äã
Server-side tool use
Some providers support server-side tool-calling loops: models can interact with web search, code interpreters, and other tools and analyze the results in a single conversational turn.
If a model invokes a tool server-side, the content of the response message will include content representing the invocation and result of the tool. Accessing the content blocks of the response will return the server-side tool calls and results in a provider-agnostic format:
Invoke with server-side tool use
from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-4.1-mini")

tool = {"type": "web_search"}
model_with_tools = model.bind_tools([tool])

response = model_with_tools.invoke("What was a positive news story from today?")
response.content_blocks
Result
[
    {
        "type": "server_tool_call",
        "name": "web_search",
        "args": {
            "query": "positive news stories today",
            "type": "search"
        },
        "id": "ws_abc123"
    },
    {
        "type": "server_tool_result",
        "tool_call_id": "ws_abc123",
        "status": "success"
    },
    {
        "type": "text",
        "text": "Here are some positive news stories from today...",
        "annotations": [
            {
                "end_index": 410,
                "start_index": 337,
                "title": "article title",
                "type": "citation",
                "url": "..."
            }
        ]
    }
]
See all 29 lines
This represents a single conversational turn; there are no associated ToolMessage objects that need to be passed in as in client-side tool-calling.
See the integration page for your given provider for available tools and usage details.
‚Äã
Rate limiting
Many chat model providers impose a limit on the number of invocations that can be made in a given time period. If you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.
To help manage rate limits, chat model integrations accept a rate_limiter parameter that can be provided during initialization to control the rate at which requests are made.
Initialize and use a rate limiter

‚Äã
Base URL or proxy
For many chat model integrations, you can configure the base URL for API requests, which allows you to use model providers that have OpenAI-compatible APIs or to use a proxy server.
Base URL

Proxy configuration

‚Äã
Log probabilities
Certain models can be configured to return token-level log probabilities representing the likelihood of a given token by setting the logprobs parameter when initializing the model:
model = init_chat_model(
    model="gpt-4o",
    model_provider="openai"
).bind(logprobs=True)

response = model.invoke("Why do parrots talk?")
print(response.response_metadata["logprobs"])
‚Äã
Token usage
A number of model providers return token usage information as part of the invocation response. When available, this information will be included on the AIMessage objects produced by the corresponding model. For more details, see the messages guide.
Some provider APIs, notably OpenAI and Azure OpenAI chat completions, require users opt-in to receiving token usage data in streaming contexts. See the streaming usage metadata section of the integration guide for details.
You can track aggregate token counts across models in an application using either a callback or context manager, as shown below:
Callback handler
Context manager
from langchain.chat_models import init_chat_model
from langchain_core.callbacks import UsageMetadataCallbackHandler

model_1 = init_chat_model(model="gpt-4o-mini")
model_2 = init_chat_model(model="claude-haiku-4-5-20251001")

callback = UsageMetadataCallbackHandler()
result_1 = model_1.invoke("Hello", config={"callbacks": [callback]})
result_2 = model_2.invoke("Hello", config={"callbacks": [callback]})
callback.usage_metadata
{
    'gpt-4o-mini-2024-07-18': {
        'input_tokens': 8,
        'output_tokens': 10,
        'total_tokens': 18,
        'input_token_details': {'audio': 0, 'cache_read': 0},
        'output_token_details': {'audio': 0, 'reasoning': 0}
    },
    'claude-haiku-4-5-20251001': {
        'input_tokens': 8,
        'output_tokens': 21,
        'total_tokens': 29,
        'input_token_details': {'cache_read': 0, 'cache_creation': 0}
    }
}
‚Äã
Invocation config
When invoking a model, you can pass additional configuration through the config parameter using a RunnableConfig dictionary. This provides run-time control over execution behavior, callbacks, and metadata tracking.
Common configuration options include:
Invocation with config
response = model.invoke(
    "Tell me a joke",
    config={
        "run_name": "joke_generation",      # Custom name for this run
        "tags": ["humor", "demo"],          # Tags for categorization
        "metadata": {"user_id": "123"},     # Custom metadata
        "callbacks": [my_callback_handler], # Callback handlers
    }
)
These configuration values are particularly useful when:
Debugging with LangSmith tracing
Implementing custom logging or monitoring
Controlling resource usage in production
Tracking invocations across complex pipelines
Key configuration attributes

See full RunnableConfig reference for all supported attributes.
‚Äã
Configurable models
You can also create a runtime-configurable model by specifying configurable_fields. If you don‚Äôt specify a model value, then 'model' and 'model_provider' will be configurable by default.
from langchain.chat_models import init_chat_model

configurable_model = init_chat_model(temperature=0)

configurable_model.invoke(
    "what's your name",
    config={"configurable": {"model": "gpt-5-nano"}},  # Run with GPT-5-Nano
)
configurable_model.invoke(
    "what's your name",
    config={"configurable": {"model": "claude-sonnet-4-5-20250929"}},  # Run with Claude
)
Configurable model with default values

Using a configurable model declaratively

Edit the source of this page on GitHub.
Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.
Was this page helpful?


Yes

No
.....Core components
Messages

Copy page

Messages are the fundamental unit of context for models in LangChain. They represent the input and output of models, carrying both the content and metadata needed to represent the state of a conversation when interacting with an LLM.
Messages are objects that contain:
 Role - Identifies the message type (e.g. system, user)
 Content - Represents the actual content of the message (like text, images, audio, documents, etc.)
 Metadata - Optional fields such as response information, message IDs, and token usage
LangChain provides a standard message type that works across all model providers, ensuring consistent behavior regardless of the model being called.
‚Äã
Basic usage
The simplest way to use messages is to create message objects and pass them to a model when invoking.
from langchain.chat_models import init_chat_model
from langchain.messages import HumanMessage, AIMessage, SystemMessage

model = init_chat_model("gpt-5-nano")

system_msg = SystemMessage("You are a helpful assistant.")
human_msg = HumanMessage("Hello, how are you?")

# Use with chat models
messages = [system_msg, human_msg]
response = model.invoke(messages)  # Returns AIMessage
‚Äã
Text prompts
Text prompts are strings - ideal for straightforward generation tasks where you don‚Äôt need to retain conversation history.
response = model.invoke("Write a haiku about spring")
Use text prompts when:
You have a single, standalone request
You don‚Äôt need conversation history
You want minimal code complexity
‚Äã
Message prompts
Alternatively, you can pass in a list of messages to the model by providing a list of message objects.
from langchain.messages import SystemMessage, HumanMessage, AIMessage

messages = [
    SystemMessage("You are a poetry expert"),
    HumanMessage("Write a haiku about spring"),
    AIMessage("Cherry blossoms bloom...")
]
response = model.invoke(messages)
Use message prompts when:
Managing multi-turn conversations
Working with multimodal content (images, audio, files)
Including system instructions
‚Äã
Dictionary format
You can also specify messages directly in OpenAI chat completions format.
messages = [
    {"role": "system", "content": "You are a poetry expert"},
    {"role": "user", "content": "Write a haiku about spring"},
    {"role": "assistant", "content": "Cherry blossoms bloom..."}
]
response = model.invoke(messages)
‚Äã
Message types
 System message - Tells the model how to behave and provide context for interactions
 Human message - Represents user input and interactions with the model
 AI message - Responses generated by the model, including text content, tool calls, and metadata
 Tool message - Represents the outputs of tool calls
‚Äã
System Message
A SystemMessage represent an initial set of instructions that primes the model‚Äôs behavior. You can use a system message to set the tone, define the model‚Äôs role, and establish guidelines for responses.
Basic instructions
system_msg = SystemMessage("You are a helpful coding assistant.")

messages = [
    system_msg,
    HumanMessage("How do I create a REST API?")
]
response = model.invoke(messages)
Detailed persona
from langchain.messages import SystemMessage, HumanMessage

system_msg = SystemMessage("""
You are a senior Python developer with expertise in web frameworks.
Always provide code examples and explain your reasoning.
Be concise but thorough in your explanations.
""")

messages = [
    system_msg,
    HumanMessage("How do I create a REST API?")
]
response = model.invoke(messages)
‚Äã
Human Message
A HumanMessage represents user input and interactions. They can contain text, images, audio, files, and any other amount of multimodal content.
‚Äã
Text content

Message object

String shortcut
response = model.invoke([
  HumanMessage("What is machine learning?")
])
‚Äã
Message metadata
Add metadata
human_msg = HumanMessage(
    content="Hello!",
    name="alice",  # Optional: identify different users
    id="msg_123",  # Optional: unique identifier for tracing
)
The name field behavior varies by provider ‚Äì some use it for user identification, others ignore it. To check, refer to the model provider‚Äôs reference.
‚Äã
AI Message
An AIMessage represents the output of a model invocation. They can include multimodal data, tool calls, and provider-specific metadata that you can later access.
response = model.invoke("Explain AI")
print(type(response))  # <class 'langchain.messages.AIMessage'>
AIMessage objects are returned by the model when calling it, which contains all of the associated metadata in the response.
Providers weigh/contextualize types of messages differently, which means it is sometimes helpful to manually create a new AIMessage object and insert it into the message history as if it came from the model.
from langchain.messages import AIMessage, SystemMessage, HumanMessage

# Create an AI message manually (e.g., for conversation history)
ai_msg = AIMessage("I'd be happy to help you with that question!")

# Add to conversation history
messages = [
    SystemMessage("You are a helpful assistant"),
    HumanMessage("Can you help me?"),
    ai_msg,  # Insert as if it came from the model
    HumanMessage("Great! What's 2+2?")
]

response = model.invoke(messages)
Attributes

‚Äã
Tool calls
When models make tool calls, they‚Äôre included in the AIMessage:
from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-5-nano")

def get_weather(location: str) -> str:
    """Get the weather at a location."""
    ...

model_with_tools = model.bind_tools([get_weather])
response = model_with_tools.invoke("What's the weather in Paris?")

for tool_call in response.tool_calls:
    print(f"Tool: {tool_call['name']}")
    print(f"Args: {tool_call['args']}")
    print(f"ID: {tool_call['id']}")
Other structured data, such as reasoning or citations, can also appear in message content.
‚Äã
Token usage
An AIMessage can hold token counts and other usage metadata in its usage_metadata field:
from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-5-nano")

response = model.invoke("Hello!")
response.usage_metadata
{'input_tokens': 8,
 'output_tokens': 304,
 'total_tokens': 312,
 'input_token_details': {'audio': 0, 'cache_read': 0},
 'output_token_details': {'audio': 0, 'reasoning': 256}}
See UsageMetadata for details.
‚Äã
Streaming and chunks
During streaming, you‚Äôll receive AIMessageChunk objects that can be combined into a full message object:
chunks = []
full_message = None
for chunk in model.stream("Hi"):
    chunks.append(chunk)
    print(chunk.text)
    full_message = chunk if full_message is None else full_message + chunk
Learn more:
Streaming tokens from chat models
Streaming tokens and/or steps from agents
‚Äã
Tool Message
For models that support tool calling, AI messages can contain tool calls. Tool messages are used to pass the results of a single tool execution back to the model.
Tools can generate ToolMessage objects directly. Below, we show a simple example. Read more in the tools guide.
from langchain.messages import AIMessage
from langchain.messages import ToolMessage

# After a model makes a tool call
# (Here, we demonstrate manually creating the messages for brevity)
ai_message = AIMessage(
    content=[],
    tool_calls=[{
        "name": "get_weather",
        "args": {"location": "San Francisco"},
        "id": "call_123"
    }]
)

# Execute tool and create result message
weather_result = "Sunny, 72¬∞F"
tool_message = ToolMessage(
    content=weather_result,
    tool_call_id="call_123"  # Must match the call ID
)

# Continue conversation
messages = [
    HumanMessage("What's the weather in San Francisco?"),
    ai_message,  # Model's tool call
    tool_message,  # Tool execution result
]
response = model.invoke(messages)  # Model processes the result
Attributes

The artifact field stores supplementary data that won‚Äôt be sent to the model but can be accessed programmatically. This is useful for storing raw results, debugging information, or data for downstream processing without cluttering the model‚Äôs context.
Example: Using artifact for retrieval metadata

‚Äã
Message content
You can think of a message‚Äôs content as the payload of data that gets sent to the model. Messages have a content attribute that is loosely-typed, supporting strings and lists of untyped objects (e.g., dictionaries). This allows support for provider-native structures directly in LangChain chat models, such as multimodal content and other data.
Separately, LangChain provides dedicated content types for text, reasoning, citations, multi-modal data, server-side tool calls, and other message content. See content blocks below.
LangChain chat models accept message content in the content attribute.
This may contain either:
A string
A list of content blocks in a provider-native format
A list of LangChain‚Äôs standard content blocks
See below for an example using multimodal inputs:
from langchain.messages import HumanMessage

# String content
human_message = HumanMessage("Hello, how are you?")

# Provider-native format (e.g., OpenAI)
human_message = HumanMessage(content=[
    {"type": "text", "text": "Hello, how are you?"},
    {"type": "image_url", "image_url": {"url": "https://example.com/image.jpg"}}
])

# List of standard content blocks
human_message = HumanMessage(content_blocks=[
    {"type": "text", "text": "Hello, how are you?"},
    {"type": "image", "url": "https://example.com/image.jpg"},
])
Specifying content_blocks when initializing a message will still populate message content, but provides a type-safe interface for doing so.
‚Äã
Standard content blocks
LangChain provides a standard representation for message content that works across providers.
Message objects implement a content_blocks property that will lazily parse the content attribute into a standard, type-safe representation. For example, messages generated from ChatAnthropic or ChatOpenAI will include thinking or reasoning blocks in the format of the respective provider, but can be lazily parsed into a consistent ReasoningContentBlock representation:
Anthropic
OpenAI
from langchain.messages import AIMessage

message = AIMessage(
    content=[
        {"type": "thinking", "thinking": "...", "signature": "WaUjzkyp..."},
        {"type": "text", "text": "..."},
    ],
    response_metadata={"model_provider": "anthropic"}
)
message.content_blocks
[{'type': 'reasoning',
  'reasoning': '...',
  'extras': {'signature': 'WaUjzkyp...'}},
 {'type': 'text', 'text': '...'}]
See the integrations guides to get started with the inference provider of your choice.
Serializing standard content
If an application outside of LangChain needs access to the standard content block representation, you can opt-in to storing content blocks in message content.
To do this, you can set the LC_OUTPUT_VERSION environment variable to v1. Or, initialize any chat model with output_version="v1":
from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-5-nano", output_version="v1")
‚Äã
Multimodal
Multimodality refers to the ability to work with data that comes in different forms, such as text, audio, images, and video. LangChain includes standard types for these data that can be used across providers.
Chat models can accept multimodal data as input and generate it as output. Below we show short examples of input messages featuring multimodal data.
Extra keys can be included top-level in the content block or nested in "extras": {"key": value}.
OpenAI and AWS Bedrock Converse, for example, require a filename for PDFs. See the provider page for your chosen model for specifics.

Image input

PDF document input

Audio input

Video input
# From base64 data
message = {
    "role": "user",
    "content": [
        {"type": "text", "text": "Describe the content of this audio."},
        {
            "type": "audio",
            "base64": "AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...",
            "mime_type": "audio/wav",
        },
    ]
}

# From provider-managed File ID
message = {
    "role": "user",
    "content": [
        {"type": "text", "text": "Describe the content of this audio."},
        {"type": "audio", "file_id": "file-abc123"},
    ]
}
Not all models support all file types. Check the model provider‚Äôs reference for supported formats and size limits.
‚Äã
Content block reference
Content blocks are represented (either when creating a message or accessing the content_blocks property) as a list of typed dictionaries. Each item in the list must adhere to one of the following block types:
Core

Multimodal

Tool Calling

Server-Side Tool Execution

Provider-Specific Blocks

View the canonical type definitions in the API reference.
Content blocks were introduced as a new property on messages in LangChain v1 to standardize content formats across providers while maintaining backward compatibility with existing code.
Content blocks are not a replacement for the content property, but rather a new property that can be used to access the content of a message in a standardized format.
‚Äã
Use with chat models
Chat models accept a sequence of message objects as input and return an AIMessage as output. Interactions are often stateless, so that a simple conversational loop involves invoking a model with a growing list of messages.
Refer to the below guides to learn more:
Built-in features for persisting and managing conversation histories
Strategies for managing context windows, including trimming and summarizing messages
Edit the source of this page on GitHub.
Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.
Was this page helpful?


Yes

No
............Core components
Tools

Copy page

Many AI applications interact with users via natural language. However, some use cases require models to interface directly with external systems‚Äîsuch as APIs, databases, or file systems‚Äîusing structured input.
Tools are components that agents call to perform actions. They extend model capabilities by letting them interact with the world through well-defined inputs and outputs.
Tools encapsulate a callable function and its input schema. These can be passed to compatible chat models, allowing the model to decide whether to invoke a tool and with what arguments. In these scenarios, tool calling enables models to generate requests that conform to a specified input schema.
Server-side tool use
Some chat models (e.g., OpenAI, Anthropic, and Gemini) feature built-in tools that are executed server-side, such as web search and code interpreters. Refer to the provider overview to learn how to access these tools with your specific chat model.
‚Äã
Create tools
‚Äã
Basic tool definition
The simplest way to create a tool is with the @tool decorator. By default, the function‚Äôs docstring becomes the tool‚Äôs description that helps the model understand when to use it:
from langchain.tools import tool

@tool
def search_database(query: str, limit: int = 10) -> str:
    """Search the customer database for records matching the query.

    Args:
        query: Search terms to look for
        limit: Maximum number of results to return
    """
    return f"Found {limit} results for '{query}'"
Type hints are required as they define the tool‚Äôs input schema. The docstring should be informative and concise to help the model understand the tool‚Äôs purpose.
‚Äã
Customize tool properties
‚Äã
Custom tool name
By default, the tool name comes from the function name. Override it when you need something more descriptive:
@tool("web_search")  # Custom name
def search(query: str) -> str:
    """Search the web for information."""
    return f"Results for: {query}"

print(search.name)  # web_search
‚Äã
Custom tool description
Override the auto-generated tool description for clearer model guidance:
@tool("calculator", description="Performs arithmetic calculations. Use this for any math problems.")
def calc(expression: str) -> str:
    """Evaluate mathematical expressions."""
    return str(eval(expression))
‚Äã
Advanced schema definition
Define complex inputs with Pydantic models or JSON schemas:

Pydantic model

JSON Schema
from pydantic import BaseModel, Field
from typing import Literal

class WeatherInput(BaseModel):
    """Input for weather queries."""
    location: str = Field(description="City name or coordinates")
    units: Literal["celsius", "fahrenheit"] = Field(
        default="celsius",
        description="Temperature unit preference"
    )
    include_forecast: bool = Field(
        default=False,
        description="Include 5-day forecast"
    )

@tool(args_schema=WeatherInput)
def get_weather(location: str, units: str = "celsius", include_forecast: bool = False) -> str:
    """Get current weather and optional forecast."""
    temp = 22 if units == "celsius" else 72
    result = f"Current weather in {location}: {temp} degrees {units[0].upper()}"
    if include_forecast:
        result += "\nNext 5 days: Sunny"
    return result
‚Äã
Reserved argument names
The following parameter names are reserved and cannot be used as tool arguments. Using these names will cause runtime errors.
Parameter name	Purpose
config	Reserved for passing RunnableConfig to tools internally
runtime	Reserved for ToolRuntime parameter (accessing state, context, store)
To access runtime information, use the ToolRuntime parameter instead of naming your own arguments config or runtime.
‚Äã
Accessing Context
Why this matters: Tools are most powerful when they can access agent state, runtime context, and long-term memory. This enables tools to make context-aware decisions, personalize responses, and maintain information across conversations.
Runtime context provides a way to inject dependencies (like database connections, user IDs, or configuration) into your tools at runtime, making them more testable and reusable.
Tools can access runtime information through the ToolRuntime parameter, which provides:
State - Mutable data that flows through execution (e.g., messages, counters, custom fields)
Context - Immutable configuration like user IDs, session details, or application-specific configuration
Store - Persistent long-term memory across conversations
Stream Writer - Stream custom updates as tools execute
Config - RunnableConfig for the execution
Tool Call ID - ID of the current tool call
‚ö° Enhanced Tool Capabilities

üìä Available Resources

üîß Tool Runtime Context

Tool Call

ToolRuntime

State Access

Context Access

Store Access

Stream Writer

Messages

Custom State

User ID

Session Info

Long-term Memory

User Preferences

Context-Aware Tools

Stateful Tools

Memory-Enabled Tools

Streaming Tools

‚Äã
ToolRuntime
Use ToolRuntime to access all runtime information in a single parameter. Simply add runtime: ToolRuntime to your tool signature, and it will be automatically injected without being exposed to the LLM.
ToolRuntime: A unified parameter that provides tools access to state, context, store, streaming, config, and tool call ID. This replaces the older pattern of using separate InjectedState, InjectedStore, get_runtime, and InjectedToolCallId annotations.
The runtime automatically provides these capabilities to your tool functions without you having to pass them explicitly or use global state.
Accessing state:
Tools can access the current graph state using ToolRuntime:
from langchain.tools import tool, ToolRuntime

# Access the current conversation state
@tool
def summarize_conversation(
    runtime: ToolRuntime
) -> str:
    """Summarize the conversation so far."""
    messages = runtime.state["messages"]

    human_msgs = sum(1 for m in messages if m.__class__.__name__ == "HumanMessage")
    ai_msgs = sum(1 for m in messages if m.__class__.__name__ == "AIMessage")
    tool_msgs = sum(1 for m in messages if m.__class__.__name__ == "ToolMessage")

    return f"Conversation has {human_msgs} user messages, {ai_msgs} AI responses, and {tool_msgs} tool results"

# Access custom state fields
@tool
def get_user_preference(
    pref_name: str,
    runtime: ToolRuntime  # ToolRuntime parameter is not visible to the model
) -> str:
    """Get a user preference value."""
    preferences = runtime.state.get("user_preferences", {})
    return preferences.get(pref_name, "Not set")
The runtime parameter is hidden from the model. For the example above, the model only sees pref_name in the tool schema - runtime is not included in the request.
Updating state:
Use Command to update the agent‚Äôs state or control the graph‚Äôs execution flow:
from langgraph.types import Command
from langchain.messages import RemoveMessage
from langgraph.graph.message import REMOVE_ALL_MESSAGES
from langchain.tools import tool, ToolRuntime

# Update the conversation history by removing all messages
@tool
def clear_conversation() -> Command:
    """Clear the conversation history."""

    return Command(
        update={
            "messages": [RemoveMessage(id=REMOVE_ALL_MESSAGES)],
        }
    )

# Update the user_name in the agent state
@tool
def update_user_name(
    new_name: str,
    runtime: ToolRuntime
) -> Command:
    """Update the user's name."""
    return Command(update={"user_name": new_name})
‚Äã
Context
Access immutable configuration and contextual data like user IDs, session details, or application-specific configuration through runtime.context.
Tools can access runtime context through ToolRuntime:
from dataclasses import dataclass
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from langchain.tools import tool, ToolRuntime


USER_DATABASE = {
    "user123": {
        "name": "Alice Johnson",
        "account_type": "Premium",
        "balance": 5000,
        "email": "alice@example.com"
    },
    "user456": {
        "name": "Bob Smith",
        "account_type": "Standard",
        "balance": 1200,
        "email": "bob@example.com"
    }
}

@dataclass
class UserContext:
    user_id: str

@tool
def get_account_info(runtime: ToolRuntime[UserContext]) -> str:
    """Get the current user's account information."""
    user_id = runtime.context.user_id

    if user_id in USER_DATABASE:
        user = USER_DATABASE[user_id]
        return f"Account holder: {user['name']}\nType: {user['account_type']}\nBalance: ${user['balance']}"
    return "User not found"

model = ChatOpenAI(model="gpt-4o")
agent = create_agent(
    model,
    tools=[get_account_info],
    context_schema=UserContext,
    system_prompt="You are a financial assistant."
)

result = agent.invoke(
    {"messages": [{"role": "user", "content": "What's my current balance?"}]},
    context=UserContext(user_id="user123")
)
‚Äã
Memory (Store)
Access persistent data across conversations using the store. The store is accessed via runtime.store and allows you to save and retrieve user-specific or application-specific data.
Tools can access and update the store through ToolRuntime:
from typing import Any
from langgraph.store.memory import InMemoryStore
from langchain.agents import create_agent
from langchain.tools import tool, ToolRuntime


# Access memory
@tool
def get_user_info(user_id: str, runtime: ToolRuntime) -> str:
    """Look up user info."""
    store = runtime.store
    user_info = store.get(("users",), user_id)
    return str(user_info.value) if user_info else "Unknown user"

# Update memory
@tool
def save_user_info(user_id: str, user_info: dict[str, Any], runtime: ToolRuntime) -> str:
    """Save user info."""
    store = runtime.store
    store.put(("users",), user_id, user_info)
    return "Successfully saved user info."

store = InMemoryStore()
agent = create_agent(
    model,
    tools=[get_user_info, save_user_info],
    store=store
)

# First session: save user info
agent.invoke({
    "messages": [{"role": "user", "content": "Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev"}]
})

# Second session: get user info
agent.invoke({
    "messages": [{"role": "user", "content": "Get user info for user with id 'abc123'"}]
})
# Here is the user info for user with ID "abc123":
# - Name: Foo
# - Age: 25
# - Email: foo@langchain.dev
See all 42 lines
‚Äã
Stream Writer
Stream custom updates from tools as they execute using runtime.stream_writer. This is useful for providing real-time feedback to users about what a tool is doing.
from langchain.tools import tool, ToolRuntime

@tool
def get_weather(city: str, runtime: ToolRuntime) -> str:
    """Get weather for a given city."""
    writer = runtime.stream_writer

    # Stream custom updates as the tool executes
    writer(f"Looking up data for city: {city}")
    writer(f"Acquired data for city: {city}")

    return f"It's always sunny in {city}!"
If you use runtime.stream_writer inside your tool, the tool must be invoked within a LangGraph execution context. See Streaming for more details.
Edit the source of this page on GitHub.
Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.
Was this page helpful?


Yes

No
.......Core components
Short-term memory

Copy page

‚Äã
Overview
Memory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.
Short term memory lets your application remember previous interactions within a single thread or conversation.
A thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.
Conversation history is the most common form of short-term memory. Long conversations pose a challenge to today‚Äôs LLMs; a full history may not fit inside an LLM‚Äôs context window, resulting in an context loss or errors.
Even if your model supports the full context length, most LLMs still perform poorly over long contexts. They get ‚Äúdistracted‚Äù by stale or off-topic content, all while suffering from slower response times and higher costs.
Chat models accept context using messages, which include instructions (a system message) and inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited, many applications can benefit from using techniques to remove or ‚Äúforget‚Äù stale information.
‚Äã
Usage
To add short-term memory (thread-level persistence) to an agent, you need to specify a checkpointer when creating an agent.
LangChain‚Äôs agent manages short-term memory as a part of your agent‚Äôs state.
By storing these in the graph‚Äôs state, the agent can access the full context for a given conversation while maintaining separation between different threads.
State is persisted to a database (or memory) using a checkpointer so the thread can be resumed at any time.
Short-term memory updates when the agent is invoked or a step (like a tool call) is completed, and the state is read at the start of each step.
from langchain.agents import create_agent
from langgraph.checkpoint.memory import InMemorySaver  


agent = create_agent(
    "gpt-5",
    tools=[get_user_info],
    checkpointer=InMemorySaver(),  
)

agent.invoke(
    {"messages": [{"role": "user", "content": "Hi! My name is Bob."}]},
    {"configurable": {"thread_id": "1"}},  
)
‚Äã
In production
In production, use a checkpointer backed by a database:
pip install langgraph-checkpoint-postgres
from langchain.agents import create_agent

from langgraph.checkpoint.postgres import PostgresSaver  


DB_URI = "postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable"
with PostgresSaver.from_conn_string(DB_URI) as checkpointer:
    checkpointer.setup() # auto create tables in PostgresSql
    agent = create_agent(
        "gpt-5",
        tools=[get_user_info],
        checkpointer=checkpointer,  
    )
‚Äã
Customizing agent memory
By default, agents use AgentState to manage short term memory, specifically the conversation history via a messages key.
You can extend AgentState to add additional fields. Custom state schemas are passed to create_agent using the state_schema parameter.
from langchain.agents import create_agent, AgentState
from langgraph.checkpoint.memory import InMemorySaver


class CustomAgentState(AgentState):  
    user_id: str
    preferences: dict

agent = create_agent(
    "gpt-5",
    tools=[get_user_info],
    state_schema=CustomAgentState,  
    checkpointer=InMemorySaver(),
)

# Custom state can be passed in invoke
result = agent.invoke(
    {
        "messages": [{"role": "user", "content": "Hello"}],
        "user_id": "user_123",  
        "preferences": {"theme": "dark"}  
    },
    {"configurable": {"thread_id": "1"}})
‚Äã
Common patterns
With short-term memory enabled, long conversations can exceed the LLM‚Äôs context window. Common solutions are:
Trim messages
Remove first or last N messages (before calling LLM)
Delete messages
Delete messages from LangGraph state permanently
Summarize messages
Summarize earlier messages in the history and replace them with a summary
Custom strategies
Custom strategies (e.g., message filtering, etc.)
This allows the agent to keep track of the conversation without exceeding the LLM‚Äôs context window.
‚Äã
Trim messages
Most LLMs have a maximum supported context window (denominated in tokens).
One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you‚Äôre using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the strategy (e.g., keep the last max_tokens) to use for handling the boundary.
To trim message history in an agent, use the @before_model middleware decorator:
from langchain.messages import RemoveMessage
from langgraph.graph.message import REMOVE_ALL_MESSAGES
from langgraph.checkpoint.memory import InMemorySaver
from langchain.agents import create_agent, AgentState
from langchain.agents.middleware import before_model
from langgraph.runtime import Runtime
from langchain_core.runnables import RunnableConfig
from typing import Any


@before_model
def trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    """Keep only the last few messages to fit context window."""
    messages = state["messages"]

    if len(messages) <= 3:
        return None  # No changes needed

    first_msg = messages[0]
    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]
    new_messages = [first_msg] + recent_messages

    return {
        "messages": [
            RemoveMessage(id=REMOVE_ALL_MESSAGES),
            *new_messages
        ]
    }

agent = create_agent(
    your_model_here,
    tools=your_tools_here,
    middleware=[trim_messages],
    checkpointer=InMemorySaver(),
)

config: RunnableConfig = {"configurable": {"thread_id": "1"}}

agent.invoke({"messages": "hi, my name is bob"}, config)
agent.invoke({"messages": "write a short poem about cats"}, config)
agent.invoke({"messages": "now do the same but for dogs"}, config)
final_response = agent.invoke({"messages": "what's my name?"}, config)

final_response["messages"][-1].pretty_print()
"""
================================== Ai Message ==================================

Your name is Bob. You told me that earlier.
If you'd like me to call you a nickname or use a different name, just say the word.
"""
‚Äã
Delete messages
You can delete messages from the graph state to manage the message history.
This is useful when you want to remove specific messages or clear the entire message history.
To delete messages from the graph state, you can use the RemoveMessage.
For RemoveMessage to work, you need to use a state key with add_messages reducer.
The default AgentState provides this.
To remove specific messages:
from langchain.messages import RemoveMessage  

def delete_messages(state):
    messages = state["messages"]
    if len(messages) > 2:
        # remove the earliest two messages
        return {"messages": [RemoveMessage(id=m.id) for m in messages[:2]]}  
To remove all messages:
from langgraph.graph.message import REMOVE_ALL_MESSAGES

def delete_messages(state):
    return {"messages": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}  
When deleting messages, make sure that the resulting message history is valid. Check the limitations of the LLM provider you‚Äôre using. For example:
Some providers expect message history to start with a user message
Most providers require assistant messages with tool calls to be followed by corresponding tool result messages.
from langchain.messages import RemoveMessage
from langchain.agents import create_agent, AgentState
from langchain.agents.middleware import after_model
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.runtime import Runtime
from langchain_core.runnables import RunnableConfig


@after_model
def delete_old_messages(state: AgentState, runtime: Runtime) -> dict | None:
    """Remove old messages to keep conversation manageable."""
    messages = state["messages"]
    if len(messages) > 2:
        # remove the earliest two messages
        return {"messages": [RemoveMessage(id=m.id) for m in messages[:2]]}
    return None


agent = create_agent(
    "gpt-5-nano",
    tools=[],
    system_prompt="Please be concise and to the point.",
    middleware=[delete_old_messages],
    checkpointer=InMemorySaver(),
)

config: RunnableConfig = {"configurable": {"thread_id": "1"}}

for event in agent.stream(
    {"messages": [{"role": "user", "content": "hi! I'm bob"}]},
    config,
    stream_mode="values",
):
    print([(message.type, message.content) for message in event["messages"]])

for event in agent.stream(
    {"messages": [{"role": "user", "content": "what's my name?"}]},
    config,
    stream_mode="values",
):
    print([(message.type, message.content) for message in event["messages"]])
[('human', "hi! I'm bob")]
[('human', "hi! I'm bob"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.')]
[('human', "hi! I'm bob"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.'), ('human', "what's my name?")]
[('human', "hi! I'm bob"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.'), ('human', "what's my name?"), ('ai', 'Your name is Bob. How can I help you today, Bob?')]
[('human', "what's my name?"), ('ai', 'Your name is Bob. How can I help you today, Bob?')]
‚Äã
Summarize messages
The problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.

To summarize message history in an agent, use the built-in SummarizationMiddleware:
from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware
from langgraph.checkpoint.memory import InMemorySaver
from langchain_core.runnables import RunnableConfig


checkpointer = InMemorySaver()

agent = create_agent(
    model="gpt-4o",
    tools=[],
    middleware=[
        SummarizationMiddleware(
            model="gpt-4o-mini",
            trigger=("tokens", 4000),
            keep=("messages", 20)
        )
    ],
    checkpointer=checkpointer,
)

config: RunnableConfig = {"configurable": {"thread_id": "1"}}
agent.invoke({"messages": "hi, my name is bob"}, config)
agent.invoke({"messages": "write a short poem about cats"}, config)
agent.invoke({"messages": "now do the same but for dogs"}, config)
final_response = agent.invoke({"messages": "what's my name?"}, config)

final_response["messages"][-1].pretty_print()
"""
================================== Ai Message ==================================

Your name is Bob!
"""
See SummarizationMiddleware for more configuration options.
‚Äã
Access memory
You can access and modify the short-term memory (state) of an agent in several ways:
‚Äã
Tools
‚Äã
Read short-term memory in a tool
Access short term memory (state) in a tool using the ToolRuntime parameter.
The tool_runtime parameter is hidden from the tool signature (so the model doesn‚Äôt see it), but the tool can access the state through it.
from langchain.agents import create_agent, AgentState
from langchain.tools import tool, ToolRuntime


class CustomState(AgentState):
    user_id: str

@tool
def get_user_info(
    runtime: ToolRuntime
) -> str:
    """Look up user info."""
    user_id = runtime.state["user_id"]
    return "User is John Smith" if user_id == "user_123" else "Unknown user"

agent = create_agent(
    model="gpt-5-nano",
    tools=[get_user_info],
    state_schema=CustomState,
)

result = agent.invoke({
    "messages": "look up user information",
    "user_id": "user_123"
})
print(result["messages"][-1].content)
# > User is John Smith.
‚Äã
Write short-term memory from tools
To modify the agent‚Äôs short-term memory (state) during execution, you can return state updates directly from the tools.
This is useful for persisting intermediate results or making information accessible to subsequent tools or prompts.
from langchain.tools import tool, ToolRuntime
from langchain_core.runnables import RunnableConfig
from langchain.messages import ToolMessage
from langchain.agents import create_agent, AgentState
from langgraph.types import Command
from pydantic import BaseModel


class CustomState(AgentState):  
    user_name: str

class CustomContext(BaseModel):
    user_id: str

@tool
def update_user_info(
    runtime: ToolRuntime[CustomContext, CustomState],
) -> Command:
    """Look up and update user info."""
    user_id = runtime.context.user_id
    name = "John Smith" if user_id == "user_123" else "Unknown user"
    return Command(update={  
        "user_name": name,
        # update the message history
        "messages": [
            ToolMessage(
                "Successfully looked up user information",
                tool_call_id=runtime.tool_call_id
            )
        ]
    })

@tool
def greet(
    runtime: ToolRuntime[CustomContext, CustomState]
) -> str | Command:
    """Use this to greet the user once you found their info."""
    user_name = runtime.state.get("user_name", None)
    if user_name is None:
       return Command(update={
            "messages": [
                ToolMessage(
                    "Please call the 'update_user_info' tool it will get and update the user's name.",
                    tool_call_id=runtime.tool_call_id
                )
            ]
        })
    return f"Hello {user_name}!"

agent = create_agent(
    model="gpt-5-nano",
    tools=[update_user_info, greet],
    state_schema=CustomState, 
    context_schema=CustomContext,
)

agent.invoke(
    {"messages": [{"role": "user", "content": "greet the user"}]},
    context=CustomContext(user_id="user_123"),
)
‚Äã
Prompt
Access short term memory (state) in middleware to create dynamic prompts based on conversation history or custom state fields.
from langchain.agents import create_agent
from typing import TypedDict
from langchain.agents.middleware import dynamic_prompt, ModelRequest


class CustomContext(TypedDict):
    user_name: str


def get_weather(city: str) -> str:
    """Get the weather in a city."""
    return f"The weather in {city} is always sunny!"


@dynamic_prompt
def dynamic_system_prompt(request: ModelRequest) -> str:
    user_name = request.runtime.context["user_name"]
    system_prompt = f"You are a helpful assistant. Address the user as {user_name}."
    return system_prompt


agent = create_agent(
    model="gpt-5-nano",
    tools=[get_weather],
    middleware=[dynamic_system_prompt],
    context_schema=CustomContext,
)

result = agent.invoke(
    {"messages": [{"role": "user", "content": "What is the weather in SF?"}]},
    context=CustomContext(user_name="John Smith"),
)
for msg in result["messages"]:
    msg.pretty_print()

Output
================================ Human Message =================================

What is the weather in SF?
================================== Ai Message ==================================
Tool Calls:
  get_weather (call_WFQlOGn4b2yoJrv7cih342FG)
 Call ID: call_WFQlOGn4b2yoJrv7cih342FG
  Args:
    city: San Francisco
================================= Tool Message =================================
Name: get_weather

The weather in San Francisco is always sunny!
================================== Ai Message ==================================

Hi John Smith, the weather in San Francisco is always sunny!
‚Äã
Before model
Access short term memory (state) in @before_model middleware to process messages before model calls.
__start__

before_model

model

tools

__end__

from langchain.messages import RemoveMessage
from langgraph.graph.message import REMOVE_ALL_MESSAGES
from langgraph.checkpoint.memory import InMemorySaver
from langchain.agents import create_agent, AgentState
from langchain.agents.middleware import before_model
from langchain_core.runnables import RunnableConfig
from langgraph.runtime import Runtime
from typing import Any


@before_model
def trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    """Keep only the last few messages to fit context window."""
    messages = state["messages"]

    if len(messages) <= 3:
        return None  # No changes needed

    first_msg = messages[0]
    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]
    new_messages = [first_msg] + recent_messages

    return {
        "messages": [
            RemoveMessage(id=REMOVE_ALL_MESSAGES),
            *new_messages
        ]
    }


agent = create_agent(
    "gpt-5-nano",
    tools=[],
    middleware=[trim_messages],
    checkpointer=InMemorySaver()
)

config: RunnableConfig = {"configurable": {"thread_id": "1"}}

agent.invoke({"messages": "hi, my name is bob"}, config)
agent.invoke({"messages": "write a short poem about cats"}, config)
agent.invoke({"messages": "now do the same but for dogs"}, config)
final_response = agent.invoke({"messages": "what's my name?"}, config)

final_response["messages"][-1].pretty_print()
"""
================================== Ai Message ==================================

Your name is Bob. You told me that earlier.
If you'd like me to call you a nickname or use a different name, just say the word.
"""
‚Äã
After model
Access short term memory (state) in @after_model middleware to process messages after model calls.
__start__

model

after_model

tools

__end__

from langchain.messages import RemoveMessage
from langgraph.checkpoint.memory import InMemorySaver
from langchain.agents import create_agent, AgentState
from langchain.agents.middleware import after_model
from langgraph.runtime import Runtime


@after_model
def validate_response(state: AgentState, runtime: Runtime) -> dict | None:
    """Remove messages containing sensitive words."""
    STOP_WORDS = ["password", "secret"]
    last_message = state["messages"][-1]
    if any(word in last_message.content for word in STOP_WORDS):
        return {"messages": [RemoveMessage(id=last_message.id)]}
    return None

agent = create_agent(
    model="gpt-5-nano",
    tools=[],
    middleware=[validate_response],
    checkpointer=InMemorySaver(),
)
Edit the source of this page on GitHub.
Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.
Was this page helpful?


Yes

No
Ask a question...
.......Core components
Streaming

Copy page

LangChain implements a streaming system to surface real-time updates.
Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.
‚Äã
Overview
LangChain‚Äôs streaming system lets you surface live feedback from agent runs to your application.
What‚Äôs possible with LangChain streaming:
 Stream agent progress ‚Äî get state updates after each agent step.
 Stream LLM tokens ‚Äî stream language model tokens as they‚Äôre generated.
 Stream custom updates ‚Äî emit user-defined signals (e.g., "Fetched 10/100 records").
 Stream multiple modes ‚Äî choose from updates (agent progress), messages (LLM tokens + metadata), or custom (arbitrary user data).
‚Äã
Agent progress
To stream agent progress, use the stream or astream methods with stream_mode="updates". This emits an event after every agent step.
For example, if you have an agent that calls a tool once, you should see the following updates:
LLM node: AIMessage with tool call requests
Tool node: ToolMessage with execution result
LLM node: Final AI response
Streaming agent progress
from langchain.agents import create_agent


def get_weather(city: str) -> str:
    """Get weather for a given city."""

    return f"It's always sunny in {city}!"

agent = create_agent(
    model="gpt-5-nano",
    tools=[get_weather],
)
for chunk in agent.stream(  
    {"messages": [{"role": "user", "content": "What is the weather in SF?"}]},
    stream_mode="updates",
):
    for step, data in chunk.items():
        print(f"step: {step}")
        print(f"content: {data['messages'][-1].content_blocks}")
Output
step: model
content: [{'type': 'tool_call', 'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_OW2NYNsNSKhRZpjW0wm2Aszd'}]

step: tools
content: [{'type': 'text', 'text': "It's always sunny in San Francisco!"}]

step: model
content: [{'type': 'text', 'text': 'It's always sunny in San Francisco!'}]
‚Äã
LLM tokens
To stream tokens as they are produced by the LLM, use stream_mode="messages". Below you can see the output of the agent streaming tool calls and the final response.
Streaming LLM tokens
from langchain.agents import create_agent


def get_weather(city: str) -> str:
    """Get weather for a given city."""

    return f"It's always sunny in {city}!"

agent = create_agent(
    model="gpt-5-nano",
    tools=[get_weather],
)
for token, metadata in agent.stream(  
    {"messages": [{"role": "user", "content": "What is the weather in SF?"}]},
    stream_mode="messages",
):
    print(f"node: {metadata['langgraph_node']}")
    print(f"content: {token.content_blocks}")
    print("\n")
Output
node: model
content: [{'type': 'tool_call_chunk', 'id': 'call_vbCyBcP8VuneUzyYlSBZZsVa', 'name': 'get_weather', 'args': '', 'index': 0}]


node: model
content: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '{"', 'index': 0}]


node: model
content: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': 'city', 'index': 0}]


node: model
content: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '":"', 'index': 0}]


node: model
content: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': 'San', 'index': 0}]


node: model
content: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': ' Francisco', 'index': 0}]


node: model
content: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '"}', 'index': 0}]


node: model
content: []


node: tools
content: [{'type': 'text', 'text': "It's always sunny in San Francisco!"}]


node: model
content: []


node: model
content: [{'type': 'text', 'text': 'Here'}]


node: model
content: [{'type': 'text', 'text': ''s'}]


node: model
content: [{'type': 'text', 'text': ' what'}]


node: model
content: [{'type': 'text', 'text': ' I'}]


node: model
content: [{'type': 'text', 'text': ' got'}]


node: model
content: [{'type': 'text', 'text': ':'}]


node: model
content: [{'type': 'text', 'text': ' "'}]


node: model
content: [{'type': 'text', 'text': "It's"}]


node: model
content: [{'type': 'text', 'text': ' always'}]


node: model
content: [{'type': 'text', 'text': ' sunny'}]


node: model
content: [{'type': 'text', 'text': ' in'}]


node: model
content: [{'type': 'text', 'text': ' San'}]


node: model
content: [{'type': 'text', 'text': ' Francisco'}]


node: model
content: [{'type': 'text', 'text': '!"\n\n'}]
See all 94 lines
‚Äã
Custom updates
To stream updates from tools as they are executed, you can use get_stream_writer.
Streaming custom updates
from langchain.agents import create_agent
from langgraph.config import get_stream_writer  


def get_weather(city: str) -> str:
    """Get weather for a given city."""
    writer = get_stream_writer()  
    # stream any arbitrary data
    writer(f"Looking up data for city: {city}")
    writer(f"Acquired data for city: {city}")
    return f"It's always sunny in {city}!"

agent = create_agent(
    model="claude-sonnet-4-5-20250929",
    tools=[get_weather],
)

for chunk in agent.stream(
    {"messages": [{"role": "user", "content": "What is the weather in SF?"}]},
    stream_mode="custom"
):
    print(chunk)
Output
Looking up data for city: San Francisco
Acquired data for city: San Francisco
If you add get_stream_writer inside your tool, you won‚Äôt be able to invoke the tool outside of a LangGraph execution context.
‚Äã
Stream multiple modes
You can specify multiple streaming modes by passing stream mode as a list: stream_mode=["updates", "custom"]:
Streaming multiple modes
from langchain.agents import create_agent
from langgraph.config import get_stream_writer


def get_weather(city: str) -> str:
    """Get weather for a given city."""
    writer = get_stream_writer()
    writer(f"Looking up data for city: {city}")
    writer(f"Acquired data for city: {city}")
    return f"It's always sunny in {city}!"

agent = create_agent(
    model="gpt-5-nano",
    tools=[get_weather],
)

for stream_mode, chunk in agent.stream(  
    {"messages": [{"role": "user", "content": "What is the weather in SF?"}]},
    stream_mode=["updates", "custom"]
):
    print(f"stream_mode: {stream_mode}")
    print(f"content: {chunk}")
    print("\n")
Output
stream_mode: updates
content: {'model': {'messages': [AIMessage(content='', response_metadata={'token_usage': {'completion_tokens': 280, 'prompt_tokens': 132, 'total_tokens': 412, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C9tlgBzGEbedGYxZ0rTCz5F7OXpL7', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--480c07cb-e405-4411-aa7f-0520fddeed66-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_KTNQIftMrl9vgNwEfAJMVu7r', 'type': 'tool_call'}], usage_metadata={'input_tokens': 132, 'output_tokens': 280, 'total_tokens': 412, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}})]}}


stream_mode: custom
content: Looking up data for city: San Francisco


stream_mode: custom
content: Acquired data for city: San Francisco


stream_mode: updates
content: {'tools': {'messages': [ToolMessage(content="It's always sunny in San Francisco!", name='get_weather', tool_call_id='call_KTNQIftMrl9vgNwEfAJMVu7r')]}}


stream_mode: updates
content: {'model': {'messages': [AIMessage(content='San Francisco weather: It's always sunny in San Francisco!\n\n', response_metadata={'token_usage': {'completion_tokens': 764, 'prompt_tokens': 168, 'total_tokens': 932, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 704, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C9tljDFVki1e1haCyikBptAuXuHYG', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--acbc740a-18fe-4a14-8619-da92a0d0ee90-0', usage_metadata={'input_tokens': 168, 'output_tokens': 764, 'total_tokens': 932, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 704}})]}}
‚Äã
Disable streaming
In some applications you might need to disable streaming of individual tokens for a given model.
This is useful in multi-agent systems to control which agents stream their output.
See the Models guide to learn how to disable streaming.
Edit the source of this page on GitHub.
Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.
Was this page helpful?


Yes

No
.......Core components
Structured output

Copy page

Structured output allows agents to return data in a specific, predictable format. Instead of parsing natural language responses, you get structured data in the form of JSON objects, Pydantic models, or dataclasses that your application can directly use.
LangChain‚Äôs create_agent handles structured output automatically. The user sets their desired structured output schema, and when the model generates the structured data, it‚Äôs captured, validated, and returned in the 'structured_response' key of the agent‚Äôs state.
def create_agent(
    ...
    response_format: Union[
        ToolStrategy[StructuredResponseT],
        ProviderStrategy[StructuredResponseT],
        type[StructuredResponseT],
    ]
‚Äã
Response Format
Controls how the agent returns structured data:
ToolStrategy[StructuredResponseT]: Uses tool calling for structured output
ProviderStrategy[StructuredResponseT]: Uses provider-native structured output
type[StructuredResponseT]: Schema type - automatically selects best strategy based on model capabilities
None: No structured output
When a schema type is provided directly, LangChain automatically chooses:
ProviderStrategy for models supporting native structured output (e.g. OpenAI, Anthropic, or Grok).
ToolStrategy for all other models.
Support for native structured output features is read dynamically from the model‚Äôs profile data if using langchain>=1.1. If data are not available, use another condition or specify manually:
custom_profile = {
    "structured_output": True,
    # ...
}
model = init_chat_model("...", profile=custom_profile)
If tools are specified, the model must support simultaneous use of tools and structured output.
The structured response is returned in the structured_response key of the agent‚Äôs final state.
‚Äã
Provider strategy
Some model providers support structured output natively through their APIs (e.g. OpenAI, Grok, Gemini). This is the most reliable method when available.
To use this strategy, configure a ProviderStrategy:
class ProviderStrategy(Generic[SchemaT]):
    schema: type[SchemaT]
‚Äã
schema
required
The schema defining the structured output format. Supports:
Pydantic models: BaseModel subclasses with field validation
Dataclasses: Python dataclasses with type annotations
TypedDict: Typed dictionary classes
JSON Schema: Dictionary with JSON schema specification
LangChain automatically uses ProviderStrategy when you pass a schema type directly to create_agent.response_format and the model supports native structured output:

Pydantic Model

Dataclass

TypedDict

JSON Schema
from pydantic import BaseModel, Field
from langchain.agents import create_agent


class ContactInfo(BaseModel):
    """Contact information for a person."""
    name: str = Field(description="The name of the person")
    email: str = Field(description="The email address of the person")
    phone: str = Field(description="The phone number of the person")

agent = create_agent(
    model="gpt-5",
    response_format=ContactInfo  # Auto-selects ProviderStrategy
)

result = agent.invoke({
    "messages": [{"role": "user", "content": "Extract contact info from: John Doe, john@example.com, (555) 123-4567"}]
})

print(result["structured_response"])
# ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')
Provider-native structured output provides high reliability and strict validation because the model provider enforces the schema. Use it when available.
If the provider natively supports structured output for your model choice, it is functionally equivalent to write response_format=ProductReview instead of response_format=ProviderStrategy(ProductReview). In either case, if structured output is not supported, the agent will fall back to a tool calling strategy.
‚Äã
Tool calling strategy
For models that don‚Äôt support native structured output, LangChain uses tool calling to achieve the same result. This works with all models that support tool calling, which is most modern models.
To use this strategy, configure a ToolStrategy:
class ToolStrategy(Generic[SchemaT]):
    schema: type[SchemaT]
    tool_message_content: str | None
    handle_errors: Union[
        bool,
        str,
        type[Exception],
        tuple[type[Exception], ...],
        Callable[[Exception], str],
    ]
‚Äã
schema
required
The schema defining the structured output format. Supports:
Pydantic models: BaseModel subclasses with field validation
Dataclasses: Python dataclasses with type annotations
TypedDict: Typed dictionary classes
JSON Schema: Dictionary with JSON schema specification
Union types: Multiple schema options. The model will choose the most appropriate schema based on the context.
‚Äã
tool_message_content
Custom content for the tool message returned when structured output is generated. If not provided, defaults to a message showing the structured response data.
‚Äã
handle_errors
Error handling strategy for structured output validation failures. Defaults to True.
True: Catch all errors with default error template
str: Catch all errors with this custom message
type[Exception]: Only catch this exception type with default message
tuple[type[Exception], ...]: Only catch these exception types with default message
Callable[[Exception], str]: Custom function that returns error message
False: No retry, let exceptions propagate

Pydantic Model

Dataclass

TypedDict

JSON Schema

Union Types
from pydantic import BaseModel, Field
from typing import Literal
from langchain.agents import create_agent
from langchain.agents.structured_output import ToolStrategy


class ProductReview(BaseModel):
    """Analysis of a product review."""
    rating: int | None = Field(description="The rating of the product", ge=1, le=5)
    sentiment: Literal["positive", "negative"] = Field(description="The sentiment of the review")
    key_points: list[str] = Field(description="The key points of the review. Lowercase, 1-3 words each.")

agent = create_agent(
    model="gpt-5",
    tools=tools,
    response_format=ToolStrategy(ProductReview)
)

result = agent.invoke({
    "messages": [{"role": "user", "content": "Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'"}]
})
result["structured_response"]
# ProductReview(rating=5, sentiment='positive', key_points=['fast shipping', 'expensive'])
‚Äã
Custom tool message content
The tool_message_content parameter allows you to customize the message that appears in the conversation history when structured output is generated:
from pydantic import BaseModel, Field
from typing import Literal
from langchain.agents import create_agent
from langchain.agents.structured_output import ToolStrategy


class MeetingAction(BaseModel):
    """Action items extracted from a meeting transcript."""
    task: str = Field(description="The specific task to be completed")
    assignee: str = Field(description="Person responsible for the task")
    priority: Literal["low", "medium", "high"] = Field(description="Priority level")

agent = create_agent(
    model="gpt-5",
    tools=[],
    response_format=ToolStrategy(
        schema=MeetingAction,
        tool_message_content="Action item captured and added to meeting notes!"
    )
)

agent.invoke({
    "messages": [{"role": "user", "content": "From our meeting: Sarah needs to update the project timeline as soon as possible"}]
})
================================ Human Message =================================

From our meeting: Sarah needs to update the project timeline as soon as possible
================================== Ai Message ==================================
Tool Calls:
  MeetingAction (call_1)
 Call ID: call_1
  Args:
    task: Update the project timeline
    assignee: Sarah
    priority: high
================================= Tool Message =================================
Name: MeetingAction

Action item captured and added to meeting notes!
Without tool_message_content, our final ToolMessage would be:
================================= Tool Message =================================
Name: MeetingAction

Returning structured response: {'task': 'update the project timeline', 'assignee': 'Sarah', 'priority': 'high'}
‚Äã
Error handling
Models can make mistakes when generating structured output via tool calling. LangChain provides intelligent retry mechanisms to handle these errors automatically.
‚Äã
Multiple structured outputs error
When a model incorrectly calls multiple structured output tools, the agent provides error feedback in a ToolMessage and prompts the model to retry:
from pydantic import BaseModel, Field
from typing import Union
from langchain.agents import create_agent
from langchain.agents.structured_output import ToolStrategy


class ContactInfo(BaseModel):
    name: str = Field(description="Person's name")
    email: str = Field(description="Email address")

class EventDetails(BaseModel):
    event_name: str = Field(description="Name of the event")
    date: str = Field(description="Event date")

agent = create_agent(
    model="gpt-5",
    tools=[],
    response_format=ToolStrategy(Union[ContactInfo, EventDetails])  # Default: handle_errors=True
)

agent.invoke({
    "messages": [{"role": "user", "content": "Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th"}]
})
================================ Human Message =================================

Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th
None
================================== Ai Message ==================================
Tool Calls:
  ContactInfo (call_1)
 Call ID: call_1
  Args:
    name: John Doe
    email: john@email.com
  EventDetails (call_2)
 Call ID: call_2
  Args:
    event_name: Tech Conference
    date: March 15th
================================= Tool Message =================================
Name: ContactInfo

Error: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.
 Please fix your mistakes.
================================= Tool Message =================================
Name: EventDetails

Error: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.
 Please fix your mistakes.
================================== Ai Message ==================================
Tool Calls:
  ContactInfo (call_3)
 Call ID: call_3
  Args:
    name: John Doe
    email: john@email.com
================================= Tool Message =================================
Name: ContactInfo

Returning structured response: {'name': 'John Doe', 'email': 'john@email.com'}
‚Äã
Schema validation error
When structured output doesn‚Äôt match the expected schema, the agent provides specific error feedback:
from pydantic import BaseModel, Field
from langchain.agents import create_agent
from langchain.agents.structured_output import ToolStrategy


class ProductRating(BaseModel):
    rating: int | None = Field(description="Rating from 1-5", ge=1, le=5)
    comment: str = Field(description="Review comment")

agent = create_agent(
    model="gpt-5",
    tools=[],
    response_format=ToolStrategy(ProductRating),  # Default: handle_errors=True
    system_prompt="You are a helpful assistant that parses product reviews. Do not make any field or value up."
)

agent.invoke({
    "messages": [{"role": "user", "content": "Parse this: Amazing product, 10/10!"}]
})
================================ Human Message =================================

Parse this: Amazing product, 10/10!
================================== Ai Message ==================================
Tool Calls:
  ProductRating (call_1)
 Call ID: call_1
  Args:
    rating: 10
    comment: Amazing product
================================= Tool Message =================================
Name: ProductRating

Error: Failed to parse structured output for tool 'ProductRating': 1 validation error for ProductRating.rating
  Input should be less than or equal to 5 [type=less_than_equal, input_value=10, input_type=int].
 Please fix your mistakes.
================================== Ai Message ==================================
Tool Calls:
  ProductRating (call_2)
 Call ID: call_2
  Args:
    rating: 5
    comment: Amazing product
================================= Tool Message =================================
Name: ProductRating

Returning structured response: {'rating': 5, 'comment': 'Amazing product'}
‚Äã
Error handling strategies
You can customize how errors are handled using the handle_errors parameter:
Custom error message:
ToolStrategy(
    schema=ProductRating,
    handle_errors="Please provide a valid rating between 1-5 and include a comment."
)
If handle_errors is a string, the agent will always prompt the model to re-try with a fixed tool message:
================================= Tool Message =================================
Name: ProductRating

Please provide a valid rating between 1-5 and include a comment.
Handle specific exceptions only:
ToolStrategy(
    schema=ProductRating,
    handle_errors=ValueError  # Only retry on ValueError, raise others
)
If handle_errors is an exception type, the agent will only retry (using the default error message) if the exception raised is the specified type. In all other cases, the exception will be raised.
Handle multiple exception types:
ToolStrategy(
    schema=ProductRating,
    handle_errors=(ValueError, TypeError)  # Retry on ValueError and TypeError
)
If handle_errors is a tuple of exceptions, the agent will only retry (using the default error message) if the exception raised is one of the specified types. In all other cases, the exception will be raised.
Custom error handler function:

from langchain.agents.structured_output import StructuredOutputValidationError
from langchain.agents.structured_output import MultipleStructuredOutputsError

def custom_error_handler(error: Exception) -> str:
    if isinstance(error, StructuredOutputValidationError):
        return "There was an issue with the format. Try again.
    elif isinstance(error, MultipleStructuredOutputsError):
        return "Multiple structured outputs were returned. Pick the most relevant one."
    else:
        return f"Error: {str(error)}"


agent = create_agent(
    model="gpt-5",
    tools=[],
    response_format=ToolStrategy(
                        schema=Union[ContactInfo, EventDetails],
                        handle_errors=custom_error_handler
                    )  # Default: handle_errors=True
)

result = agent.invoke({
    "messages": [{"role": "user", "content": "Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th"}]
})

for msg in result['messages']:
    # If message is actually a ToolMessage object (not a dict), check its class name
    if type(msg).__name__ == "ToolMessage":
        print(msg.content)
    # If message is a dictionary or you want a fallback
    elif isinstance(msg, dict) and msg.get('tool_call_id'):
        print(msg['content'])

On StructuredOutputValidationError:
================================= Tool Message =================================
Name: ToolStrategy

There was an issue with the format. Try again.
On MultipleStructuredOutputsError:
================================= Tool Message =================================
Name: ToolStrategy

Multiple structured outputs were returned. Pick the most relevant one.
On other errors:
================================= Tool Message =================================
Name: ToolStrategy

Error: <error message>
No error handling:
response_format = ToolStrategy(
    schema=ProductRating,
    handle_errors=False  # All errors raised
)
Edit the source of this page on GitHub.
Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.
Was this page helpful?


Yes

No
....langchain-google-genai¬∂
PyPI - Version PyPI - License PyPI - Downloads

Reference docs

This page contains reference documentation for Google GenAI. See the docs for conceptual guides, tutorials, and examples on using Google GenAI modules.

 langchain_google_genai ¬∂
LangChain Google Generative AI Integration (GenAI).

This module integrates Google's Generative AI models, specifically the Gemini series, with the LangChain framework. It provides classes for interacting with chat models and generating embeddings, leveraging Google's advanced AI capabilities.

Chat Models

The ChatGoogleGenerativeAI class is the primary interface for interacting with Google's Gemini chat models. It allows users to send and receive messages using a specified Gemini model, suitable for various conversational AI applications.

LLMs

The GoogleGenerativeAI class is the primary interface for interacting with Google's Gemini LLMs. It allows users to generate text using a specified Gemini model.

Embeddings

The GoogleGenerativeAIEmbeddings class provides functionalities to generate embeddings using Google's models. These embeddings can be used for a range of NLP tasks, including semantic analysis, similarity comparisons, and more.

Using Chat Models

After setting up your environment with the required API key, you can interact with the Google Gemini models.


from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro")
llm.invoke("Sing a ballad of LangChain.")
Using LLMs

The package also supports generating text with Google's models.


from langchain_google_genai import GoogleGenerativeAI

llm = GoogleGenerativeAI(model="gemini-2.5-pro")
llm.invoke("Once upon a time, a library called LangChain")
Embedding Generation

The package also supports creating embeddings with Google's models, useful for textual similarity and other NLP applications.


from langchain_google_genai import GoogleGenerativeAIEmbeddings

embeddings = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001")
embeddings.embed_query("hello, world!")
 ChatGoogleGenerativeAI ¬∂
Bases: _BaseGoogleGenerativeAI, BaseChatModel

Google GenAI chat model integration.

Instantiation
To use, you must have either:

The GOOGLE_API_KEY environment variable set with your API key, or
Pass your API key using the google_api_key kwarg to the ChatGoogleGenerativeAI constructor.

from langchain_google_genai import ChatGoogleGenerativeAI

model = ChatGoogleGenerativeAI(model="gemini-3-pro-preview")
model.invoke("Write me a ballad about LangChain")
Invoke
See the docs for more info.


messages = [
    ("system", "Translate the user sentence to French."),
    ("human", "I love programming."),
]
model.invoke(messages)

AIMessage(
    content=[
        {
            "type": "text",
            "text": "**J'adore la programmation.**\n\nYou can also say:...",
            "extras": {"signature": "Eq0W..."},
        }
    ],
    additional_kwargs={},
    response_metadata={
        "prompt_feedback": {"block_reason": 0, "safety_ratings": []},
        "finish_reason": "STOP",
        "model_name": "gemini-3-pro-preview",
        "safety_ratings": [],
        "model_provider": "google_genai",
    },
    id="lc_run--63a04ced-6b63-4cf6-86a1-c32fa565938e-0",
    usage_metadata={
        "input_tokens": 12,
        "output_tokens": 826,
        "total_tokens": 838,
        "input_token_details": {"cache_read": 0},
        "output_token_details": {"reasoning": 777},
    },
)
Stream

from langchain_google_genai import ChatGoogleGenerativeAI

model = ChatGoogleGenerativeAI(model="gemini-2.5-flash")

for chunk in model.stream(messages):
    print(chunk)

AIMessageChunk(
    content="J",
    response_metadata={"finish_reason": "STOP", "safety_ratings": []},
    id="run-e905f4f4-58cb-4a10-a960-448a2bb649e3",
    usage_metadata={
        "input_tokens": 18,
        "output_tokens": 1,
        "total_tokens": 19,
    },
)
AIMessageChunk(
    content="'adore programmer. \\n",
    response_metadata={
        "finish_reason": "STOP",
        "safety_ratings": [
            {
                "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                "probability": "NEGLIGIBLE",
                "blocked": False,
            },
            {
                "category": "HARM_CATEGORY_HATE_SPEECH",
                "probability": "NEGLIGIBLE",
                "blocked": False,
            },
            {
                "category": "HARM_CATEGORY_HARASSMENT",
                "probability": "NEGLIGIBLE",
                "blocked": False,
            },
            {
                "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                "probability": "NEGLIGIBLE",
                "blocked": False,
            },
        ],
    },
    id="run-e905f4f4-58cb-4a10-a960-448a2bb649e3",
    usage_metadata={
        "input_tokens": 18,
        "output_tokens": 5,
        "total_tokens": 23,
    },
)

stream = model.stream(messages)
full = next(stream)
for chunk in stream:
    full += chunk
full

AIMessageChunk(
    content="J'adore programmer. \\n",
    response_metadata={
        "finish_reason": "STOPSTOP",
        "safety_ratings": [
            {
                "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                "probability": "NEGLIGIBLE",
                "blocked": False,
            },
            {
                "category": "HARM_CATEGORY_HATE_SPEECH",
                "probability": "NEGLIGIBLE",
                "blocked": False,
            },
            {
                "category": "HARM_CATEGORY_HARASSMENT",
                "probability": "NEGLIGIBLE",
                "blocked": False,
            },
            {
                "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                "probability": "NEGLIGIBLE",
                "blocked": False,
            },
        ],
    },
    id="run-3ce13a42-cd30-4ad7-a684-f1f0b37cdeec",
    usage_metadata={
        "input_tokens": 36,
        "output_tokens": 6,
        "total_tokens": 42,
    },
)
Async

await model.ainvoke(messages)

# stream:
async for chunk in (await model.astream(messages))

# batch:
await model.abatch([messages])
Tool calling
See the docs for more info.


from pydantic import BaseModel, Field


class GetWeather(BaseModel):
    '''Get the current weather in a given location'''

    location: str = Field(
        ..., description="The city and state, e.g. San Francisco, CA"
    )


class GetPopulation(BaseModel):
    '''Get the current population in a given location'''

    location: str = Field(
        ..., description="The city and state, e.g. San Francisco, CA"
    )


llm_with_tools = llm.bind_tools([GetWeather, GetPopulation])
ai_msg = llm_with_tools.invoke(
    "Which city is hotter today and which is bigger: LA or NY?"
)
ai_msg.tool_calls

[
    {
        "name": "GetWeather",
        "args": {"location": "Los Angeles, CA"},
        "id": "c186c99f-f137-4d52-947f-9e3deabba6f6",
    },
    {
        "name": "GetWeather",
        "args": {"location": "New York City, NY"},
        "id": "cebd4a5d-e800-4fa5-babd-4aa286af4f31",
    },
    {
        "name": "GetPopulation",
        "args": {"location": "Los Angeles, CA"},
        "id": "4f92d897-f5e4-4d34-a3bc-93062c92591e",
    },
    {
        "name": "GetPopulation",
        "args": {"location": "New York City, NY"},
        "id": "634582de-5186-4e4b-968b-f192f0a93678",
    },
]
Structured output
See the docs for more info.


from typing import Optional

from pydantic import BaseModel, Field


class Joke(BaseModel):
    '''Joke to tell user.'''

    setup: str = Field(description="The setup of the joke")
    punchline: str = Field(description="The punchline to the joke")
    rating: Optional[int] = Field(
        description="How funny the joke is, from 1 to 10"
    )


# Default method uses function calling
structured_model = model.with_structured_output(Joke)

# For more reliable output, use json_schema with native responseSchema
structured_model_json = model.with_structured_output(Joke, method="json_schema")
structured_model_json.invoke("Tell me a joke about cats")

Joke(
    setup="Why are cats so good at video games?",
    punchline="They have nine lives on the internet",
    rating=None,
)
Two methods are supported for structured output:

method='function_calling' (default): Uses tool calling to extract structured data. Compatible with all models.
method='json_schema': Uses Gemini's native structured output.

Supports unions (anyOf), recursive schemas ($ref), property ordering preservation, and streaming of partial JSON chunks.

Uses Gemini's response_json_schema API param. Refer to the Gemini API docs for more details.

The json_schema method is recommended for better reliability as it constrains the model's generation process directly rather than relying on post-processing tool calls.

Image input
See the docs for more info.


import base64
import httpx
from langchain.messages import HumanMessage

image_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
image_data = base64.b64encode(httpx.get(image_url).content).decode("utf-8")
message = HumanMessage(
    content=[
        {"type": "text", "text": "describe the weather in this image"},
        {
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{image_data}"},
        },
    ]
)
ai_msg = model.invoke([message])
ai_msg.content

The weather in this image appears to be sunny and pleasant. The sky is a bright
blue with scattered white clouds, suggesting fair weather. The lush green grass
and trees indicate a warm and possibly slightly breezy day. There are no...
PDF input
See the docs for more info.


import base64
from langchain.messages import HumanMessage

pdf_bytes = open("/path/to/your/test.pdf", "rb").read()
pdf_base64 = base64.b64encode(pdf_bytes).decode("utf-8")

message = HumanMessage(
    content=[
        {"type": "text", "text": "describe the document in a sentence"},
        {
            "type": "file",
            "source_type": "base64",
            "mime_type": "application/pdf",
            "data": pdf_base64,
        },
    ]
)
ai_msg = model.invoke([message])
Audio input
See the docs for more info.


import base64
from langchain.messages import HumanMessage

audio_bytes = open("/path/to/your/audio.mp3", "rb").read()
audio_base64 = base64.b64encode(audio_bytes).decode("utf-8")

message = HumanMessage(
    content=[
        {"type": "text", "text": "summarize this audio in a sentence"},
        {
            "type": "file",
            "source_type": "base64",
            "mime_type": "audio/mp3",
            "data": audio_base64,
        },
    ]
)
ai_msg = model.invoke([message])
Video input
See the docs for more info.


import base64
from langchain.messages import HumanMessage

video_bytes = open("/path/to/your/video.mp4", "rb").read()
video_base64 = base64.b64encode(video_bytes).decode("utf-8")

message = HumanMessage(
    content=[
        {
            "type": "text",
            "text": "describe what's in this video in a sentence",
        },
        {
            "type": "file",
            "source_type": "base64",
            "mime_type": "video/mp4",
            "data": video_base64,
        },
    ]
)
ai_msg = model.invoke([message])
You can also pass YouTube URLs directly:


from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage

model = ChatGoogleGenerativeAI(model="gemini-3-pro-preview")

message = HumanMessage(
    content=[
        {"type": "text", "text": "Summarize the video in 3 sentences."},
        {
            "type": "media",
            "file_uri": "https://www.youtube.com/watch?v=dQw4w9WgXcQ",
            "mime_type": "video/mp4",
        },
    ]
)
response = model.invoke([message])
print(response.text)
Image generation
See the docs for more info.

Audio generation
See the docs for more info.

File upload
You can also upload files to Google's servers and reference them by URI.

This works for PDFs, images, videos, and audio files.


import time
from google import genai
from langchain.messages import HumanMessage

client = genai.Client()

myfile = client.files.upload(file="/path/to/your/sample.pdf")
while myfile.state.name == "PROCESSING":
    time.sleep(2)
    myfile = client.files.get(name=myfile.name)

message = HumanMessage(
    content=[
        {"type": "text", "text": "What is in the document?"},
        {
            "type": "media",
            "file_uri": myfile.uri,
            "mime_type": "application/pdf",
        },
    ]
)
ai_msg = model.invoke([message])
Thinking
See the docs for more info.

For thinking models, you have the option to adjust the number of internal thinking tokens used (thinking_budget) or to disable thinking altogether. Note that not all models allow disabling thinking.

See the Gemini API docs for more details on thinking models.

To see a thinking model's thoughts, set include_thoughts=True to have the model's reasoning summaries included in the response.


model = ChatGoogleGenerativeAI(
    model="gemini-3-pro-preview",
    include_thoughts=True,
)
ai_msg = model.invoke("How many 'r's are in the word 'strawberry'?")
Google search
See the docs for more info.


from langchain_google_genai import ChatGoogleGenerativeAI

model = ChatGoogleGenerativeAI(model="gemini-3-pro-preview")

model_with_search = model.bind_tools([{"google_search": {}}])
response = model_with_search.invoke(
    "When is the next total solar eclipse in US?"
)

response.content_blocks
Code execution
See the docs for more info.


from langchain_google_genai import ChatGoogleGenerativeAI

model = ChatGoogleGenerativeAI(model="gemini-3-pro-preview")

model_with_code_interpreter = model.bind_tools([{"code_execution": {}}])
response = model_with_code_interpreter.invoke("Use Python to calculate 3^3.")

response.content_blocks

[{'type': 'server_tool_call',
  'name': 'code_interpreter',
  'args': {'code': 'print(3**3)', 'language': <Language.PYTHON: 1>},
  'id': '...'},
 {'type': 'server_tool_result',
  'tool_call_id': '',
  'status': 'success',
  'output': '27\n',
  'extras': {'block_type': 'code_execution_result',
   'outcome': <Outcome.OUTCOME_OK: 1>}},
 {'type': 'text', 'text': 'The calculation of 3 to the power of 3 is 27.'}]
Token usage
See the docs for more info.


ai_msg = model.invoke(messages)
ai_msg.usage_metadata

{"input_tokens": 18, "output_tokens": 5, "total_tokens": 23}
Safety settings
Gemini models have default safety settings that can be overridden. If you are receiving lots of "Safety Warnings" from your models, you can try tweaking the safety_settings attribute of the model. For example, to turn off safety blocking for dangerous content, you can construct your LLM as follows:


from langchain_google_genai import (
    ChatGoogleGenerativeAI,
    HarmBlockThreshold,
    HarmCategory,
)

llm = ChatGoogleGenerativeAI(
    model="gemini-3-pro-preview",
    safety_settings={
        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
    },
)
For an enumeration of the categories and thresholds available, see Google's safety setting types.

Context caching
See the docs for more info.

Context caching allows you to store and reuse content (e.g., PDFs, images) for faster processing. The cached_content parameter accepts a cache name created via the Google Generative AI API.

See the Gemini docs for more details on cached content.

Below are two examples: caching a single file directly and caching multiple files using Part.

Single file example

This caches a single file and queries it.


from google import genai
from google.genai import types
import time
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.messages import HumanMessage

client = genai.Client()

# Upload file
file = client.files.upload(file="path/to/your/file")
while file.state.name == "PROCESSING":
    time.sleep(2)
    file = client.files.get(name=file.name)

# Create cache
model = "gemini-3-pro-preview"
cache = client.caches.create(
    model=model,
    config=types.CreateCachedContentConfig(
        display_name="Cached Content",
        system_instruction=(
            "You are an expert content analyzer, and your job is to answer "
            "the user's query based on the file you have access to."
        ),
        contents=[file],
        ttl="300s",
    ),
)

# Query with LangChain
llm = ChatGoogleGenerativeAI(
    model=model,
    cached_content=cache.name,
)
message = HumanMessage(content="Summarize the main points of the content.")
llm.invoke([message])
Multiple files example

This caches two files using Part and queries them together.


from google import genai
from google.genai.types import CreateCachedContentConfig, Content, Part
import time
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.messages import HumanMessage

client = genai.Client()

# Upload files
file_1 = client.files.upload(file="./file1")
while file_1.state.name == "PROCESSING":
    time.sleep(2)
    file_1 = client.files.get(name=file_1.name)

file_2 = client.files.upload(file="./file2")
while file_2.state.name == "PROCESSING":
    time.sleep(2)
    file_2 = client.files.get(name=file_2.name)

# Create cache with multiple files
contents = [
    Content(
        role="user",
        parts=[
            Part.from_uri(file_uri=file_1.uri, mime_type=file_1.mime_type),
            Part.from_uri(file_uri=file_2.uri, mime_type=file_2.mime_type),
        ],
    )
]
model = "gemini-3-pro-preview"
cache = client.caches.create(
    model=model,
    config=CreateCachedContentConfig(
        display_name="Cached Contents",
        system_instruction=(
            "You are an expert content analyzer, and your job is to answer "
            "the user's query based on the files you have access to."
        ),
        contents=contents,
        ttl="300s",
    ),
)

# Query with LangChain
llm = ChatGoogleGenerativeAI(
    model=model,
    cached_content=cache.name,
)
message = HumanMessage(
    content="Provide a summary of the key information across both files."
)
llm.invoke([message])
Response metadata

ai_msg = model.invoke(messages)
ai_msg.response_metadata

{
    "prompt_feedback": {"block_reason": 0, "safety_ratings": []},
    "finish_reason": "STOP",
    "safety_ratings": [
        {
            "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
            "probability": "NEGLIGIBLE",
            "blocked": False,
        },
        {
            "category": "HARM_CATEGORY_HATE_SPEECH",
            "probability": "NEGLIGIBLE",
            "blocked": False,
        },
        {
            "category": "HARM_CATEGORY_HARASSMENT",
            "probability": "NEGLIGIBLE",
            "blocked": False,
        },
        {
            "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
            "probability": "NEGLIGIBLE",
            "blocked": False,
        },
    ],
}
METHOD	DESCRIPTION
get_name	Get the name of the Runnable.
get_input_schema	Get a Pydantic model that can be used to validate input to the Runnable.
get_input_jsonschema	Get a JSON schema that represents the input to the Runnable.
get_output_schema	Get a Pydantic model that can be used to validate output to the Runnable.
get_output_jsonschema	Get a JSON schema that represents the output of the Runnable.
config_schema	The type of config this Runnable accepts specified as a Pydantic model.
get_config_jsonschema	Get a JSON schema that represents the config of the Runnable.
get_graph	Return a graph representation of this Runnable.
get_prompts	Return a list of prompts used by this Runnable.
__or__	Runnable "or" operator.
__ror__	Runnable "reverse-or" operator.
pipe	Pipe Runnable objects.
pick	Pick keys from the output dict of this Runnable.
assign	Assigns new fields to the dict output of this Runnable.
ainvoke	Transform a single input into an output.
batch	Default implementation runs invoke in parallel using a thread pool executor.
batch_as_completed	Run invoke in parallel on a list of inputs.
abatch	Default implementation runs ainvoke in parallel using asyncio.gather.
abatch_as_completed	Run ainvoke in parallel on a list of inputs.
stream	Default implementation of stream, which calls invoke.
astream	Default implementation of astream, which calls ainvoke.
astream_log	Stream all output from a Runnable, as reported to the callback system.
astream_events	Generate a stream of events.
transform	Transform inputs to outputs.
atransform	Transform inputs to outputs.
bind	Bind arguments to a Runnable, returning a new Runnable.
with_config	Bind config to a Runnable, returning a new Runnable.
with_listeners	Bind lifecycle listeners to a Runnable, returning a new Runnable.
with_alisteners	Bind async lifecycle listeners to a Runnable.
with_types	Bind input and output types to a Runnable, returning a new Runnable.
with_retry	Create a new Runnable that retries the original Runnable on exceptions.
map	Return a new Runnable that maps a list of inputs to a list of outputs.
with_fallbacks	Add fallbacks to a Runnable, returning a new Runnable.
as_tool	Create a BaseTool from a Runnable.
get_lc_namespace	Get the namespace of the LangChain object.
lc_id	Return a unique identifier for this class for serialization purposes.
to_json	Serialize the Runnable to JSON.
to_json_not_implemented	Serialize a "not implemented" object.
configurable_fields	Configure particular Runnable fields at runtime.
configurable_alternatives	Configure alternatives for Runnable objects that can be set at runtime.
set_verbose	If verbose is None, set it.
generate_prompt	Pass a sequence of prompts to the model and return model generations.
agenerate_prompt	Asynchronously pass a sequence of prompts and return model generations.
get_token_ids	Return the ordered IDs of the tokens in a text.
get_num_tokens_from_messages	Get the number of tokens in the messages.
generate	Pass a sequence of prompts to the model and return model generations.
agenerate	Asynchronously pass a sequence of prompts to a model and return generations.
dict	Return a dictionary of the LLM.
__init__	Needed for arg validation.
is_lc_serializable	Is this class serializable?
build_extra	Build extra kwargs from additional params that were passed in.
validate_environment	Validates params and passes them to google-generativeai package.
invoke	Override invoke on ChatGoogleGenerativeAI to add code_execution.
get_num_tokens	Get the number of tokens present in the text. Uses the model's tokenizer.
with_structured_output	Model wrapper that returns outputs formatted to match the given schema.
bind_tools	Bind tool-like objects to this chat model.
 name class-attribute instance-attribute ¬∂

name: str | None = None
The name of the Runnable. Used for debugging and tracing.

 InputType property ¬∂

InputType: TypeAlias
Get the input type for this Runnable.

 OutputType property ¬∂

OutputType: Any
Get the output type for this Runnable.

 input_schema property ¬∂

input_schema: type[BaseModel]
The type of input this Runnable accepts specified as a Pydantic model.

 output_schema property ¬∂

output_schema: type[BaseModel]
Output schema.

The type of output this Runnable produces specified as a Pydantic model.

 config_specs property ¬∂

config_specs: list[ConfigurableFieldSpec]
List configurable fields for this Runnable.

 lc_attributes property ¬∂

lc_attributes: dict
List of attribute names that should be included in the serialized kwargs.

These attributes must be accepted by the constructor.

Default is an empty dictionary.

 cache class-attribute instance-attribute ¬∂

cache: BaseCache | bool | None = Field(default=None, exclude=True)
Whether to cache the response.

If True, will use the global cache.
If False, will not use a cache
If None, will use the global cache if it's set, otherwise no cache.
If instance of BaseCache, will use the provided cache.
Caching is not currently supported for streaming methods of models.

 verbose class-attribute instance-attribute ¬∂

verbose: bool = Field(default_factory=_get_verbosity, exclude=True, repr=False)
Whether to print out response text.

 callbacks class-attribute instance-attribute ¬∂

callbacks: Callbacks = Field(default=None, exclude=True)
Callbacks to add to the run trace.

 tags class-attribute instance-attribute ¬∂

tags: list[str] | None = Field(default=None, exclude=True)
Tags to add to the run trace.

 metadata class-attribute instance-attribute ¬∂

metadata: dict[str, Any] | None = Field(default=None, exclude=True)
Metadata to add to the run trace.

 custom_get_token_ids class-attribute instance-attribute ¬∂

custom_get_token_ids: Callable[[str], list[int]] | None = Field(
    default=None, exclude=True
)
Optional encoder to use for counting tokens.

 rate_limiter class-attribute instance-attribute ¬∂

rate_limiter: BaseRateLimiter | None = Field(default=None, exclude=True)
An optional rate limiter to use for limiting the number of requests.

 disable_streaming class-attribute instance-attribute ¬∂

disable_streaming: bool | Literal['tool_calling'] = False
Whether to disable streaming for this model.

If streaming is bypassed, then stream/astream/astream_events will defer to invoke/ainvoke.

If True, will always bypass streaming case.
If 'tool_calling', will bypass streaming case only when the model is called with a tools keyword argument. In other words, LangChain will automatically switch to non-streaming behavior (invoke) only when the tools argument is provided. This offers the best of both worlds.
If False (Default), will always use streaming case if available.
The main reason for this flag is that code might be written using stream and a user may want to swap out a given model for another model whose the implementation does not properly support streaming.

 output_version class-attribute instance-attribute ¬∂

output_version: str | None = Field(
    default_factory=from_env("LC_OUTPUT_VERSION", default=None)
)
Version of AIMessage output format to store in message content.

AIMessage.content_blocks will lazily parse the contents of content into a standard format. This flag can be used to additionally store the standard format in message content, e.g., for serialization purposes.

Supported values:

'v0': provider-specific format in content (can lazily-parse with content_blocks)
'v1': standardized format in content (consistent with content_blocks)
Partner packages (e.g., langchain-openai) can also use this field to roll out new content formats in a backward-compatible way.

Added in langchain-core 1.0.0

 profile class-attribute instance-attribute ¬∂

profile: ModelProfile | None = Field(default=None, exclude=True)
Profile detailing model capabilities.

Beta feature

This is a beta feature. The format of model profiles is subject to change.

If not specified, automatically loaded from the provider package on initialization if data is available.

Example profile data includes context window sizes, supported modalities, or support for tool calling, structured output, and other features.

Added in langchain-core 1.1.0

 model class-attribute instance-attribute ¬∂

model: str = Field(...)
Model name to use.

 google_api_key class-attribute instance-attribute ¬∂

google_api_key: SecretStr | None = Field(
    alias="api_key",
    default_factory=secret_from_env(["GOOGLE_API_KEY", "GEMINI_API_KEY"], default=None),
)
Google AI API key.

If not specified, will check the env vars GOOGLE_API_KEY and GEMINI_API_KEY with precedence given to GOOGLE_API_KEY.

 credentials class-attribute instance-attribute ¬∂

credentials: Any = None
The default custom credentials to use when making API calls.

If not provided, credentials will be ascertained from the GOOGLE_API_KEY or GEMINI_API_KEY env vars with precedence given to GOOGLE_API_KEY.

 temperature class-attribute instance-attribute ¬∂

temperature: float = 0.7
Run inference with this temperature.

Must be within [0.0, 2.0].

Gemini 3.0+ models

Setting temperature < 1.0 for Gemini 3.0+ models can cause infinite loops, degraded reasoning performance, and failure on complex tasks.

 top_p class-attribute instance-attribute ¬∂

top_p: float | None = None
Decode using nucleus sampling.

Consider the smallest set of tokens whose probability sum is at least top_p.

Must be within [0.0, 1.0].

 top_k class-attribute instance-attribute ¬∂

top_k: int | None = None
Decode using top-k sampling: consider the set of top_k most probable tokens.

Must be positive.

 max_output_tokens class-attribute instance-attribute ¬∂

max_output_tokens: int | None = Field(default=None, alias='max_tokens')
Maximum number of tokens to include in a candidate.

Must be greater than zero.

If unset, will use the model's default value, which varies by model.

See docs for model-specific limits.

To constrain the number of thinking tokens to use when generating a response, see the thinking_budget parameter.

 n class-attribute instance-attribute ¬∂

n: int = 1
Number of chat completions to generate for each prompt.

Note that the API may not return the full n completions if duplicates are generated.

 max_retries class-attribute instance-attribute ¬∂

max_retries: int = Field(default=6, alias='retries')
The maximum number of retries to make when generating.

 timeout class-attribute instance-attribute ¬∂

timeout: float | None = Field(default=None, alias='request_timeout')
The maximum number of seconds to wait for a response.

 client_options class-attribute instance-attribute ¬∂

client_options: dict | None = Field(default=None)
A dictionary of client options to pass to the Google API client.

Example: api_endpoint

Warning

If both client_options['api_endpoint'] and base_url are specified, the api_endpoint in client_options takes precedence.

 base_url class-attribute instance-attribute ¬∂

base_url: str | None = Field(default=None)
Base URL to use for the API client.

This is a convenience alias for client_options['api_endpoint'].

REST transport (transport="rest"): Accepts full URLs with paths

https://api.example.com/v1/path
https://webhook.site/unique-path
gRPC transports (transport="grpc" or transport="grpc_asyncio"): Only accepts hostname:port format

api.example.com:443
custom.googleapis.com:443
https://api.example.com (auto-formatted to api.example.com:443)
NOT https://webhook.site/path (paths are not supported in gRPC)
NOT api.example.com/path (paths are not supported in gRPC)
Warning

If client_options already contains an api_endpoint, this parameter will be ignored in favor of the existing value.

 transport class-attribute instance-attribute ¬∂

transport: str | None = Field(default=None, alias='api_transport')
A string, one of: ['rest', 'grpc', 'grpc_asyncio'].

The Google client library defaults to 'grpc' for sync clients.

For async clients, 'rest' is converted to 'grpc_asyncio' unless a custom endpoint is specified.

 additional_headers class-attribute instance-attribute ¬∂

additional_headers: dict[str, str] | None = Field(default=None)
Key-value dictionary representing additional headers for the model call

 response_modalities class-attribute instance-attribute ¬∂

response_modalities: list[Modality] | None = Field(default=None)
A list of modalities of the response

 media_resolution class-attribute instance-attribute ¬∂

media_resolution: MediaResolution | None = Field(default=None)
Media resolution for the input media.

May be defined at the individual part level, allowing for mixed-resolution requests (e.g., images and videos of different resolutions in the same request).

May be 'low', 'medium', or 'high'.

Can be set either per-part or globally for all media inputs in the request. To set globally, set in the generation_config.

Model compatibility

Setting per-part media resolution requests to Gemini 2.5 models is not supported.

 thinking_budget class-attribute instance-attribute ¬∂

thinking_budget: int | None = Field(default=None)
Indicates the thinking budget in tokens.

Used to disable thinking for supported models (when set to 0) or to constrain the number of tokens used for thinking.

Dynamic thinking (allowing the model to decide how many tokens to use) is enabled when set to -1.

More information, including per-model limits, can be found in the Gemini API docs.

 include_thoughts class-attribute instance-attribute ¬∂

include_thoughts: bool | None = Field(default=None)
Indicates whether to include thoughts in the response.

Note

This parameter is only applicable for models that support thinking.

This does not disable thinking; to disable thinking, set thinking_budget to 0. for supported models. See the thinking_budget parameter for more details.

 safety_settings class-attribute instance-attribute ¬∂

safety_settings: dict[HarmCategory, HarmBlockThreshold] | None = None
Default safety settings to use for all generations.

Example


from google.generativeai.types.safety_types import HarmBlockThreshold, HarmCategory

safety_settings = {
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
}
 thinking_level class-attribute instance-attribute ¬∂

thinking_level: Literal['low', 'high'] | None = Field(default=None)
Indicates the thinking level.

Supported values
'low': Minimizes latency and cost.
'high': Maximizes reasoning depth.
Replaces thinking_budget

thinking_budget is deprecated for Gemini 3+ models. If both parameters are provided, thinking_level takes precedence.

If left unspecified, the model's default thinking level is used. For Gemini 3+, this defaults to 'high'.

 convert_system_message_to_human class-attribute instance-attribute ¬∂

convert_system_message_to_human: bool = False
Whether to merge any leading SystemMessage into the following HumanMessage.

Gemini does not support system messages; any unsupported messages will raise an error.

 response_mime_type class-attribute instance-attribute ¬∂

response_mime_type: str | None = None
Output response MIME type of the generated candidate text.

Supported MIME types
'text/plain': (default) Text output.
'application/json': JSON response in the candidates.
'text/x.enum': Enum in plain text. (legacy; use JSON schema output instead)
Note

The model also needs to be prompted to output the appropriate response type, otherwise the behavior is undefined.

(In other words, simply setting this param doesn't force the model to comply; it only tells the model the kind of output expected. You still need to prompt it correctly.)

 response_schema class-attribute instance-attribute ¬∂

response_schema: dict[str, Any] | None = None
Enforce a schema to the output.

The format of the dictionary should follow Open API schema.

Has JSON Schema support including:

anyOf for unions
$ref for recursive schemas
Output property ordering
Minimum/maximum constraints
Streaming of partial JSON chunks
Refer to the Gemini API docs for more details.

 cached_content class-attribute instance-attribute ¬∂

cached_content: str | None = None
The name of the cached content used as context to serve the prediction.

Note

Only used in explicit caching, where users can have control over caching (e.g. what content to cache) and enjoy guaranteed cost savings. Format: cachedContents/{cachedContent}.

 stop class-attribute instance-attribute ¬∂

stop: list[str] | None = None
Stop sequences for the model.

 streaming class-attribute instance-attribute ¬∂

streaming: bool | None = None
Whether to stream responses from the model.

 model_kwargs class-attribute instance-attribute ¬∂

model_kwargs: dict[str, Any] = Field(default_factory=dict)
Holds any unexpected initialization parameters.

 get_name ¬∂

get_name(suffix: str | None = None, *, name: str | None = None) -> str
Get the name of the Runnable.

PARAMETER	DESCRIPTION
suffix	An optional suffix to append to the name.
TYPE: str | NoneDEFAULT: None

name	An optional name to use instead of the Runnable's name.
TYPE: str | NoneDEFAULT: None

RETURNS	DESCRIPTION
str	The name of the Runnable.
 get_input_schema ¬∂

get_input_schema(config: RunnableConfig | None = None) -> type[BaseModel]
Get a Pydantic model that can be used to validate input to the Runnable.

Runnable objects that leverage the configurable_fields and configurable_alternatives methods will have a dynamic input schema that depends on which configuration the Runnable is invoked with.

This method allows to get an input schema for a specific configuration.

PARAMETER	DESCRIPTION
config	A config to use when generating the schema.
TYPE: RunnableConfig | NoneDEFAULT: None

RETURNS	DESCRIPTION
type[BaseModel]	A Pydantic model that can be used to validate input.
 get_input_jsonschema ¬∂

get_input_jsonschema(config: RunnableConfig | None = None) -> dict[str, Any]
Get a JSON schema that represents the input to the Runnable.

PARAMETER	DESCRIPTION
config	A config to use when generating the schema.
TYPE: RunnableConfig | NoneDEFAULT: None

RETURNS	DESCRIPTION
dict[str, Any]	A JSON schema that represents the input to the Runnable.
Example

from langchain_core.runnables import RunnableLambda


def add_one(x: int) -> int:
    return x + 1


runnable = RunnableLambda(add_one)

print(runnable.get_input_jsonschema())
Added in langchain-core 0.3.0

 get_output_schema ¬∂

get_output_schema(config: RunnableConfig | None = None) -> type[BaseModel]
Get a Pydantic model that can be used to validate output to the Runnable.

Runnable objects that leverage the configurable_fields and configurable_alternatives methods will have a dynamic output schema that depends on which configuration the Runnable is invoked with.

This method allows to get an output schema for a specific configuration.

PARAMETER	DESCRIPTION
config	A config to use when generating the schema.
TYPE: RunnableConfig | NoneDEFAULT: None

RETURNS	DESCRIPTION
type[BaseModel]	A Pydantic model that can be used to validate output.
 get_output_jsonschema ¬∂

get_output_jsonschema(config: RunnableConfig | None = None) -> dict[str, Any]
Get a JSON schema that represents the output of the Runnable.

PARAMETER	DESCRIPTION
config	A config to use when generating the schema.
TYPE: RunnableConfig | NoneDEFAULT: None

RETURNS	DESCRIPTION
dict[str, Any]	A JSON schema that represents the output of the Runnable.
Example

from langchain_core.runnables import RunnableLambda


def add_one(x: int) -> int:
    return x + 1


runnable = RunnableLambda(add_one)

print(runnable.get_output_jsonschema())
Added in langchain-core 0.3.0

 config_schema ¬∂

config_schema(*, include: Sequence[str] | None = None) -> type[BaseModel]
The type of config this Runnable accepts specified as a Pydantic model.

To mark a field as configurable, see the configurable_fields and configurable_alternatives methods.

PARAMETER	DESCRIPTION
include	A list of fields to include in the config schema.
TYPE: Sequence[str] | NoneDEFAULT: None

RETURNS	DESCRIPTION
type[BaseModel]	A Pydantic model that can be used to validate config.
 get_config_jsonschema ¬∂

get_config_jsonschema(*, include: Sequence[str] | None = None) -> dict[str, Any]
Get a JSON schema that represents the config of the Runnable.

PARAMETER	DESCRIPTION
include	A list of fields to include in the config schema.
TYPE: Sequence[str] | NoneDEFAULT: None

RETURNS	DESCRIPTION
dict[str, Any]	A JSON schema that represents the config of the Runnable.
Added in langchain-core 0.3.0

 get_graph ¬∂

get_graph(config: RunnableConfig | None = None) -> Graph
Return a graph representation of this Runnable.

 get_prompts ¬∂

get_prompts(config: RunnableConfig | None = None) -> list[BasePromptTemplate]
Return a list of prompts used by this Runnable.

 __or__ ¬∂

__or__(
    other: Runnable[Any, Other]
    | Callable[[Iterator[Any]], Iterator[Other]]
    | Callable[[AsyncIterator[Any]], AsyncIterator[Other]]
    | Callable[[Any], Other]
    | Mapping[str, Runnable[Any, Other] | Callable[[Any], Other] | Any],
) -> RunnableSerializable[Input, Other]
Runnable "or" operator.

Compose this Runnable with another object to create a RunnableSequence.

PARAMETER	DESCRIPTION
other	Another Runnable or a Runnable-like object.
TYPE: Runnable[Any, Other] | Callable[[Iterator[Any]], Iterator[Other]] | Callable[[AsyncIterator[Any]], AsyncIterator[Other]] | Callable[[Any], Other] | Mapping[str, Runnable[Any, Other] | Callable[[Any], Other] | Any]

RETURNS	DESCRIPTION
RunnableSerializable[Input, Other]	A new Runnable.
 __ror__ ¬∂

__ror__(
    other: Runnable[Other, Any]
    | Callable[[Iterator[Other]], Iterator[Any]]
    | Callable[[AsyncIterator[Other]], AsyncIterator[Any]]
    | Callable[[Other], Any]
    | Mapping[str, Runnable[Other, Any] | Callable[[Other], Any] | Any],
) -> RunnableSerializable[Other, Output]
Runnable "reverse-or" operator.

Compose this Runnable with another object to create a RunnableSequence.

PARAMETER	DESCRIPTION
other	Another Runnable or a Runnable-like object.
TYPE: Runnable[Other, Any] | Callable[[Iterator[Other]], Iterator[Any]] | Callable[[AsyncIterator[Other]], AsyncIterator[Any]] | Callable[[Other], Any] | Mapping[str, Runnable[Other, Any] | Callable[[Other], Any] | Any]

RETURNS	DESCRIPTION
RunnableSerializable[Other, Output]	A new Runnable.
 pipe ¬∂

pipe(
    *others: Runnable[Any, Other] | Callable[[Any], Other], name: str | None = None
) -> RunnableSerializable[Input, Other]
Pipe Runnable objects.

Compose this Runnable with Runnable-like objects to make a RunnableSequence.

Equivalent to RunnableSequence(self, *others) or self | others[0] | ...

Example

from langchain_core.runnables import RunnableLambda


def add_one(x: int) -> int:
    return x + 1


def mul_two(x: int) -> int:
    return x * 2


runnable_1 = RunnableLambda(add_one)
runnable_2 = RunnableLambda(mul_two)
sequence = runnable_1.pipe(runnable_2)
# Or equivalently:
# sequence = runnable_1 | runnable_2
# sequence = RunnableSequence(first=runnable_1, last=runnable_2)
sequence.invoke(1)
await sequence.ainvoke(1)
# -> 4

sequence.batch([1, 2, 3])
await sequence.abatch([1, 2, 3])
# -> [4, 6, 8]
PARAMETER	DESCRIPTION
*others	Other Runnable or Runnable-like objects to compose
TYPE: Runnable[Any, Other] | Callable[[Any], Other]DEFAULT: ()

name	An optional name for the resulting RunnableSequence.
TYPE: str | NoneDEFAULT: None

RETURNS	DESCRIPTION
RunnableSerializable[Input, Other]	A new Runnable.
 pick ¬∂

pick(keys: str | list[str]) -> RunnableSerializable[Any, Any]
Pick keys from the output dict of this Runnable.

Pick a single key


import json

from langchain_core.runnables import RunnableLambda, RunnableMap

as_str = RunnableLambda(str)
as_json = RunnableLambda(json.loads)
chain = RunnableMap(str=as_str, json=as_json)

chain.invoke("[1, 2, 3]")
# -> {"str": "[1, 2, 3]", "json": [1, 2, 3]}

json_only_chain = chain.pick("json")
json_only_chain.invoke("[1, 2, 3]")
# -> [1, 2, 3]
Pick a list of keys


from typing import Any

import json

from langchain_core.runnables import RunnableLambda, RunnableMap

as_str = RunnableLambda(str)
as_json = RunnableLambda(json.loads)


def as_bytes(x: Any) -> bytes:
    return bytes(x, "utf-8")


chain = RunnableMap(
    str=as_str, json=as_json, bytes=RunnableLambda(as_bytes)
)

chain.invoke("[1, 2, 3]")
# -> {"str": "[1, 2, 3]", "json": [1, 2, 3], "bytes": b"[1, 2, 3]"}

json_and_bytes_chain = chain.pick(["json", "bytes"])
json_and_bytes_chain.invoke("[1, 2, 3]")
# -> {"json": [1, 2, 3], "bytes": b"[1, 2, 3]"}
PARAMETER	DESCRIPTION
keys	A key or list of keys to pick from the output dict.
TYPE: str | list[str]

RETURNS	DESCRIPTION
RunnableSerializable[Any, Any]	a new Runnable.
 assign ¬∂

assign(
    **kwargs: Runnable[dict[str, Any], Any]
    | Callable[[dict[str, Any]], Any]
    | Mapping[str, Runnable[dict[str, Any], Any] | Callable[[dict[str, Any]], Any]],
) -> RunnableSerializable[Any, Any]
Assigns new fields to the dict output of this Runnable.


from langchain_core.language_models.fake import FakeStreamingListLLM
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import SystemMessagePromptTemplate
from langchain_core.runnables import Runnable
from operator import itemgetter

prompt = (
    SystemMessagePromptTemplate.from_template("You are a nice assistant.")
    + "{question}"
)
model = FakeStreamingListLLM(responses=["foo-lish"])

chain: Runnable = prompt | model | {"str": StrOutputParser()}

chain_with_assign = chain.assign(hello=itemgetter("str") | model)

print(chain_with_assign.input_schema.model_json_schema())
# {'title': 'PromptInput', 'type': 'object', 'properties':
{'question': {'title': 'Question', 'type': 'string'}}}
print(chain_with_assign.output_schema.model_json_schema())
# {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':
{'str': {'title': 'Str',
'type': 'string'}, 'hello': {'title': 'Hello', 'type': 'string'}}}
PARAMETER	DESCRIPTION
**kwargs	A mapping of keys to Runnable or Runnable-like objects that will be invoked with the entire output dict of this Runnable.
TYPE: Runnable[dict[str, Any], Any] | Callable[[dict[str, Any]], Any] | Mapping[str, Runnable[dict[str, Any], Any] | Callable[[dict[str, Any]], Any]]DEFAULT: {}

RETURNS	DESCRIPTION
RunnableSerializable[Any, Any]	A new Runnable.
 ainvoke async ¬∂

ainvoke(
    input: LanguageModelInput,
    config: RunnableConfig | None = None,
    *,
    stop: list[str] | None = None,
    **kwargs: Any,
) -> AIMessage
Transform a single input into an output.

PARAMETER	DESCRIPTION
input	The input to the Runnable.
TYPE: Input

config	A config to use when invoking the Runnable.
The config supports standard keys like 'tags', 'metadata' for tracing purposes, 'max_concurrency' for controlling how much work to do in parallel, and other keys.

Please refer to RunnableConfig for more details.

TYPE: RunnableConfig | NoneDEFAULT: None

RETURNS	DESCRIPTION
Output	The output of the Runnable.
 batch ¬∂

batch(
    inputs: list[Input],
    config: RunnableConfig | list[RunnableConfig] | None = None,
    *,
    return_exceptions: bool = False,
    **kwargs: Any | None,
) -> list[Output]
Default implementation runs invoke in parallel using a thread pool executor.

The default implementation of batch works well for IO bound runnables.

Subclasses must override this method if they can batch more efficiently; e.g., if the underlying Runnable uses an API which supports a batch mode.

PARAMETER	DESCRIPTION
inputs	A list of inputs to the Runnable.
TYPE: list[Input]

config	A config to use when invoking the Runnable. The config supports standard keys like 'tags', 'metadata' for tracing purposes, 'max_concurrency' for controlling how much work to do in parallel, and other keys.
Please refer to RunnableConfig for more details.

TYPE: RunnableConfig | list[RunnableConfig] | NoneDEFAULT: None

return_exceptions	Whether to return exceptions instead of raising them.
TYPE: boolDEFAULT: False

**kwargs	Additional keyword arguments to pass to the Runnable.
TYPE: Any | NoneDEFAULT: {}

RETURNS	DESCRIPTION
list[Output]	A list of outputs from the Runnable.
 batch_as_completed ¬∂

batch_as_completed(
    inputs: Sequence[Input],
    config: RunnableConfig | Sequence[RunnableConfig] | None = None,
    *,
    return_exceptions: bool = False,
    **kwargs: Any | None,
) -> Iterator[tuple[int, Output | Exception]]
Run invoke in parallel on a list of inputs.

Yields results as they complete.

PARAMETER	DESCRIPTION
inputs	A list of inputs to the Runnable.
TYPE: Sequence[Input]

config	A config to use when invoking the Runnable.
The config supports standard keys like 'tags', 'metadata' for tracing purposes, 'max_concurrency' for controlling how much work to do in parallel, and other keys.

Please refer to RunnableConfig for more details.

TYPE: RunnableConfig | Sequence[RunnableConfig] | NoneDEFAULT: None

return_exceptions	Whether to return exceptions instead of raising them.
TYPE: boolDEFAULT: False

**kwargs	Additional keyword arguments to pass to the Runnable.
TYPE: Any | NoneDEFAULT: {}

YIELDS	DESCRIPTION
tuple[int, Output | Exception]	Tuples of the index of the input and the output from the Runnable.
 abatch async ¬∂

abatch(
    inputs: list[Input],
    config: RunnableConfig | list[RunnableConfig] | None = None,
    *,
    return_exceptions: bool = False,
    **kwargs: Any | None,
) -> list[Output]
Default implementation runs ainvoke in parallel using asyncio.gather.

The default implementation of batch works well for IO bound runnables.

Subclasses must override this method if they can batch more efficiently; e.g., if the underlying Runnable uses an API which supports a batch mode.

PARAMETER	DESCRIPTION
inputs	A list of inputs to the Runnable.
TYPE: list[Input]

config	A config to use when invoking the Runnable.
The config supports standard keys like 'tags', 'metadata' for tracing purposes, 'max_concurrency' for controlling how much work to do in parallel, and other keys.

Please refer to RunnableConfig for more details.

TYPE: RunnableConfig | list[RunnableConfig] | NoneDEFAULT: None

return_exceptions	Whether to return exceptions instead of raising them.
TYPE: boolDEFAULT: False

**kwargs	Additional keyword arguments to pass to the Runnable.
TYPE: Any | NoneDEFAULT: {}

RETURNS	DESCRIPTION
list[Output]	A list of outputs from the Runnable.
 abatch_as_completed async ¬∂

abatch_as_completed(
    inputs: Sequence[Input],
    config: RunnableConfig | Sequence[RunnableConfig] | None = None,
    *,
    return_exceptions: bool = False,
    **kwargs: Any | None,
) -> AsyncIterator[tuple[int, Output | Exception]]
Run ainvoke in parallel on a list of inputs.

Yields results as they complete.

PARAMETER	DESCRIPTION
inputs	A list of inputs to the Runnable.
TYPE: Sequence[Input]

config	A config to use when invoking the Runnable.
The config supports standard keys like 'tags', 'metadata' for tracing purposes, 'max_concurrency' for controlling how much work to do in parallel, and other keys.

Please refer to RunnableConfig for more details.

TYPE: RunnableConfig | Sequence[RunnableConfig] | NoneDEFAULT: None

return_exceptions	Whether to return exceptions instead of raising them.
TYPE: boolDEFAULT: False

**kwargs	Additional keyword arguments to pass to the Runnable.
TYPE: Any | NoneDEFAULT: {}

YIELDS	DESCRIPTION
AsyncIterator[tuple[int, Output | Exception]]	A tuple of the index of the input and the output from the Runnable.
 stream ¬∂

stream(
    input: LanguageModelInput,
    config: RunnableConfig | None = None,
    *,
    stop: list[str] | None = None,
    **kwargs: Any,
) -> Iterator[AIMessageChunk]
Default implementation of stream, which calls invoke.

Subclasses must override this method if they support streaming output.

PARAMETER	DESCRIPTION
input	The input to the Runnable.
TYPE: Input

config	The config to use for the Runnable.
TYPE: RunnableConfig | NoneDEFAULT: None

**kwargs	Additional keyword arguments to pass to the Runnable.
TYPE: Any | NoneDEFAULT: {}

YIELDS	DESCRIPTION
Output	The output of the Runnable.
 astream async ¬∂

astream(
    input: LanguageModelInput,
    config: RunnableConfig | None = None,
    *,
    stop: list[str] | None = None,
    **kwargs: Any,
) -> AsyncIterator[AIMessageChunk]
Default implementation of astream, which calls ainvoke.

Subclasses must override this method if they support streaming output.

PARAMETER	DESCRIPTION
input	The input to the Runnable.
TYPE: Input

config	The config to use for the Runnable.
TYPE: RunnableConfig | NoneDEFAULT: None

**kwargs	Additional keyword arguments to pass to the Runnable.
TYPE: Any | NoneDEFAULT: {}

YIELDS	DESCRIPTION
AsyncIterator[Output]	The output of the Runnable.
 astream_log async ¬∂

astream_log(
    input: Any,
    config: RunnableConfig | None = None,
    *,
    diff: bool = True,
    with_streamed_output_list: bool = True,
    include_names: Sequence[str] | None = None,
    include_types: Sequence[str] | None = None,
    include_tags: Sequence[str] | None = None,
    exclude_names: Sequence[str] | None = None,
    exclude_types: Sequence[str] | None = None,
    exclude_tags: Sequence[str] | None = None,
    **kwargs: Any,
) -> AsyncIterator[RunLogPatch] | AsyncIterator[RunLog]
Stream all output from a Runnable, as reported to the callback system.

This includes all inner runs of LLMs, Retrievers, Tools, etc.

Output is streamed as Log objects, which include a list of Jsonpatch ops that describe how the state of the run has changed in each step, and the final state of the run.

The Jsonpatch ops can be applied in order to construct state.

PARAMETER	DESCRIPTION
input	The input to the Runnable.
TYPE: Any

config	The config to use for the Runnable.
TYPE: RunnableConfig | NoneDEFAULT: None

diff	Whether to yield diffs between each step or the current state.
TYPE: boolDEFAULT: True

with_streamed_output_list	Whether to yield the streamed_output list.
TYPE: boolDEFAULT: True

include_names	Only include logs with these names.
TYPE: Sequence[str] | NoneDEFAULT: None

include_types	Only include logs with these types.
TYPE: Sequence[str] | NoneDEFAULT: None

include_tags	Only include logs with these tags.
TYPE: Sequence[str] | NoneDEFAULT: None

exclude_names	Exclude logs with these names.
TYPE: Sequence[str] | NoneDEFAULT: None

exclude_types	Exclude logs with these types.
TYPE: Sequence[str] | NoneDEFAULT: None

exclude_tags	Exclude logs with these tags.
TYPE: Sequence[str] | NoneDEFAULT: None

**kwargs	Additional keyword arguments to pass to the Runnable.
TYPE: AnyDEFAULT: {}

YIELDS	DESCRIPTION
AsyncIterator[RunLogPatch] | AsyncIterator[RunLog]	A RunLogPatch or RunLog object.
 astream_events async ¬∂

astream_events(
    input: Any,
    config: RunnableConfig | None = None,
    *,
    version: Literal["v1", "v2"] = "v2",
    include_names: Sequence[str] | None = None,
    include_types: Sequence[str] | None = None,
    include_tags: Sequence[str] | None = None,
    exclude_names: Sequence[str] | None = None,
    exclude_types: Sequence[str] | None = None,
    exclude_tags: Sequence[str] | None = None,
    **kwargs: Any,
) -> AsyncIterator[StreamEvent]
Generate a stream of events.

Use to create an iterator over StreamEvent that provide real-time information about the progress of the Runnable, including StreamEvent from intermediate results.

A StreamEvent is a dictionary with the following schema:

event: Event names are of the format: on_[runnable_type]_(start|stream|end).
name: The name of the Runnable that generated the event.
run_id: Randomly generated ID associated with the given execution of the Runnable that emitted the event. A child Runnable that gets invoked as part of the execution of a parent Runnable is assigned its own unique ID.
parent_ids: The IDs of the parent runnables that generated the event. The root Runnable will have an empty list. The order of the parent IDs is from the root to the immediate parent. Only available for v2 version of the API. The v1 version of the API will return an empty list.
tags: The tags of the Runnable that generated the event.
metadata: The metadata of the Runnable that generated the event.
data: The data associated with the event. The contents of this field depend on the type of event. See the table below for more details.
Below is a table that illustrates some events that might be emitted by various chains. Metadata fields have been omitted from the table for brevity. Chain definitions have been included after the table.

Note

This reference table is for the v2 version of the schema.

event	name	chunk	input	output
on_chat_model_start	'[model name]'		{"messages": [[SystemMessage, HumanMessage]]}	
on_chat_model_stream	'[model name]'	AIMessageChunk(content="hello")		
on_chat_model_end	'[model name]'		{"messages": [[SystemMessage, HumanMessage]]}	AIMessageChunk(content="hello world")
on_llm_start	'[model name]'		{'input': 'hello'}	
on_llm_stream	'[model name]'	'Hello'		
on_llm_end	'[model name]'		'Hello human!'	
on_chain_start	'format_docs'			
on_chain_stream	'format_docs'	'hello world!, goodbye world!'		
on_chain_end	'format_docs'		[Document(...)]	'hello world!, goodbye world!'
on_tool_start	'some_tool'		{"x": 1, "y": "2"}	
on_tool_end	'some_tool'			{"x": 1, "y": "2"}
on_retriever_start	'[retriever name]'		{"query": "hello"}	
on_retriever_end	'[retriever name]'		{"query": "hello"}	[Document(...), ..]
on_prompt_start	'[template_name]'		{"question": "hello"}	
on_prompt_end	'[template_name]'		{"question": "hello"}	ChatPromptValue(messages: [SystemMessage, ...])
In addition to the standard events, users can also dispatch custom events (see example below).

Custom events will be only be surfaced with in the v2 version of the API!

A custom event has following format:

Attribute	Type	Description
name	str	A user defined name for the event.
data	Any	The data associated with the event. This can be anything, though we suggest making it JSON serializable.
Here are declarations associated with the standard events shown above:

format_docs:


def format_docs(docs: list[Document]) -> str:
    '''Format the docs.'''
    return ", ".join([doc.page_content for doc in docs])


format_docs = RunnableLambda(format_docs)
some_tool:


@tool
def some_tool(x: int, y: str) -> dict:
    '''Some_tool.'''
    return {"x": x, "y": y}
prompt:


template = ChatPromptTemplate.from_messages(
    [
        ("system", "You are Cat Agent 007"),
        ("human", "{question}"),
    ]
).with_config({"run_name": "my_template", "tags": ["my_template"]})
Example


from langchain_core.runnables import RunnableLambda


async def reverse(s: str) -> str:
    return s[::-1]


chain = RunnableLambda(func=reverse)

events = [
    event async for event in chain.astream_events("hello", version="v2")
]

# Will produce the following events
# (run_id, and parent_ids has been omitted for brevity):
[
    {
        "data": {"input": "hello"},
        "event": "on_chain_start",
        "metadata": {},
        "name": "reverse",
        "tags": [],
    },
    {
        "data": {"chunk": "olleh"},
        "event": "on_chain_stream",
        "metadata": {},
        "name": "reverse",
        "tags": [],
    },
    {
        "data": {"output": "olleh"},
        "event": "on_chain_end",
        "metadata": {},
        "name": "reverse",
        "tags": [],
    },
]
Dispatch custom event

from langchain_core.callbacks.manager import (
    adispatch_custom_event,
)
from langchain_core.runnables import RunnableLambda, RunnableConfig
import asyncio


async def slow_thing(some_input: str, config: RunnableConfig) -> str:
    """Do something that takes a long time."""
    await asyncio.sleep(1) # Placeholder for some slow operation
    await adispatch_custom_event(
        "progress_event",
        {"message": "Finished step 1 of 3"},
        config=config # Must be included for python < 3.10
    )
    await asyncio.sleep(1) # Placeholder for some slow operation
    await adispatch_custom_event(
        "progress_event",
        {"message": "Finished step 2 of 3"},
        config=config # Must be included for python < 3.10
    )
    await asyncio.sleep(1) # Placeholder for some slow operation
    return "Done"

slow_thing = RunnableLambda(slow_thing)

async for event in slow_thing.astream_events("some_input", version="v2"):
    print(event)
PARAMETER	DESCRIPTION
input	The input to the Runnable.
TYPE: Any

config	The config to use for the Runnable.
TYPE: RunnableConfig | NoneDEFAULT: None

version	The version of the schema to use, either 'v2' or 'v1'.
Users should use 'v2'.

'v1' is for backwards compatibility and will be deprecated in 0.4.0.

No default will be assigned until the API is stabilized. custom events will only be surfaced in 'v2'.

TYPE: Literal['v1', 'v2']DEFAULT: 'v2'

include_names	Only include events from Runnable objects with matching names.
TYPE: Sequence[str] | NoneDEFAULT: None

include_types	Only include events from Runnable objects with matching types.
TYPE: Sequence[str] | NoneDEFAULT: None

include_tags	Only include events from Runnable objects with matching tags.
TYPE: Sequence[str] | NoneDEFAULT: None

exclude_names	Exclude events from Runnable objects with matching names.
TYPE: Sequence[str] | NoneDEFAULT: None

exclude_types	Exclude events from Runnable objects with matching types.
TYPE: Sequence[str] | NoneDEFAULT: None

exclude_tags	Exclude events from Runnable objects with matching tags.
TYPE: Sequence[str] | NoneDEFAULT: None

**kwargs	Additional keyword arguments to pass to the Runnable.
These will be passed to astream_log as this implementation of astream_events is built on top of astream_log.

TYPE: AnyDEFAULT: {}

YIELDS	DESCRIPTION
AsyncIterator[StreamEvent]	An async stream of StreamEvent.
RAISES	DESCRIPTION
NotImplementedError	If the version is not 'v1' or 'v2'.
 transform ¬∂

transform(
    input: Iterator[Input], config: RunnableConfig | None = None, **kwargs: Any | None
) -> Iterator[Output]
Transform inputs to outputs.

Default implementation of transform, which buffers input and calls astream.

Subclasses must override this method if they can start producing output while input is still being generated.

PARAMETER	DESCRIPTION
input	An iterator of inputs to the Runnable.
TYPE: Iterator[Input]

config	The config to use for the Runnable.
TYPE: RunnableConfig | NoneDEFAULT: None

**kwargs	Additional keyword arguments to pass to the Runnable.
TYPE: Any | NoneDEFAULT: {}

YIELDS	DESCRIPTION
Output	The output of the Runnable.
 atransform async ¬∂

atransform(
    input: AsyncIterator[Input],
    config: RunnableConfig | None = None,
    **kwargs: Any | None,
) -> AsyncIterator[Output]
Transform inputs to outputs.

Default implementation of atransform, which buffers input and calls astream.

Subclasses must override this method if they can start producing output while input is still being generated.

PARAMETER	DESCRIPTION
input	An async iterator of inputs to the Runnable.
TYPE: AsyncIterator[Input]

config	The config to use for the Runnable.
TYPE: RunnableConfig | NoneDEFAULT: None

**kwargs	Additional keyword arguments to pass to the Runnable.
TYPE: Any | NoneDEFAULT: {}

YIELDS	DESCRIPTION
AsyncIterator[Output]	The output of the Runnable.
 bind ¬∂

bind(**kwargs: Any) -> Runnable[Input, Output]
Bind arguments to a Runnable, returning a new Runnable.

Useful when a Runnable in a chain requires an argument that is not in the output of the previous Runnable or included in the user input.

PARAMETER	DESCRIPTION
**kwargs	The arguments to bind to the Runnable.
TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
Runnable[Input, Output]	A new Runnable with the arguments bound.
Example

from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser

model = ChatOllama(model="llama3.1")

# Without bind
chain = model | StrOutputParser()

chain.invoke("Repeat quoted words exactly: 'One two three four five.'")
# Output is 'One two three four five.'

# With bind
chain = model.bind(stop=["three"]) | StrOutputParser()

chain.invoke("Repeat quoted words exactly: 'One two three four five.'")
# Output is 'One two'
 with_config ¬∂

with_config(
    config: RunnableConfig | None = None, **kwargs: Any
) -> Runnable[Input, Output]
Bind config to a Runnable, returning a new Runnable.

PARAMETER	DESCRIPTION
config	The config to bind to the Runnable.
TYPE: RunnableConfig | NoneDEFAULT: None

**kwargs	Additional keyword arguments to pass to the Runnable.
TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
Runnable[Input, Output]	A new Runnable with the config bound.
 with_listeners ¬∂

with_listeners(
    *,
    on_start: Callable[[Run], None]
    | Callable[[Run, RunnableConfig], None]
    | None = None,
    on_end: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None,
    on_error: Callable[[Run], None]
    | Callable[[Run, RunnableConfig], None]
    | None = None,
) -> Runnable[Input, Output]
Bind lifecycle listeners to a Runnable, returning a new Runnable.

The Run object contains information about the run, including its id, type, input, output, error, start_time, end_time, and any tags or metadata added to the run.

PARAMETER	DESCRIPTION
on_start	Called before the Runnable starts running, with the Run object.
TYPE: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | NoneDEFAULT: None

on_end	Called after the Runnable finishes running, with the Run object.
TYPE: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | NoneDEFAULT: None

on_error	Called if the Runnable throws an error, with the Run object.
TYPE: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | NoneDEFAULT: None

RETURNS	DESCRIPTION
Runnable[Input, Output]	A new Runnable with the listeners bound.
Example

from langchain_core.runnables import RunnableLambda
from langchain_core.tracers.schemas import Run

import time


def test_runnable(time_to_sleep: int):
    time.sleep(time_to_sleep)


def fn_start(run_obj: Run):
    print("start_time:", run_obj.start_time)


def fn_end(run_obj: Run):
    print("end_time:", run_obj.end_time)


chain = RunnableLambda(test_runnable).with_listeners(
    on_start=fn_start, on_end=fn_end
)
chain.invoke(2)
 with_alisteners ¬∂

with_alisteners(
    *,
    on_start: AsyncListener | None = None,
    on_end: AsyncListener | None = None,
    on_error: AsyncListener | None = None,
) -> Runnable[Input, Output]
Bind async lifecycle listeners to a Runnable.

Returns a new Runnable.

The Run object contains information about the run, including its id, type, input, output, error, start_time, end_time, and any tags or metadata added to the run.

PARAMETER	DESCRIPTION
on_start	Called asynchronously before the Runnable starts running, with the Run object.
TYPE: AsyncListener | NoneDEFAULT: None

on_end	Called asynchronously after the Runnable finishes running, with the Run object.
TYPE: AsyncListener | NoneDEFAULT: None

on_error	Called asynchronously if the Runnable throws an error, with the Run object.
TYPE: AsyncListener | NoneDEFAULT: None

RETURNS	DESCRIPTION
Runnable[Input, Output]	A new Runnable with the listeners bound.
Example

from langchain_core.runnables import RunnableLambda, Runnable
from datetime import datetime, timezone
import time
import asyncio


def format_t(timestamp: float) -> str:
    return datetime.fromtimestamp(timestamp, tz=timezone.utc).isoformat()


async def test_runnable(time_to_sleep: int):
    print(f"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}")
    await asyncio.sleep(time_to_sleep)
    print(f"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}")


async def fn_start(run_obj: Runnable):
    print(f"on start callback starts at {format_t(time.time())}")
    await asyncio.sleep(3)
    print(f"on start callback ends at {format_t(time.time())}")


async def fn_end(run_obj: Runnable):
    print(f"on end callback starts at {format_t(time.time())}")
    await asyncio.sleep(2)
    print(f"on end callback ends at {format_t(time.time())}")


runnable = RunnableLambda(test_runnable).with_alisteners(
    on_start=fn_start, on_end=fn_end
)


async def concurrent_runs():
    await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))


asyncio.run(concurrent_runs())
# Result:
# on start callback starts at 2025-03-01T07:05:22.875378+00:00
# on start callback starts at 2025-03-01T07:05:22.875495+00:00
# on start callback ends at 2025-03-01T07:05:25.878862+00:00
# on start callback ends at 2025-03-01T07:05:25.878947+00:00
# Runnable[2s]: starts at 2025-03-01T07:05:25.879392+00:00
# Runnable[3s]: starts at 2025-03-01T07:05:25.879804+00:00
# Runnable[2s]: ends at 2025-03-01T07:05:27.881998+00:00
# on end callback starts at 2025-03-01T07:05:27.882360+00:00
# Runnable[3s]: ends at 2025-03-01T07:05:28.881737+00:00
# on end callback starts at 2025-03-01T07:05:28.882428+00:00
# on end callback ends at 2025-03-01T07:05:29.883893+00:00
# on end callback ends at 2025-03-01T07:05:30.884831+00:00
 with_types ¬∂

with_types(
    *, input_type: type[Input] | None = None, output_type: type[Output] | None = None
) -> Runnable[Input, Output]
Bind input and output types to a Runnable, returning a new Runnable.

PARAMETER	DESCRIPTION
input_type	The input type to bind to the Runnable.
TYPE: type[Input] | NoneDEFAULT: None

output_type	The output type to bind to the Runnable.
TYPE: type[Output] | NoneDEFAULT: None

RETURNS	DESCRIPTION
Runnable[Input, Output]	A new Runnable with the types bound.
 with_retry ¬∂

with_retry(
    *,
    retry_if_exception_type: tuple[type[BaseException], ...] = (Exception,),
    wait_exponential_jitter: bool = True,
    exponential_jitter_params: ExponentialJitterParams | None = None,
    stop_after_attempt: int = 3,
) -> Runnable[Input, Output]
Create a new Runnable that retries the original Runnable on exceptions.

PARAMETER	DESCRIPTION
retry_if_exception_type	A tuple of exception types to retry on.
TYPE: tuple[type[BaseException], ...]DEFAULT: (Exception,)

wait_exponential_jitter	Whether to add jitter to the wait time between retries.
TYPE: boolDEFAULT: True

stop_after_attempt	The maximum number of attempts to make before giving up.
TYPE: intDEFAULT: 3

exponential_jitter_params	Parameters for tenacity.wait_exponential_jitter. Namely: initial, max, exp_base, and jitter (all float values).
TYPE: ExponentialJitterParams | NoneDEFAULT: None

RETURNS	DESCRIPTION
Runnable[Input, Output]	A new Runnable that retries the original Runnable on exceptions.
Example

from langchain_core.runnables import RunnableLambda

count = 0


def _lambda(x: int) -> None:
    global count
    count = count + 1
    if x == 1:
        raise ValueError("x is 1")
    else:
        pass


runnable = RunnableLambda(_lambda)
try:
    runnable.with_retry(
        stop_after_attempt=2,
        retry_if_exception_type=(ValueError,),
    ).invoke(1)
except ValueError:
    pass

assert count == 2
 map ¬∂

map() -> Runnable[list[Input], list[Output]]
Return a new Runnable that maps a list of inputs to a list of outputs.

Calls invoke with each input.

RETURNS	DESCRIPTION
Runnable[list[Input], list[Output]]	A new Runnable that maps a list of inputs to a list of outputs.
Example

from langchain_core.runnables import RunnableLambda


def _lambda(x: int) -> int:
    return x + 1


runnable = RunnableLambda(_lambda)
print(runnable.map().invoke([1, 2, 3]))  # [2, 3, 4]
 with_fallbacks ¬∂

with_fallbacks(
    fallbacks: Sequence[Runnable[Input, Output]],
    *,
    exceptions_to_handle: tuple[type[BaseException], ...] = (Exception,),
    exception_key: str | None = None,
) -> RunnableWithFallbacks[Input, Output]
Add fallbacks to a Runnable, returning a new Runnable.

The new Runnable will try the original Runnable, and then each fallback in order, upon failures.

PARAMETER	DESCRIPTION
fallbacks	A sequence of runnables to try if the original Runnable fails.
TYPE: Sequence[Runnable[Input, Output]]

exceptions_to_handle	A tuple of exception types to handle.
TYPE: tuple[type[BaseException], ...]DEFAULT: (Exception,)

exception_key	If string is specified then handled exceptions will be passed to fallbacks as part of the input under the specified key.
If None, exceptions will not be passed to fallbacks.

If used, the base Runnable and its fallbacks must accept a dictionary as input.

TYPE: str | NoneDEFAULT: None

RETURNS	DESCRIPTION
RunnableWithFallbacks[Input, Output]	A new Runnable that will try the original Runnable, and then each Fallback in order, upon failures.
Example

from typing import Iterator

from langchain_core.runnables import RunnableGenerator


def _generate_immediate_error(input: Iterator) -> Iterator[str]:
    raise ValueError()
    yield ""


def _generate(input: Iterator) -> Iterator[str]:
    yield from "foo bar"


runnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(
    [RunnableGenerator(_generate)]
)
print("".join(runnable.stream({})))  # foo bar
PARAMETER	DESCRIPTION
fallbacks	A sequence of runnables to try if the original Runnable fails.
TYPE: Sequence[Runnable[Input, Output]]

exceptions_to_handle	A tuple of exception types to handle.
TYPE: tuple[type[BaseException], ...]DEFAULT: (Exception,)

exception_key	If string is specified then handled exceptions will be passed to fallbacks as part of the input under the specified key.
If None, exceptions will not be passed to fallbacks.

If used, the base Runnable and its fallbacks must accept a dictionary as input.

TYPE: str | NoneDEFAULT: None

RETURNS	DESCRIPTION
RunnableWithFallbacks[Input, Output]	A new Runnable that will try the original Runnable, and then each Fallback in order, upon failures.
 as_tool ¬∂

as_tool(
    args_schema: type[BaseModel] | None = None,
    *,
    name: str | None = None,
    description: str | None = None,
    arg_types: dict[str, type] | None = None,
) -> BaseTool
Create a BaseTool from a Runnable.

as_tool will instantiate a BaseTool with a name, description, and args_schema from a Runnable. Where possible, schemas are inferred from runnable.get_input_schema.

Alternatively (e.g., if the Runnable takes a dict as input and the specific dict keys are not typed), the schema can be specified directly with args_schema.

You can also pass arg_types to just specify the required arguments and their types.

PARAMETER	DESCRIPTION
args_schema	The schema for the tool.
TYPE: type[BaseModel] | NoneDEFAULT: None

name	The name of the tool.
TYPE: str | NoneDEFAULT: None

description	The description of the tool.
TYPE: str | NoneDEFAULT: None

arg_types	A dictionary of argument names to types.
TYPE: dict[str, type] | NoneDEFAULT: None

RETURNS	DESCRIPTION
BaseTool	A BaseTool instance.
TypedDict input


from typing_extensions import TypedDict
from langchain_core.runnables import RunnableLambda


class Args(TypedDict):
    a: int
    b: list[int]


def f(x: Args) -> str:
    return str(x["a"] * max(x["b"]))


runnable = RunnableLambda(f)
as_tool = runnable.as_tool()
as_tool.invoke({"a": 3, "b": [1, 2]})
dict input, specifying schema via args_schema


from typing import Any
from pydantic import BaseModel, Field
from langchain_core.runnables import RunnableLambda

def f(x: dict[str, Any]) -> str:
    return str(x["a"] * max(x["b"]))

class FSchema(BaseModel):
    """Apply a function to an integer and list of integers."""

    a: int = Field(..., description="Integer")
    b: list[int] = Field(..., description="List of ints")

runnable = RunnableLambda(f)
as_tool = runnable.as_tool(FSchema)
as_tool.invoke({"a": 3, "b": [1, 2]})
dict input, specifying schema via arg_types


from typing import Any
from langchain_core.runnables import RunnableLambda


def f(x: dict[str, Any]) -> str:
    return str(x["a"] * max(x["b"]))


runnable = RunnableLambda(f)
as_tool = runnable.as_tool(arg_types={"a": int, "b": list[int]})
as_tool.invoke({"a": 3, "b": [1, 2]})
str input


from langchain_core.runnables import RunnableLambda


def f(x: str) -> str:
    return x + "a"


def g(x: str) -> str:
    return x + "z"


runnable = RunnableLambda(f) | g
as_tool = runnable.as_tool()
as_tool.invoke("b")
 get_lc_namespace classmethod ¬∂

get_lc_namespace() -> list[str]
Get the namespace of the LangChain object.

For example, if the class is langchain.llms.openai.OpenAI, then the namespace is ["langchain", "llms", "openai"]

RETURNS	DESCRIPTION
list[str]	The namespace.
 lc_id classmethod ¬∂

lc_id() -> list[str]
Return a unique identifier for this class for serialization purposes.

The unique identifier is a list of strings that describes the path to the object.

For example, for the class langchain.llms.openai.OpenAI, the id is ["langchain", "llms", "openai", "OpenAI"].

 to_json ¬∂

to_json() -> SerializedConstructor | SerializedNotImplemented
Serialize the Runnable to JSON.

RETURNS	DESCRIPTION
SerializedConstructor | SerializedNotImplemented	A JSON-serializable representation of the Runnable.
 to_json_not_implemented ¬∂

to_json_not_implemented() -> SerializedNotImplemented
Serialize a "not implemented" object.

RETURNS	DESCRIPTION
SerializedNotImplemented	SerializedNotImplemented.
 configurable_fields ¬∂

configurable_fields(
    **kwargs: AnyConfigurableField,
) -> RunnableSerializable[Input, Output]
Configure particular Runnable fields at runtime.

PARAMETER	DESCRIPTION
**kwargs	A dictionary of ConfigurableField instances to configure.
TYPE: AnyConfigurableFieldDEFAULT: {}

RAISES	DESCRIPTION
ValueError	If a configuration key is not found in the Runnable.
RETURNS	DESCRIPTION
RunnableSerializable[Input, Output]	A new Runnable with the fields configured.
Example


from langchain_core.runnables import ConfigurableField
from langchain_openai import ChatOpenAI

model = ChatOpenAI(max_tokens=20).configurable_fields(
    max_tokens=ConfigurableField(
        id="output_token_number",
        name="Max tokens in the output",
        description="The maximum number of tokens in the output",
    )
)

# max_tokens = 20
print(
    "max_tokens_20: ", model.invoke("tell me something about chess").content
)

# max_tokens = 200
print(
    "max_tokens_200: ",
    model.with_config(configurable={"output_token_number": 200})
    .invoke("tell me something about chess")
    .content,
)
 configurable_alternatives ¬∂

configurable_alternatives(
    which: ConfigurableField,
    *,
    default_key: str = "default",
    prefix_keys: bool = False,
    **kwargs: Runnable[Input, Output] | Callable[[], Runnable[Input, Output]],
) -> RunnableSerializable[Input, Output]
Configure alternatives for Runnable objects that can be set at runtime.

PARAMETER	DESCRIPTION
which	The ConfigurableField instance that will be used to select the alternative.
TYPE: ConfigurableField

default_key	The default key to use if no alternative is selected.
TYPE: strDEFAULT: 'default'

prefix_keys	Whether to prefix the keys with the ConfigurableField id.
TYPE: boolDEFAULT: False

**kwargs	A dictionary of keys to Runnable instances or callables that return Runnable instances.
TYPE: Runnable[Input, Output] | Callable[[], Runnable[Input, Output]]DEFAULT: {}

RETURNS	DESCRIPTION
RunnableSerializable[Input, Output]	A new Runnable with the alternatives configured.
Example


from langchain_anthropic import ChatAnthropic
from langchain_core.runnables.utils import ConfigurableField
from langchain_openai import ChatOpenAI

model = ChatAnthropic(
    model_name="claude-sonnet-4-5-20250929"
).configurable_alternatives(
    ConfigurableField(id="llm"),
    default_key="anthropic",
    openai=ChatOpenAI(),
)

# uses the default model ChatAnthropic
print(model.invoke("which organization created you?").content)

# uses ChatOpenAI
print(
    model.with_config(configurable={"llm": "openai"})
    .invoke("which organization created you?")
    .content
)
 set_verbose ¬∂

set_verbose(verbose: bool | None) -> bool
If verbose is None, set it.

This allows users to pass in None as verbose to access the global setting.

PARAMETER	DESCRIPTION
verbose	The verbosity setting to use.
TYPE: bool | None

RETURNS	DESCRIPTION
bool	The verbosity setting to use.
 generate_prompt ¬∂

generate_prompt(
    prompts: list[PromptValue],
    stop: list[str] | None = None,
    callbacks: Callbacks = None,
    **kwargs: Any,
) -> LLMResult
Pass a sequence of prompts to the model and return model generations.

This method should make use of batched calls for models that expose a batched API.

Use this method when you want to:

Take advantage of batched calls,
Need more output from the model than just the top generated value,
Are building chains that are agnostic to the underlying language model type (e.g., pure text completion models vs chat models).
PARAMETER	DESCRIPTION
prompts	List of PromptValue objects.
A PromptValue is an object that can be converted to match the format of any language model (string for pure text generation models and BaseMessage objects for chat models).

TYPE: list[PromptValue]

stop	Stop words to use when generating.
Model output is cut off at the first occurrence of any of these substrings.

TYPE: list[str] | NoneDEFAULT: None

callbacks	Callbacks to pass through.
Used for executing additional functionality, such as logging or streaming, throughout generation.

TYPE: CallbacksDEFAULT: None

**kwargs	Arbitrary additional keyword arguments.
These are usually passed to the model provider API call.

TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
LLMResult	An LLMResult, which contains a list of candidate Generation objects for each input prompt and additional model provider-specific output.
 agenerate_prompt async ¬∂

agenerate_prompt(
    prompts: list[PromptValue],
    stop: list[str] | None = None,
    callbacks: Callbacks = None,
    **kwargs: Any,
) -> LLMResult
Asynchronously pass a sequence of prompts and return model generations.

This method should make use of batched calls for models that expose a batched API.

Use this method when you want to:

Take advantage of batched calls,
Need more output from the model than just the top generated value,
Are building chains that are agnostic to the underlying language model type (e.g., pure text completion models vs chat models).
PARAMETER	DESCRIPTION
prompts	List of PromptValue objects.
A PromptValue is an object that can be converted to match the format of any language model (string for pure text generation models and BaseMessage objects for chat models).

TYPE: list[PromptValue]

stop	Stop words to use when generating.
Model output is cut off at the first occurrence of any of these substrings.

TYPE: list[str] | NoneDEFAULT: None

callbacks	Callbacks to pass through.
Used for executing additional functionality, such as logging or streaming, throughout generation.

TYPE: CallbacksDEFAULT: None

**kwargs	Arbitrary additional keyword arguments.
These are usually passed to the model provider API call.

TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
LLMResult	An LLMResult, which contains a list of candidate Generation objects for each input prompt and additional model provider-specific output.
 get_token_ids ¬∂

get_token_ids(text: str) -> list[int]
Return the ordered IDs of the tokens in a text.

PARAMETER	DESCRIPTION
text	The string input to tokenize.
TYPE: str

RETURNS	DESCRIPTION
list[int]	A list of IDs corresponding to the tokens in the text, in order they occur in the text.
 get_num_tokens_from_messages ¬∂

get_num_tokens_from_messages(
    messages: list[BaseMessage], tools: Sequence | None = None
) -> int
Get the number of tokens in the messages.

Useful for checking if an input fits in a model's context window.

This should be overridden by model-specific implementations to provide accurate token counts via model-specific tokenizers.

Note

The base implementation of get_num_tokens_from_messages ignores tool schemas.
The base implementation of get_num_tokens_from_messages adds additional prefixes to messages in represent user roles, which will add to the overall token count. Model-specific implementations may choose to handle this differently.
PARAMETER	DESCRIPTION
messages	The message inputs to tokenize.
TYPE: list[BaseMessage]

tools	If provided, sequence of dict, BaseModel, function, or BaseTool objects to be converted to tool schemas.
TYPE: Sequence | NoneDEFAULT: None

RETURNS	DESCRIPTION
int	The sum of the number of tokens across the messages.
 generate ¬∂

generate(
    messages: list[list[BaseMessage]],
    stop: list[str] | None = None,
    callbacks: Callbacks = None,
    *,
    tags: list[str] | None = None,
    metadata: dict[str, Any] | None = None,
    run_name: str | None = None,
    run_id: UUID | None = None,
    **kwargs: Any,
) -> LLMResult
Pass a sequence of prompts to the model and return model generations.

This method should make use of batched calls for models that expose a batched API.

Use this method when you want to:

Take advantage of batched calls,
Need more output from the model than just the top generated value,
Are building chains that are agnostic to the underlying language model type (e.g., pure text completion models vs chat models).
PARAMETER	DESCRIPTION
messages	List of list of messages.
TYPE: list[list[BaseMessage]]

stop	Stop words to use when generating.
Model output is cut off at the first occurrence of any of these substrings.

TYPE: list[str] | NoneDEFAULT: None

callbacks	Callbacks to pass through.
Used for executing additional functionality, such as logging or streaming, throughout generation.

TYPE: CallbacksDEFAULT: None

tags	The tags to apply.
TYPE: list[str] | NoneDEFAULT: None

metadata	The metadata to apply.
TYPE: dict[str, Any] | NoneDEFAULT: None

run_name	The name of the run.
TYPE: str | NoneDEFAULT: None

run_id	The ID of the run.
TYPE: UUID | NoneDEFAULT: None

**kwargs	Arbitrary additional keyword arguments.
These are usually passed to the model provider API call.

TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
LLMResult	An LLMResult, which contains a list of candidate Generations for each input prompt and additional model provider-specific output.
 agenerate async ¬∂

agenerate(
    messages: list[list[BaseMessage]],
    stop: list[str] | None = None,
    callbacks: Callbacks = None,
    *,
    tags: list[str] | None = None,
    metadata: dict[str, Any] | None = None,
    run_name: str | None = None,
    run_id: UUID | None = None,
    **kwargs: Any,
) -> LLMResult
Asynchronously pass a sequence of prompts to a model and return generations.

This method should make use of batched calls for models that expose a batched API.

Use this method when you want to:

Take advantage of batched calls,
Need more output from the model than just the top generated value,
Are building chains that are agnostic to the underlying language model type (e.g., pure text completion models vs chat models).
PARAMETER	DESCRIPTION
messages	List of list of messages.
TYPE: list[list[BaseMessage]]

stop	Stop words to use when generating.
Model output is cut off at the first occurrence of any of these substrings.

TYPE: list[str] | NoneDEFAULT: None

callbacks	Callbacks to pass through.
Used for executing additional functionality, such as logging or streaming, throughout generation.

TYPE: CallbacksDEFAULT: None

tags	The tags to apply.
TYPE: list[str] | NoneDEFAULT: None

metadata	The metadata to apply.
TYPE: dict[str, Any] | NoneDEFAULT: None

run_name	The name of the run.
TYPE: str | NoneDEFAULT: None

run_id	The ID of the run.
TYPE: UUID | NoneDEFAULT: None

**kwargs	Arbitrary additional keyword arguments.
These are usually passed to the model provider API call.

TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
LLMResult	An LLMResult, which contains a list of candidate Generations for each input prompt and additional model provider-specific output.
 dict ¬∂

dict(**kwargs: Any) -> dict
Return a dictionary of the LLM.

 __init__ ¬∂

__init__(**kwargs: Any) -> None
Needed for arg validation.

 is_lc_serializable classmethod ¬∂

is_lc_serializable() -> bool
Is this class serializable?

By design, even if a class inherits from Serializable, it is not serializable by default. This is to prevent accidental serialization of objects that should not be serialized.

RETURNS	DESCRIPTION
bool	Whether the class is serializable. Default is False.
 build_extra classmethod ¬∂

build_extra(values: dict[str, Any]) -> Any
Build extra kwargs from additional params that were passed in.

 validate_environment ¬∂

validate_environment() -> Self
Validates params and passes them to google-generativeai package.

 invoke ¬∂

invoke(
    input: LanguageModelInput,
    config: RunnableConfig | None = None,
    *,
    code_execution: bool | None = None,
    stop: list[str] | None = None,
    **kwargs: Any,
) -> AIMessage
Override invoke on ChatGoogleGenerativeAI to add code_execution.

See the models page to see if your chosen model supports code execution. When enabled, the model can execute code to solve problems.

 get_num_tokens ¬∂

get_num_tokens(text: str) -> int
Get the number of tokens present in the text. Uses the model's tokenizer.

Useful for checking if an input will fit in a model's context window.

PARAMETER	DESCRIPTION
text	The string input to tokenize.
TYPE: str

RETURNS	DESCRIPTION
int	The integer number of tokens in the text.
Example

llm = ChatGoogleGenerativeAI(model="gemini-3-pro-preview")
num_tokens = llm.get_num_tokens("Hello, world!")
print(num_tokens)
# -> 4
 with_structured_output ¬∂

with_structured_output(
    schema: dict | type[BaseModel],
    method: Literal["function_calling", "json_mode", "json_schema"]
    | None = "function_calling",
    *,
    include_raw: bool = False,
    **kwargs: Any,
) -> Runnable[LanguageModelInput, dict | BaseModel]
Model wrapper that returns outputs formatted to match the given schema.

PARAMETER	DESCRIPTION
schema	The output schema. Can be passed in as:
An OpenAI function/tool schema,
A JSON Schema,
A TypedDict class,
Or a Pydantic class.
If schema is a Pydantic class then the model output will be a Pydantic instance of that class, and the model-generated fields will be validated by the Pydantic class. Otherwise the model output will be a dict and will not be validated.

See langchain_core.utils.function_calling.convert_to_openai_tool for more on how to properly specify types and descriptions of schema fields when specifying a Pydantic or TypedDict class.

TYPE: Dict | type

include_raw	If False then only the parsed structured output is returned.
If an error occurs during model output parsing it will be raised.

If True then both the raw model response (a BaseMessage) and the parsed model response will be returned.

If an error occurs during output parsing it will be caught and returned as well.

The final output is always a dict with keys 'raw', 'parsed', and 'parsing_error'.

TYPE: boolDEFAULT: False

RAISES	DESCRIPTION
ValueError	If there are any unsupported kwargs.
NotImplementedError	If the model does not implement with_structured_output().
RETURNS	DESCRIPTION
Runnable[LanguageModelInput, Dict | BaseModel]	A Runnable that takes same inputs as a langchain_core.language_models.chat.BaseChatModel. If include_raw is False and schema is a Pydantic class, Runnable outputs an instance of schema (i.e., a Pydantic object). Otherwise, if include_raw is False then Runnable outputs a dict.
If include_raw is True, then Runnable outputs a dict with keys:

'raw': BaseMessage
'parsed': None if there was a parsing error, otherwise the type depends on the schema as described above.
'parsing_error': BaseException | None
Example: Pydantic schema (include_raw=False):


from pydantic import BaseModel


class AnswerWithJustification(BaseModel):
    '''An answer to the user question along with justification for the answer.'''

    answer: str
    justification: str


model = ChatModel(model="model-name", temperature=0)
structured_model = model.with_structured_output(AnswerWithJustification)

structured_model.invoke(
    "What weighs more a pound of bricks or a pound of feathers"
)

# -> AnswerWithJustification(
#     answer='They weigh the same',
#     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'
# )
Example: Pydantic schema (include_raw=True):


from pydantic import BaseModel


class AnswerWithJustification(BaseModel):
    '''An answer to the user question along with justification for the answer.'''

    answer: str
    justification: str


model = ChatModel(model="model-name", temperature=0)
structured_model = model.with_structured_output(
    AnswerWithJustification, include_raw=True
)

structured_model.invoke(
    "What weighs more a pound of bricks or a pound of feathers"
)
# -> {
#     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{"answer":"They weigh the same.","justification":"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ."}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}),
#     'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'),
#     'parsing_error': None
# }
Example: Dictionary schema (include_raw=False):


from pydantic import BaseModel
from langchain_core.utils.function_calling import convert_to_openai_tool


class AnswerWithJustification(BaseModel):
    '''An answer to the user question along with justification for the answer.'''

    answer: str
    justification: str


dict_schema = convert_to_openai_tool(AnswerWithJustification)
model = ChatModel(model="model-name", temperature=0)
structured_model = model.with_structured_output(dict_schema)

structured_model.invoke(
    "What weighs more a pound of bricks or a pound of feathers"
)
# -> {
#     'answer': 'They weigh the same',
#     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'
# }
Behavior changed in langchain-core 0.2.26

Added support for TypedDict class.

 bind_tools ¬∂

bind_tools(
    tools: Sequence[dict[str, Any] | type | Callable[..., Any] | BaseTool | Tool],
    tool_config: dict | _ToolConfigDict | None = None,
    *,
    tool_choice: _ToolChoiceType | bool | None = None,
    **kwargs: Any,
) -> Runnable[LanguageModelInput, AIMessage]
Bind tool-like objects to this chat model.

Assumes model is compatible with google-generativeAI tool-calling API.

PARAMETER	DESCRIPTION
tools	A list of tool definitions to bind to this chat model.
Can be a pydantic model, Callable, or BaseTool. Pydantic models, Callable, and BaseTool objects will be automatically converted to their schema dictionary representation.

Tools with Union types in their arguments are now supported and converted to anyOf schemas.

TYPE: Sequence[dict[str, Any] | type | Callable[..., Any] | BaseTool | Tool]

**kwargs	Any additional parameters to pass to the Runnable constructor.
TYPE: AnyDEFAULT: {}

 GoogleGenerativeAIEmbeddings ¬∂
Bases: BaseModel, Embeddings

Google Generative AI Embeddings.

To use, you must have either:

The GOOGLE_API_KEY environment variable set with your API key, or
Pass your API key using the google_api_key kwarg to the GoogleGenerativeAIEmbeddings constructor.
Example

from langchain_google_genai import GoogleGenerativeAIEmbeddings

embeddings = GoogleGenerativeAIEmbeddings(model="gemini-embedding-001")
embeddings.embed_query("What's our Q1 revenue?")
METHOD	DESCRIPTION
validate_environment	Validates params and passes them to google-generativeai package.
embed_documents	Embed a list of strings using the batch endpoint
embed_query	Embed a text, using the non-batch endpoint
aembed_documents	Embed a list of strings using the batch endpoint
aembed_query	Embed a text, using the non-batch endpoint.
 model class-attribute instance-attribute ¬∂

model: str = Field(...)
The name of the embedding model to use.

Example: 'models/gemini-embedding-001'

 task_type class-attribute instance-attribute ¬∂

task_type: str | None = Field(default=None)
The task type.

Valid options include:

'task_type_unspecified'
'retrieval_query'
'retrieval_document'
'semantic_similarity'
'classification'
'clustering'
 google_api_key class-attribute instance-attribute ¬∂

google_api_key: SecretStr | None = Field(
    default_factory=secret_from_env("GOOGLE_API_KEY", default=None)
)
The Google API key to use.

If not provided, the GOOGLE_API_KEY environment variable will be used.

 credentials class-attribute instance-attribute ¬∂

credentials: Any = Field(default=None, exclude=True)
The default custom credentials to use when making API calls.

(google.auth.credentials.Credentials)

If not provided, credentials will be ascertained from the GOOGLE_API_KEY env var.

 client_options class-attribute instance-attribute ¬∂

client_options: dict | None = Field(default=None)
A dictionary of client options to pass to the Google API client.

Example: api_endpoint

 base_url class-attribute instance-attribute ¬∂

base_url: str | None = Field(default=None)
The base URL to use for the API client.

Alias of client_options['api_endpoint'].

 transport class-attribute instance-attribute ¬∂

transport: str | None = Field(default=None)
A string, one of: ['rest', 'grpc', 'grpc_asyncio'].

 request_options class-attribute instance-attribute ¬∂

request_options: dict | None = Field(default=None)
A dictionary of request options to pass to the Google API client.

Example: {'timeout': 10}

 validate_environment ¬∂

validate_environment() -> Self
Validates params and passes them to google-generativeai package.

 embed_documents ¬∂

embed_documents(
    texts: list[str],
    *,
    batch_size: int = _DEFAULT_BATCH_SIZE,
    task_type: str | None = None,
    titles: list[str] | None = None,
    output_dimensionality: int | None = None,
) -> list[list[float]]
Embed a list of strings using the batch endpoint

Google Generative AI currently sets a max batch size of 100 strings.

PARAMETER	DESCRIPTION
texts	The list of strings to embed.
TYPE: list[str]

batch_size	Batch size of embeddings to send to the model
TYPE: intDEFAULT: _DEFAULT_BATCH_SIZE

task_type	task_type
TYPE: str | NoneDEFAULT: None

titles	Optional list of titles for texts provided.
Only applicable when TaskType is 'RETRIEVAL_DOCUMENT'.

TYPE: list[str] | NoneDEFAULT: None

output_dimensionality	Optional reduced dimension for the output embedding
TYPE: int | NoneDEFAULT: None

RETURNS	DESCRIPTION
list[list[float]]	List of embeddings, one for each text.
 embed_query ¬∂

embed_query(
    text: str,
    *,
    task_type: str | None = None,
    title: str | None = None,
    output_dimensionality: int | None = None,
) -> list[float]
Embed a text, using the non-batch endpoint

PARAMETER	DESCRIPTION
text	The text to embed.
TYPE: str

task_type	task_type
TYPE: str | NoneDEFAULT: None

title	Optional title for the text.
Only applicable when TaskType is 'RETRIEVAL_DOCUMENT'.

TYPE: str | NoneDEFAULT: None

output_dimensionality	Optional reduced dimension for the output embedding
TYPE: int | NoneDEFAULT: None

RETURNS	DESCRIPTION
list[float]	Embedding for the text.
 aembed_documents async ¬∂

aembed_documents(
    texts: list[str],
    *,
    batch_size: int = _DEFAULT_BATCH_SIZE,
    task_type: str | None = None,
    titles: list[str] | None = None,
    output_dimensionality: int | None = None,
) -> list[list[float]]
Embed a list of strings using the batch endpoint

Google Generative AI currently sets a max batch size of 100 strings.

PARAMETER	DESCRIPTION
texts	The list of strings to embed.
TYPE: list[str]

batch_size	The batch size of embeddings to send to the model
TYPE: intDEFAULT: _DEFAULT_BATCH_SIZE

task_type	task_type
TYPE: str | NoneDEFAULT: None

titles	Optional list of titles for texts provided.
Only applicable when TaskType is 'RETRIEVAL_DOCUMENT'.

TYPE: list[str] | NoneDEFAULT: None

output_dimensionality	Optional reduced dimension for the output embedding
TYPE: int | NoneDEFAULT: None

RETURNS	DESCRIPTION
list[list[float]]	List of embeddings, one for each text.
 aembed_query async ¬∂

aembed_query(
    text: str,
    *,
    task_type: str | None = None,
    title: str | None = None,
    output_dimensionality: int | None = None,
) -> list[float]
Embed a text, using the non-batch endpoint.

PARAMETER	DESCRIPTION
text	The text to embed.
TYPE: str

task_type	task_type
TYPE: str | NoneDEFAULT: None

title	Optional title for the text.
Only applicable when TaskType is 'RETRIEVAL_DOCUMENT'.

TYPE: str | NoneDEFAULT: None

output_dimensionality	Optional reduced dimension for the output embedding
TYPE: int | NoneDEFAULT: None

RETURNS	DESCRIPTION
list[float]	Embedding for the text.
 GoogleVectorStore ¬∂
Bases: VectorStore

Google GenerativeAI Vector Store.

Currently, it computes the embedding vectors on the server side.

Add texts to an existing corpus


store = GoogleVectorStore(corpus_id="123")
store.add_documents(documents, document_id="456")
Create a new corpus


store = GoogleVectorStore.create_corpus(
    corpus_id="123", display_name="My Google corpus"
)
Query the corpus for relevant passages


store.as_retriever()
    .get_relevant_documents("Who caught the gingerbread man?"
)
You can also operate at Google's Document level.

Add texts to an existing Google Vector Store Document


doc_store = GoogleVectorStore(corpus_id="123", document_id="456")
doc_store.add_documents(documents)
Create a new Google Vector Store Document


doc_store = GoogleVectorStore.create_document(
    corpus_id="123", document_id="456", display_name="My Google document"
)
Query the Google document


doc_store.as_retriever()             .get_relevant_documents("Who caught the gingerbread man?")
METHOD	DESCRIPTION
get_by_ids	Get documents by their IDs.
aget_by_ids	Async get documents by their IDs.
aadd_texts	Async run more texts through the embeddings and add to the VectorStore.
add_documents	Add or update documents in the VectorStore.
aadd_documents	Async run more documents through the embeddings and add to the VectorStore.
search	Return docs most similar to query using a specified search type.
asearch	Async return docs most similar to query using a specified search type.
asimilarity_search_with_score	Async run similarity search with distance.
similarity_search_with_relevance_scores	Return docs and relevance scores in the range [0, 1].
asimilarity_search_with_relevance_scores	Async return docs and relevance scores in the range [0, 1].
asimilarity_search	Async return docs most similar to query.
similarity_search_by_vector	Return docs most similar to embedding vector.
asimilarity_search_by_vector	Async return docs most similar to embedding vector.
max_marginal_relevance_search	Return docs selected using the maximal marginal relevance.
amax_marginal_relevance_search	Async return docs selected using the maximal marginal relevance.
max_marginal_relevance_search_by_vector	Return docs selected using the maximal marginal relevance.
amax_marginal_relevance_search_by_vector	Async return docs selected using the maximal marginal relevance.
from_documents	Return VectorStore initialized from documents and embeddings.
afrom_documents	Async return VectorStore initialized from documents and embeddings.
afrom_texts	Async return VectorStore initialized from texts and embeddings.
as_retriever	Return VectorStoreRetriever initialized from this VectorStore.
__init__	Returns an existing Google Semantic Retriever corpus or document.
create_corpus	Create a Google Semantic Retriever corpus.
create_document	Create a Google Semantic Retriever document.
from_texts	Returns a vector store of an existing document with the specified text.
add_texts	Add texts to the vector store.
similarity_search	Search the vector store for relevant texts.
similarity_search_with_score	Run similarity search with distance.
delete	Delete chunks.
adelete	Delete chunks asynchronously.
 embeddings property ¬∂

embeddings: Embeddings | None
Access the query embedding object if available.

 name property ¬∂

name: str
Returns the name of the Google entity.

You shouldn't need to care about this unless you want to access your corpus or document via Google Generative AI API.

 corpus_id property ¬∂

corpus_id: str
Returns the corpus ID managed by this vector store.

 document_id property ¬∂

document_id: str | None
Returns the document ID managed by this vector store.

 get_by_ids ¬∂

get_by_ids(ids: Sequence[str]) -> list[Document]
Get documents by their IDs.

The returned documents are expected to have the ID field set to the ID of the document in the vector store.

Fewer documents may be returned than requested if some IDs are not found or if there are duplicated IDs.

Users should not assume that the order of the returned documents matches the order of the input IDs. Instead, users should rely on the ID field of the returned documents.

This method should NOT raise exceptions if no documents are found for some IDs.

PARAMETER	DESCRIPTION
ids	List of IDs to retrieve.
TYPE: Sequence[str]

RETURNS	DESCRIPTION
list[Document]	List of Document objects.
 aget_by_ids async ¬∂

aget_by_ids(ids: Sequence[str]) -> list[Document]
Async get documents by their IDs.

The returned documents are expected to have the ID field set to the ID of the document in the vector store.

Fewer documents may be returned than requested if some IDs are not found or if there are duplicated IDs.

Users should not assume that the order of the returned documents matches the order of the input IDs. Instead, users should rely on the ID field of the returned documents.

This method should NOT raise exceptions if no documents are found for some IDs.

PARAMETER	DESCRIPTION
ids	List of IDs to retrieve.
TYPE: Sequence[str]

RETURNS	DESCRIPTION
list[Document]	List of Document objects.
 aadd_texts async ¬∂

aadd_texts(
    texts: Iterable[str],
    metadatas: list[dict] | None = None,
    *,
    ids: list[str] | None = None,
    **kwargs: Any,
) -> list[str]
Async run more texts through the embeddings and add to the VectorStore.

PARAMETER	DESCRIPTION
texts	Iterable of strings to add to the VectorStore.
TYPE: Iterable[str]

metadatas	Optional list of metadatas associated with the texts.
TYPE: list[dict] | NoneDEFAULT: None

ids	Optional list
TYPE: list[str] | NoneDEFAULT: None

**kwargs	VectorStore specific parameters.
TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
list[str]	List of IDs from adding the texts into the VectorStore.
RAISES	DESCRIPTION
ValueError	If the number of metadatas does not match the number of texts.
ValueError	If the number of IDs does not match the number of texts.
 add_documents ¬∂

add_documents(documents: list[Document], **kwargs: Any) -> list[str]
Add or update documents in the VectorStore.

PARAMETER	DESCRIPTION
documents	Documents to add to the VectorStore.
TYPE: list[Document]

**kwargs	Additional keyword arguments.
If kwargs contains IDs and documents contain ids, the IDs in the kwargs will receive precedence.

TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
list[str]	List of IDs of the added texts.
 aadd_documents async ¬∂

aadd_documents(documents: list[Document], **kwargs: Any) -> list[str]
Async run more documents through the embeddings and add to the VectorStore.

PARAMETER	DESCRIPTION
documents	Documents to add to the VectorStore.
TYPE: list[Document]

**kwargs	Additional keyword arguments.
TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
list[str]	List of IDs of the added texts.
 search ¬∂

search(query: str, search_type: str, **kwargs: Any) -> list[Document]
Return docs most similar to query using a specified search type.

PARAMETER	DESCRIPTION
query	Input text.
TYPE: str

search_type	Type of search to perform.
Can be 'similarity', 'mmr', or 'similarity_score_threshold'.

TYPE: str

**kwargs	Arguments to pass to the search method.
TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
list[Document]	List of Document objects most similar to the query.
RAISES	DESCRIPTION
ValueError	If search_type is not one of 'similarity', 'mmr', or 'similarity_score_threshold'.
 asearch async ¬∂

asearch(query: str, search_type: str, **kwargs: Any) -> list[Document]
Async return docs most similar to query using a specified search type.

PARAMETER	DESCRIPTION
query	Input text.
TYPE: str

search_type	Type of search to perform.
Can be 'similarity', 'mmr', or 'similarity_score_threshold'.

TYPE: str

**kwargs	Arguments to pass to the search method.
TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
list[Document]	List of Document objects most similar to the query.
RAISES	DESCRIPTION
ValueError	If search_type is not one of 'similarity', 'mmr', or 'similarity_score_threshold'.
 asimilarity_search_with_score async ¬∂

asimilarity_search_with_score(
    *args: Any, **kwargs: Any
) -> list[tuple[Document, float]]
Async run similarity search with distance.

PARAMETER	DESCRIPTION
*args	Arguments to pass to the search method.
TYPE: AnyDEFAULT: ()

**kwargs	Arguments to pass to the search method.
TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
list[tuple[Document, float]]	List of tuples of (doc, similarity_score).
 similarity_search_with_relevance_scores ¬∂

similarity_search_with_relevance_scores(
    query: str, k: int = 4, **kwargs: Any
) -> list[tuple[Document, float]]
Return docs and relevance scores in the range [0, 1].

0 is dissimilar, 1 is most similar.

PARAMETER	DESCRIPTION
query	Input text.
TYPE: str

k	Number of Document objects to return.
TYPE: intDEFAULT: 4

**kwargs	Kwargs to be passed to similarity search.
Should include score_threshold, an optional floating point value between 0 to 1 to filter the resulting set of retrieved docs.

TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
list[tuple[Document, float]]	List of tuples of (doc, similarity_score).
 asimilarity_search_with_relevance_scores async ¬∂

asimilarity_search_with_relevance_scores(
    query: str, k: int = 4, **kwargs: Any
) -> list[tuple[Document, float]]
Async return docs and relevance scores in the range [0, 1].

0 is dissimilar, 1 is most similar.

PARAMETER	DESCRIPTION
query	Input text.
TYPE: str

k	Number of Document objects to return.
TYPE: intDEFAULT: 4

**kwargs	Kwargs to be passed to similarity search.
Should include score_threshold, an optional floating point value between 0 to 1 to filter the resulting set of retrieved docs.

TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
list[tuple[Document, float]]	List of tuples of (doc, similarity_score)
 asimilarity_search async ¬∂

asimilarity_search(query: str, k: int = 4, **kwargs: Any) -> list[Document]
Async return docs most similar to query.

PARAMETER	DESCRIPTION
query	Input text.
TYPE: str

k	Number of Document objects to return.
TYPE: intDEFAULT: 4

**kwargs	Arguments to pass to the search method.
TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
list[Document]	List of Document objects most similar to the query.
 similarity_search_by_vector ¬∂

similarity_search_by_vector(
    embedding: list[float], k: int = 4, **kwargs: Any
) -> list[Document]
Return docs most similar to embedding vector.

PARAMETER	DESCRIPTION
embedding	Embedding to look up documents similar to.
TYPE: list[float]

k	Number of Document objects to return.
TYPE: intDEFAULT: 4

**kwargs	Arguments to pass to the search method.
TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
list[Document]	List of Document objects most similar to the query vector.
 asimilarity_search_by_vector async ¬∂

asimilarity_search_by_vector(
    embedding: list[float], k: int = 4, **kwargs: Any
) -> list[Document]
Async return docs most similar to embedding vector.

PARAMETER	DESCRIPTION
embedding	Embedding to look up documents similar to.
TYPE: list[float]

k	Number of Document objects to return.
TYPE: intDEFAULT: 4

**kwargs	Arguments to pass to the search method.
TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
list[Document]	List of Document objects most similar to the query vector.
 max_marginal_relevance_search ¬∂

max_marginal_relevance_search(
    query: str, k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, **kwargs: Any
) -> list[Document]
Return docs selected using the maximal marginal relevance.

Maximal marginal relevance optimizes for similarity to query AND diversity among selected documents.

PARAMETER	DESCRIPTION
query	Text to look up documents similar to.
TYPE: str

k	Number of Document objects to return.
TYPE: intDEFAULT: 4

fetch_k	Number of Document objects to fetch to pass to MMR algorithm.
TYPE: intDEFAULT: 20

lambda_mult	Number between 0 and 1 that determines the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity.
TYPE: floatDEFAULT: 0.5

**kwargs	Arguments to pass to the search method.
TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
list[Document]	List of Document objects selected by maximal marginal relevance.
 amax_marginal_relevance_search async ¬∂

amax_marginal_relevance_search(
    query: str, k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, **kwargs: Any
) -> list[Document]
Async return docs selected using the maximal marginal relevance.

Maximal marginal relevance optimizes for similarity to query AND diversity among selected documents.

PARAMETER	DESCRIPTION
query	Text to look up documents similar to.
TYPE: str

k	Number of Document objects to return.
TYPE: intDEFAULT: 4

fetch_k	Number of Document objects to fetch to pass to MMR algorithm.
TYPE: intDEFAULT: 20

lambda_mult	Number between 0 and 1 that determines the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity.
TYPE: floatDEFAULT: 0.5

**kwargs	Arguments to pass to the search method.
TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
list[Document]	List of Document objects selected by maximal marginal relevance.
 max_marginal_relevance_search_by_vector ¬∂

max_marginal_relevance_search_by_vector(
    embedding: list[float],
    k: int = 4,
    fetch_k: int = 20,
    lambda_mult: float = 0.5,
    **kwargs: Any,
) -> list[Document]
Return docs selected using the maximal marginal relevance.

Maximal marginal relevance optimizes for similarity to query AND diversity among selected documents.

PARAMETER	DESCRIPTION
embedding	Embedding to look up documents similar to.
TYPE: list[float]

k	Number of Document objects to return.
TYPE: intDEFAULT: 4

fetch_k	Number of Document objects to fetch to pass to MMR algorithm.
TYPE: intDEFAULT: 20

lambda_mult	Number between 0 and 1 that determines the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity.
TYPE: floatDEFAULT: 0.5

**kwargs	Arguments to pass to the search method.
TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
list[Document]	List of Document objects selected by maximal marginal relevance.
 amax_marginal_relevance_search_by_vector async ¬∂

amax_marginal_relevance_search_by_vector(
    embedding: list[float],
    k: int = 4,
    fetch_k: int = 20,
    lambda_mult: float = 0.5,
    **kwargs: Any,
) -> list[Document]
Async return docs selected using the maximal marginal relevance.

Maximal marginal relevance optimizes for similarity to query AND diversity among selected documents.

PARAMETER	DESCRIPTION
embedding	Embedding to look up documents similar to.
TYPE: list[float]

k	Number of Document objects to return.
TYPE: intDEFAULT: 4

fetch_k	Number of Document objects to fetch to pass to MMR algorithm.
TYPE: intDEFAULT: 20

lambda_mult	Number between 0 and 1 that determines the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity.
TYPE: floatDEFAULT: 0.5

**kwargs	Arguments to pass to the search method.
TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
list[Document]	List of Document objects selected by maximal marginal relevance.
 from_documents classmethod ¬∂

from_documents(documents: list[Document], embedding: Embeddings, **kwargs: Any) -> Self
Return VectorStore initialized from documents and embeddings.

PARAMETER	DESCRIPTION
documents	List of Document objects to add to the VectorStore.
TYPE: list[Document]

embedding	Embedding function to use.
TYPE: Embeddings

**kwargs	Additional keyword arguments.
TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
Self	VectorStore initialized from documents and embeddings.
 afrom_documents async classmethod ¬∂

afrom_documents(
    documents: list[Document], embedding: Embeddings, **kwargs: Any
) -> Self
Async return VectorStore initialized from documents and embeddings.

PARAMETER	DESCRIPTION
documents	List of Document objects to add to the VectorStore.
TYPE: list[Document]

embedding	Embedding function to use.
TYPE: Embeddings

**kwargs	Additional keyword arguments.
TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
Self	VectorStore initialized from documents and embeddings.
 afrom_texts async classmethod ¬∂

afrom_texts(
    texts: list[str],
    embedding: Embeddings,
    metadatas: list[dict] | None = None,
    *,
    ids: list[str] | None = None,
    **kwargs: Any,
) -> Self
Async return VectorStore initialized from texts and embeddings.

PARAMETER	DESCRIPTION
texts	Texts to add to the VectorStore.
TYPE: list[str]

embedding	Embedding function to use.
TYPE: Embeddings

metadatas	Optional list of metadatas associated with the texts.
TYPE: list[dict] | NoneDEFAULT: None

ids	Optional list of IDs associated with the texts.
TYPE: list[str] | NoneDEFAULT: None

**kwargs	Additional keyword arguments.
TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
Self	VectorStore initialized from texts and embeddings.
 as_retriever ¬∂

as_retriever(**kwargs: Any) -> VectorStoreRetriever
Return VectorStoreRetriever initialized from this VectorStore.

PARAMETER	DESCRIPTION
**kwargs	Keyword arguments to pass to the search function.
Can include:

search_type: Defines the type of search that the Retriever should perform. Can be 'similarity' (default), 'mmr', or 'similarity_score_threshold'.
search_kwargs: Keyword arguments to pass to the search function.

Can include things like:

k: Amount of documents to return (Default: 4)
score_threshold: Minimum relevance threshold for similarity_score_threshold
fetch_k: Amount of documents to pass to MMR algorithm (Default: 20)
lambda_mult: Diversity of results returned by MMR; 1 for minimum diversity and 0 for maximum. (Default: 0.5)
filter: Filter by document metadata
TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
VectorStoreRetriever	Retriever class for VectorStore.
Examples:


# Retrieve more documents with higher diversity
# Useful if your dataset has many similar documents
docsearch.as_retriever(
    search_type="mmr", search_kwargs={"k": 6, "lambda_mult": 0.25}
)

# Fetch more documents for the MMR algorithm to consider
# But only return the top 5
docsearch.as_retriever(search_type="mmr", search_kwargs={"k": 5, "fetch_k": 50})

# Only retrieve documents that have a relevance score
# Above a certain threshold
docsearch.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={"score_threshold": 0.8},
)

# Only get the single most similar document from the dataset
docsearch.as_retriever(search_kwargs={"k": 1})

# Use a filter to only retrieve documents from a specific paper
docsearch.as_retriever(
    search_kwargs={"filter": {"paper_title": "GPT-4 Technical Report"}}
)
 __init__ ¬∂

__init__(*, corpus_id: str, document_id: str | None = None, **kwargs: Any) -> None
Returns an existing Google Semantic Retriever corpus or document.

If just the corpus ID is provided, the vector store operates over all documents within that corpus.

If the document ID is provided, the vector store operates over just that document.

RAISES	DESCRIPTION
DoesNotExistsException	If the IDs do not match to anything on Google server. In this case, consider using create_corpus or create_document to create one.
 create_corpus classmethod ¬∂

create_corpus(
    corpus_id: str | None = None, display_name: str | None = None
) -> GoogleVectorStore
Create a Google Semantic Retriever corpus.

PARAMETER	DESCRIPTION
corpus_id	The ID to use to create the new corpus. If not provided, Google server will provide one.
TYPE: str | NoneDEFAULT: None

display_name	The title of the new corpus. If not provided, Google server will provide one.
TYPE: str | NoneDEFAULT: None

RETURNS	DESCRIPTION
GoogleVectorStore	An instance of vector store that points to the newly created corpus.
 create_document classmethod ¬∂

create_document(
    corpus_id: str,
    document_id: str | None = None,
    display_name: str | None = None,
    metadata: dict[str, Any] | None = None,
) -> GoogleVectorStore
Create a Google Semantic Retriever document.

PARAMETER	DESCRIPTION
corpus_id	ID of an existing corpus.
TYPE: str

document_id	The ID to use to create the new Google Semantic Retriever document. If not provided, Google server will provide one.
TYPE: str | NoneDEFAULT: None

display_name	The title of the new document. If not provided, Google server will provide one.
TYPE: str | NoneDEFAULT: None

RETURNS	DESCRIPTION
GoogleVectorStore	An instance of vector store that points to the newly created document.
 from_texts classmethod ¬∂

from_texts(
    texts: list[str],
    embedding: Embeddings | None = None,
    metadatas: list[dict[str, Any]] | None = None,
    *,
    corpus_id: str | None = None,
    document_id: str | None = None,
    **kwargs: Any,
) -> GoogleVectorStore
Returns a vector store of an existing document with the specified text.

PARAMETER	DESCRIPTION
corpus_id	REQUIRED. Must be an existing corpus.
TYPE: str | NoneDEFAULT: None

document_id	REQUIRED. Must be an existing document.
TYPE: str | NoneDEFAULT: None

texts	Texts to be loaded into the vector store.
TYPE: list[str]

RETURNS	DESCRIPTION
GoogleVectorStore	A vector store pointing to the specified Google Semantic Retriever Document.
RAISES	DESCRIPTION
DoesNotExistsException	If the IDs do not match to anything at Google server.
 add_texts ¬∂

add_texts(
    texts: Iterable[str],
    metadatas: list[dict[str, Any]] | None = None,
    *,
    document_id: str | None = None,
    **kwargs: Any,
) -> list[str]
Add texts to the vector store.

If the vector store points to a corpus (instead of a document), you must also provide a document_id.

RETURNS	DESCRIPTION
list[str]	Chunk's names created on Google servers.
 similarity_search ¬∂

similarity_search(
    query: str, k: int = 4, filter: dict[str, Any] | None = None, **kwargs: Any
) -> list[Document]
Search the vector store for relevant texts.

 similarity_search_with_score ¬∂

similarity_search_with_score(
    query: str, k: int = 4, filter: dict[str, Any] | None = None, **kwargs: Any
) -> list[tuple[Document, float]]
Run similarity search with distance.

 delete ¬∂

delete(ids: list[str] | None = None, **kwargs: Any) -> bool | None
Delete chunks.

Note that the "ids" are not corpus ID or document ID. Rather, these are the entity names returned by add_texts.

RETURNS	DESCRIPTION
bool | None	True if successful. Otherwise, you should get an exception anyway.
 adelete async ¬∂

adelete(ids: list[str] | None = None, **kwargs: Any) -> bool | None
Delete chunks asynchronously.

Note that the "ids" are not corpus ID or document ID. Rather, these are the entity names returned by add_texts.

RETURNS	DESCRIPTION
bool | None	True if successful. Otherwise, you should get an exception anyway.
 GoogleGenerativeAI ¬∂
Bases: _BaseGoogleGenerativeAI, BaseLLM

Google GenerativeAI text completion large language models (legacy LLMs).

Basic Usage


from langchain_google_genai import GoogleGenerativeAI

llm = GoogleGenerativeAI(model="gemini-2.5-pro")
METHOD	DESCRIPTION
get_name	Get the name of the Runnable.
get_input_schema	Get a Pydantic model that can be used to validate input to the Runnable.
get_input_jsonschema	Get a JSON schema that represents the input to the Runnable.
get_output_schema	Get a Pydantic model that can be used to validate output to the Runnable.
get_output_jsonschema	Get a JSON schema that represents the output of the Runnable.
config_schema	The type of config this Runnable accepts specified as a Pydantic model.
get_config_jsonschema	Get a JSON schema that represents the config of the Runnable.
get_graph	Return a graph representation of this Runnable.
get_prompts	Return a list of prompts used by this Runnable.
__or__	Runnable "or" operator.
__ror__	Runnable "reverse-or" operator.
pipe	Pipe Runnable objects.
pick	Pick keys from the output dict of this Runnable.
assign	Assigns new fields to the dict output of this Runnable.
invoke	Transform a single input into an output.
ainvoke	Transform a single input into an output.
batch	Default implementation runs invoke in parallel using a thread pool executor.
batch_as_completed	Run invoke in parallel on a list of inputs.
abatch	Default implementation runs ainvoke in parallel using asyncio.gather.
abatch_as_completed	Run ainvoke in parallel on a list of inputs.
stream	Default implementation of stream, which calls invoke.
astream	Default implementation of astream, which calls ainvoke.
astream_log	Stream all output from a Runnable, as reported to the callback system.
astream_events	Generate a stream of events.
transform	Transform inputs to outputs.
atransform	Transform inputs to outputs.
bind	Bind arguments to a Runnable, returning a new Runnable.
with_config	Bind config to a Runnable, returning a new Runnable.
with_listeners	Bind lifecycle listeners to a Runnable, returning a new Runnable.
with_alisteners	Bind async lifecycle listeners to a Runnable.
with_types	Bind input and output types to a Runnable, returning a new Runnable.
with_retry	Create a new Runnable that retries the original Runnable on exceptions.
map	Return a new Runnable that maps a list of inputs to a list of outputs.
with_fallbacks	Add fallbacks to a Runnable, returning a new Runnable.
as_tool	Create a BaseTool from a Runnable.
is_lc_serializable	Is this class serializable?
get_lc_namespace	Get the namespace of the LangChain object.
lc_id	Return a unique identifier for this class for serialization purposes.
to_json	Serialize the Runnable to JSON.
to_json_not_implemented	Serialize a "not implemented" object.
configurable_fields	Configure particular Runnable fields at runtime.
configurable_alternatives	Configure alternatives for Runnable objects that can be set at runtime.
set_verbose	If verbose is None, set it.
generate_prompt	Pass a sequence of prompts to the model and return model generations.
agenerate_prompt	Asynchronously pass a sequence of prompts and return model generations.
with_structured_output	Not implemented on this class.
get_token_ids	Return the ordered IDs of the tokens in a text.
get_num_tokens_from_messages	Get the number of tokens in the messages.
generate	Pass a sequence of prompts to a model and return generations.
agenerate	Asynchronously pass a sequence of prompts to a model and return generations.
__str__	Return a string representation of the object for printing.
dict	Return a dictionary of the LLM.
save	Save the LLM.
__init__	Needed for arg validation.
validate_environment	Validates params and passes them to google-generativeai package.
get_num_tokens	Get the number of tokens present in the text.
 name class-attribute instance-attribute ¬∂

name: str | None = None
The name of the Runnable. Used for debugging and tracing.

 InputType property ¬∂

InputType: TypeAlias
Get the input type for this Runnable.

 OutputType property ¬∂

OutputType: type[str]
Get the input type for this Runnable.

 input_schema property ¬∂

input_schema: type[BaseModel]
The type of input this Runnable accepts specified as a Pydantic model.

 output_schema property ¬∂

output_schema: type[BaseModel]
Output schema.

The type of output this Runnable produces specified as a Pydantic model.

 config_specs property ¬∂

config_specs: list[ConfigurableFieldSpec]
List configurable fields for this Runnable.

 lc_attributes property ¬∂

lc_attributes: dict
List of attribute names that should be included in the serialized kwargs.

These attributes must be accepted by the constructor.

Default is an empty dictionary.

 cache class-attribute instance-attribute ¬∂

cache: BaseCache | bool | None = Field(default=None, exclude=True)
Whether to cache the response.

If True, will use the global cache.
If False, will not use a cache
If None, will use the global cache if it's set, otherwise no cache.
If instance of BaseCache, will use the provided cache.
Caching is not currently supported for streaming methods of models.

 verbose class-attribute instance-attribute ¬∂

verbose: bool = Field(default_factory=_get_verbosity, exclude=True, repr=False)
Whether to print out response text.

 callbacks class-attribute instance-attribute ¬∂

callbacks: Callbacks = Field(default=None, exclude=True)
Callbacks to add to the run trace.

 tags class-attribute instance-attribute ¬∂

tags: list[str] | None = Field(default=None, exclude=True)
Tags to add to the run trace.

 metadata class-attribute instance-attribute ¬∂

metadata: dict[str, Any] | None = Field(default=None, exclude=True)
Metadata to add to the run trace.

 custom_get_token_ids class-attribute instance-attribute ¬∂

custom_get_token_ids: Callable[[str], list[int]] | None = Field(
    default=None, exclude=True
)
Optional encoder to use for counting tokens.

 model class-attribute instance-attribute ¬∂

model: str = Field(...)
Model name to use.

 google_api_key class-attribute instance-attribute ¬∂

google_api_key: SecretStr | None = Field(
    alias="api_key",
    default_factory=secret_from_env(["GOOGLE_API_KEY", "GEMINI_API_KEY"], default=None),
)
Google AI API key.

If not specified, will check the env vars GOOGLE_API_KEY and GEMINI_API_KEY with precedence given to GOOGLE_API_KEY.

 credentials class-attribute instance-attribute ¬∂

credentials: Any = None
The default custom credentials to use when making API calls.

If not provided, credentials will be ascertained from the GOOGLE_API_KEY or GEMINI_API_KEY env vars with precedence given to GOOGLE_API_KEY.

 temperature class-attribute instance-attribute ¬∂

temperature: float = 0.7
Run inference with this temperature.

Must be within [0.0, 2.0].

Gemini 3.0+ models

Setting temperature < 1.0 for Gemini 3.0+ models can cause infinite loops, degraded reasoning performance, and failure on complex tasks.

 top_p class-attribute instance-attribute ¬∂

top_p: float | None = None
Decode using nucleus sampling.

Consider the smallest set of tokens whose probability sum is at least top_p.

Must be within [0.0, 1.0].

 top_k class-attribute instance-attribute ¬∂

top_k: int | None = None
Decode using top-k sampling: consider the set of top_k most probable tokens.

Must be positive.

 max_output_tokens class-attribute instance-attribute ¬∂

max_output_tokens: int | None = Field(default=None, alias='max_tokens')
Maximum number of tokens to include in a candidate.

Must be greater than zero.

If unset, will use the model's default value, which varies by model.

See docs for model-specific limits.

To constrain the number of thinking tokens to use when generating a response, see the thinking_budget parameter.

 n class-attribute instance-attribute ¬∂

n: int = 1
Number of chat completions to generate for each prompt.

Note that the API may not return the full n completions if duplicates are generated.

 max_retries class-attribute instance-attribute ¬∂

max_retries: int = Field(default=6, alias='retries')
The maximum number of retries to make when generating.

 timeout class-attribute instance-attribute ¬∂

timeout: float | None = Field(default=None, alias='request_timeout')
The maximum number of seconds to wait for a response.

 client_options class-attribute instance-attribute ¬∂

client_options: dict | None = Field(default=None)
A dictionary of client options to pass to the Google API client.

Example: api_endpoint

Warning

If both client_options['api_endpoint'] and base_url are specified, the api_endpoint in client_options takes precedence.

 base_url class-attribute instance-attribute ¬∂

base_url: str | None = Field(default=None)
Base URL to use for the API client.

This is a convenience alias for client_options['api_endpoint'].

REST transport (transport="rest"): Accepts full URLs with paths

https://api.example.com/v1/path
https://webhook.site/unique-path
gRPC transports (transport="grpc" or transport="grpc_asyncio"): Only accepts hostname:port format

api.example.com:443
custom.googleapis.com:443
https://api.example.com (auto-formatted to api.example.com:443)
NOT https://webhook.site/path (paths are not supported in gRPC)
NOT api.example.com/path (paths are not supported in gRPC)
Warning

If client_options already contains an api_endpoint, this parameter will be ignored in favor of the existing value.

 transport class-attribute instance-attribute ¬∂

transport: str | None = Field(default=None, alias='api_transport')
A string, one of: ['rest', 'grpc', 'grpc_asyncio'].

The Google client library defaults to 'grpc' for sync clients.

For async clients, 'rest' is converted to 'grpc_asyncio' unless a custom endpoint is specified.

 additional_headers class-attribute instance-attribute ¬∂

additional_headers: dict[str, str] | None = Field(default=None)
Key-value dictionary representing additional headers for the model call

 response_modalities class-attribute instance-attribute ¬∂

response_modalities: list[Modality] | None = Field(default=None)
A list of modalities of the response

 media_resolution class-attribute instance-attribute ¬∂

media_resolution: MediaResolution | None = Field(default=None)
Media resolution for the input media.

May be defined at the individual part level, allowing for mixed-resolution requests (e.g., images and videos of different resolutions in the same request).

May be 'low', 'medium', or 'high'.

Can be set either per-part or globally for all media inputs in the request. To set globally, set in the generation_config.

Model compatibility

Setting per-part media resolution requests to Gemini 2.5 models is not supported.

 thinking_budget class-attribute instance-attribute ¬∂

thinking_budget: int | None = Field(default=None)
Indicates the thinking budget in tokens.

Used to disable thinking for supported models (when set to 0) or to constrain the number of tokens used for thinking.

Dynamic thinking (allowing the model to decide how many tokens to use) is enabled when set to -1.

More information, including per-model limits, can be found in the Gemini API docs.

 include_thoughts class-attribute instance-attribute ¬∂

include_thoughts: bool | None = Field(default=None)
Indicates whether to include thoughts in the response.

Note

This parameter is only applicable for models that support thinking.

This does not disable thinking; to disable thinking, set thinking_budget to 0. for supported models. See the thinking_budget parameter for more details.

 safety_settings class-attribute instance-attribute ¬∂

safety_settings: dict[HarmCategory, HarmBlockThreshold] | None = None
Default safety settings to use for all generations.

Example


from google.generativeai.types.safety_types import HarmBlockThreshold, HarmCategory

safety_settings = {
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
}
 get_name ¬∂

get_name(suffix: str | None = None, *, name: str | None = None) -> str
Get the name of the Runnable.

PARAMETER	DESCRIPTION
suffix	An optional suffix to append to the name.
TYPE: str | NoneDEFAULT: None

name	An optional name to use instead of the Runnable's name.
TYPE: str | NoneDEFAULT: None

RETURNS	DESCRIPTION
str	The name of the Runnable.
 get_input_schema ¬∂

get_input_schema(config: RunnableConfig | None = None) -> type[BaseModel]
Get a Pydantic model that can be used to validate input to the Runnable.

Runnable objects that leverage the configurable_fields and configurable_alternatives methods will have a dynamic input schema that depends on which configuration the Runnable is invoked with.

This method allows to get an input schema for a specific configuration.

PARAMETER	DESCRIPTION
config	A config to use when generating the schema.
TYPE: RunnableConfig | NoneDEFAULT: None

RETURNS	DESCRIPTION
type[BaseModel]	A Pydantic model that can be used to validate input.
 get_input_jsonschema ¬∂

get_input_jsonschema(config: RunnableConfig | None = None) -> dict[str, Any]
Get a JSON schema that represents the input to the Runnable.

PARAMETER	DESCRIPTION
config	A config to use when generating the schema.
TYPE: RunnableConfig | NoneDEFAULT: None

RETURNS	DESCRIPTION
dict[str, Any]	A JSON schema that represents the input to the Runnable.
Example

from langchain_core.runnables import RunnableLambda


def add_one(x: int) -> int:
    return x + 1


runnable = RunnableLambda(add_one)

print(runnable.get_input_jsonschema())
Added in langchain-core 0.3.0

 get_output_schema ¬∂

get_output_schema(config: RunnableConfig | None = None) -> type[BaseModel]
Get a Pydantic model that can be used to validate output to the Runnable.

Runnable objects that leverage the configurable_fields and configurable_alternatives methods will have a dynamic output schema that depends on which configuration the Runnable is invoked with.

This method allows to get an output schema for a specific configuration.

PARAMETER	DESCRIPTION
config	A config to use when generating the schema.
TYPE: RunnableConfig | NoneDEFAULT: None

RETURNS	DESCRIPTION
type[BaseModel]	A Pydantic model that can be used to validate output.
 get_output_jsonschema ¬∂

get_output_jsonschema(config: RunnableConfig | None = None) -> dict[str, Any]
Get a JSON schema that represents the output of the Runnable.

PARAMETER	DESCRIPTION
config	A config to use when generating the schema.
TYPE: RunnableConfig | NoneDEFAULT: None

RETURNS	DESCRIPTION
dict[str, Any]	A JSON schema that represents the output of the Runnable.
Example

from langchain_core.runnables import RunnableLambda


def add_one(x: int) -> int:
    return x + 1


runnable = RunnableLambda(add_one)

print(runnable.get_output_jsonschema())
Added in langchain-core 0.3.0

 config_schema ¬∂

config_schema(*, include: Sequence[str] | None = None) -> type[BaseModel]
The type of config this Runnable accepts specified as a Pydantic model.

To mark a field as configurable, see the configurable_fields and configurable_alternatives methods.

PARAMETER	DESCRIPTION
include	A list of fields to include in the config schema.
TYPE: Sequence[str] | NoneDEFAULT: None

RETURNS	DESCRIPTION
type[BaseModel]	A Pydantic model that can be used to validate config.
 get_config_jsonschema ¬∂

get_config_jsonschema(*, include: Sequence[str] | None = None) -> dict[str, Any]
Get a JSON schema that represents the config of the Runnable.

PARAMETER	DESCRIPTION
include	A list of fields to include in the config schema.
TYPE: Sequence[str] | NoneDEFAULT: None

RETURNS	DESCRIPTION
dict[str, Any]	A JSON schema that represents the config of the Runnable.
Added in langchain-core 0.3.0

 get_graph ¬∂

get_graph(config: RunnableConfig | None = None) -> Graph
Return a graph representation of this Runnable.

 get_prompts ¬∂

get_prompts(config: RunnableConfig | None = None) -> list[BasePromptTemplate]
Return a list of prompts used by this Runnable.

 __or__ ¬∂

__or__(
    other: Runnable[Any, Other]
    | Callable[[Iterator[Any]], Iterator[Other]]
    | Callable[[AsyncIterator[Any]], AsyncIterator[Other]]
    | Callable[[Any], Other]
    | Mapping[str, Runnable[Any, Other] | Callable[[Any], Other] | Any],
) -> RunnableSerializable[Input, Other]
Runnable "or" operator.

Compose this Runnable with another object to create a RunnableSequence.

PARAMETER	DESCRIPTION
other	Another Runnable or a Runnable-like object.
TYPE: Runnable[Any, Other] | Callable[[Iterator[Any]], Iterator[Other]] | Callable[[AsyncIterator[Any]], AsyncIterator[Other]] | Callable[[Any], Other] | Mapping[str, Runnable[Any, Other] | Callable[[Any], Other] | Any]

RETURNS	DESCRIPTION
RunnableSerializable[Input, Other]	A new Runnable.
 __ror__ ¬∂

__ror__(
    other: Runnable[Other, Any]
    | Callable[[Iterator[Other]], Iterator[Any]]
    | Callable[[AsyncIterator[Other]], AsyncIterator[Any]]
    | Callable[[Other], Any]
    | Mapping[str, Runnable[Other, Any] | Callable[[Other], Any] | Any],
) -> RunnableSerializable[Other, Output]
Runnable "reverse-or" operator.

Compose this Runnable with another object to create a RunnableSequence.

PARAMETER	DESCRIPTION
other	Another Runnable or a Runnable-like object.
TYPE: Runnable[Other, Any] | Callable[[Iterator[Other]], Iterator[Any]] | Callable[[AsyncIterator[Other]], AsyncIterator[Any]] | Callable[[Other], Any] | Mapping[str, Runnable[Other, Any] | Callable[[Other], Any] | Any]

RETURNS	DESCRIPTION
RunnableSerializable[Other, Output]	A new Runnable.
 pipe ¬∂

pipe(
    *others: Runnable[Any, Other] | Callable[[Any], Other], name: str | None = None
) -> RunnableSerializable[Input, Other]
Pipe Runnable objects.

Compose this Runnable with Runnable-like objects to make a RunnableSequence.

Equivalent to RunnableSequence(self, *others) or self | others[0] | ...

Example

from langchain_core.runnables import RunnableLambda


def add_one(x: int) -> int:
    return x + 1


def mul_two(x: int) -> int:
    return x * 2


runnable_1 = RunnableLambda(add_one)
runnable_2 = RunnableLambda(mul_two)
sequence = runnable_1.pipe(runnable_2)
# Or equivalently:
# sequence = runnable_1 | runnable_2
# sequence = RunnableSequence(first=runnable_1, last=runnable_2)
sequence.invoke(1)
await sequence.ainvoke(1)
# -> 4

sequence.batch([1, 2, 3])
await sequence.abatch([1, 2, 3])
# -> [4, 6, 8]
PARAMETER	DESCRIPTION
*others	Other Runnable or Runnable-like objects to compose
TYPE: Runnable[Any, Other] | Callable[[Any], Other]DEFAULT: ()

name	An optional name for the resulting RunnableSequence.
TYPE: str | NoneDEFAULT: None

RETURNS	DESCRIPTION
RunnableSerializable[Input, Other]	A new Runnable.
 pick ¬∂

pick(keys: str | list[str]) -> RunnableSerializable[Any, Any]
Pick keys from the output dict of this Runnable.

Pick a single key


import json

from langchain_core.runnables import RunnableLambda, RunnableMap

as_str = RunnableLambda(str)
as_json = RunnableLambda(json.loads)
chain = RunnableMap(str=as_str, json=as_json)

chain.invoke("[1, 2, 3]")
# -> {"str": "[1, 2, 3]", "json": [1, 2, 3]}

json_only_chain = chain.pick("json")
json_only_chain.invoke("[1, 2, 3]")
# -> [1, 2, 3]
Pick a list of keys


from typing import Any

import json

from langchain_core.runnables import RunnableLambda, RunnableMap

as_str = RunnableLambda(str)
as_json = RunnableLambda(json.loads)


def as_bytes(x: Any) -> bytes:
    return bytes(x, "utf-8")


chain = RunnableMap(
    str=as_str, json=as_json, bytes=RunnableLambda(as_bytes)
)

chain.invoke("[1, 2, 3]")
# -> {"str": "[1, 2, 3]", "json": [1, 2, 3], "bytes": b"[1, 2, 3]"}

json_and_bytes_chain = chain.pick(["json", "bytes"])
json_and_bytes_chain.invoke("[1, 2, 3]")
# -> {"json": [1, 2, 3], "bytes": b"[1, 2, 3]"}
PARAMETER	DESCRIPTION
keys	A key or list of keys to pick from the output dict.
TYPE: str | list[str]

RETURNS	DESCRIPTION
RunnableSerializable[Any, Any]	a new Runnable.
 assign ¬∂

assign(
    **kwargs: Runnable[dict[str, Any], Any]
    | Callable[[dict[str, Any]], Any]
    | Mapping[str, Runnable[dict[str, Any], Any] | Callable[[dict[str, Any]], Any]],
) -> RunnableSerializable[Any, Any]
Assigns new fields to the dict output of this Runnable.


from langchain_core.language_models.fake import FakeStreamingListLLM
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import SystemMessagePromptTemplate
from langchain_core.runnables import Runnable
from operator import itemgetter

prompt = (
    SystemMessagePromptTemplate.from_template("You are a nice assistant.")
    + "{question}"
)
model = FakeStreamingListLLM(responses=["foo-lish"])

chain: Runnable = prompt | model | {"str": StrOutputParser()}

chain_with_assign = chain.assign(hello=itemgetter("str") | model)

print(chain_with_assign.input_schema.model_json_schema())
# {'title': 'PromptInput', 'type': 'object', 'properties':
{'question': {'title': 'Question', 'type': 'string'}}}
print(chain_with_assign.output_schema.model_json_schema())
# {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':
{'str': {'title': 'Str',
'type': 'string'}, 'hello': {'title': 'Hello', 'type': 'string'}}}
PARAMETER	DESCRIPTION
**kwargs	A mapping of keys to Runnable or Runnable-like objects that will be invoked with the entire output dict of this Runnable.
TYPE: Runnable[dict[str, Any], Any] | Callable[[dict[str, Any]], Any] | Mapping[str, Runnable[dict[str, Any], Any] | Callable[[dict[str, Any]], Any]]DEFAULT: {}

RETURNS	DESCRIPTION
RunnableSerializable[Any, Any]	A new Runnable.
 invoke ¬∂

invoke(
    input: LanguageModelInput,
    config: RunnableConfig | None = None,
    *,
    stop: list[str] | None = None,
    **kwargs: Any,
) -> str
Transform a single input into an output.

PARAMETER	DESCRIPTION
input	The input to the Runnable.
TYPE: Input

config	A config to use when invoking the Runnable.
The config supports standard keys like 'tags', 'metadata' for tracing purposes, 'max_concurrency' for controlling how much work to do in parallel, and other keys.

Please refer to RunnableConfig for more details.

TYPE: RunnableConfig | NoneDEFAULT: None

RETURNS	DESCRIPTION
Output	The output of the Runnable.
 ainvoke async ¬∂

ainvoke(
    input: LanguageModelInput,
    config: RunnableConfig | None = None,
    *,
    stop: list[str] | None = None,
    **kwargs: Any,
) -> str
Transform a single input into an output.

PARAMETER	DESCRIPTION
input	The input to the Runnable.
TYPE: Input

config	A config to use when invoking the Runnable.
The config supports standard keys like 'tags', 'metadata' for tracing purposes, 'max_concurrency' for controlling how much work to do in parallel, and other keys.

Please refer to RunnableConfig for more details.

TYPE: RunnableConfig | NoneDEFAULT: None

RETURNS	DESCRIPTION
Output	The output of the Runnable.
 batch ¬∂

batch(
    inputs: list[LanguageModelInput],
    config: RunnableConfig | list[RunnableConfig] | None = None,
    *,
    return_exceptions: bool = False,
    **kwargs: Any,
) -> list[str]
Default implementation runs invoke in parallel using a thread pool executor.

The default implementation of batch works well for IO bound runnables.

Subclasses must override this method if they can batch more efficiently; e.g., if the underlying Runnable uses an API which supports a batch mode.

PARAMETER	DESCRIPTION
inputs	A list of inputs to the Runnable.
TYPE: list[Input]

config	A config to use when invoking the Runnable. The config supports standard keys like 'tags', 'metadata' for tracing purposes, 'max_concurrency' for controlling how much work to do in parallel, and other keys.
Please refer to RunnableConfig for more details.

TYPE: RunnableConfig | list[RunnableConfig] | NoneDEFAULT: None

return_exceptions	Whether to return exceptions instead of raising them.
TYPE: boolDEFAULT: False

**kwargs	Additional keyword arguments to pass to the Runnable.
TYPE: Any | NoneDEFAULT: {}

RETURNS	DESCRIPTION
list[Output]	A list of outputs from the Runnable.
 batch_as_completed ¬∂

batch_as_completed(
    inputs: Sequence[Input],
    config: RunnableConfig | Sequence[RunnableConfig] | None = None,
    *,
    return_exceptions: bool = False,
    **kwargs: Any | None,
) -> Iterator[tuple[int, Output | Exception]]
Run invoke in parallel on a list of inputs.

Yields results as they complete.

PARAMETER	DESCRIPTION
inputs	A list of inputs to the Runnable.
TYPE: Sequence[Input]

config	A config to use when invoking the Runnable.
The config supports standard keys like 'tags', 'metadata' for tracing purposes, 'max_concurrency' for controlling how much work to do in parallel, and other keys.

Please refer to RunnableConfig for more details.

TYPE: RunnableConfig | Sequence[RunnableConfig] | NoneDEFAULT: None

return_exceptions	Whether to return exceptions instead of raising them.
TYPE: boolDEFAULT: False

**kwargs	Additional keyword arguments to pass to the Runnable.
TYPE: Any | NoneDEFAULT: {}

YIELDS	DESCRIPTION
tuple[int, Output | Exception]	Tuples of the index of the input and the output from the Runnable.
 abatch async ¬∂

abatch(
    inputs: list[LanguageModelInput],
    config: RunnableConfig | list[RunnableConfig] | None = None,
    *,
    return_exceptions: bool = False,
    **kwargs: Any,
) -> list[str]
Default implementation runs ainvoke in parallel using asyncio.gather.

The default implementation of batch works well for IO bound runnables.

Subclasses must override this method if they can batch more efficiently; e.g., if the underlying Runnable uses an API which supports a batch mode.

PARAMETER	DESCRIPTION
inputs	A list of inputs to the Runnable.
TYPE: list[Input]

config	A config to use when invoking the Runnable.
The config supports standard keys like 'tags', 'metadata' for tracing purposes, 'max_concurrency' for controlling how much work to do in parallel, and other keys.

Please refer to RunnableConfig for more details.

TYPE: RunnableConfig | list[RunnableConfig] | NoneDEFAULT: None

return_exceptions	Whether to return exceptions instead of raising them.
TYPE: boolDEFAULT: False

**kwargs	Additional keyword arguments to pass to the Runnable.
TYPE: Any | NoneDEFAULT: {}

RETURNS	DESCRIPTION
list[Output]	A list of outputs from the Runnable.
 abatch_as_completed async ¬∂

abatch_as_completed(
    inputs: Sequence[Input],
    config: RunnableConfig | Sequence[RunnableConfig] | None = None,
    *,
    return_exceptions: bool = False,
    **kwargs: Any | None,
) -> AsyncIterator[tuple[int, Output | Exception]]
Run ainvoke in parallel on a list of inputs.

Yields results as they complete.

PARAMETER	DESCRIPTION
inputs	A list of inputs to the Runnable.
TYPE: Sequence[Input]

config	A config to use when invoking the Runnable.
The config supports standard keys like 'tags', 'metadata' for tracing purposes, 'max_concurrency' for controlling how much work to do in parallel, and other keys.

Please refer to RunnableConfig for more details.

TYPE: RunnableConfig | Sequence[RunnableConfig] | NoneDEFAULT: None

return_exceptions	Whether to return exceptions instead of raising them.
TYPE: boolDEFAULT: False

**kwargs	Additional keyword arguments to pass to the Runnable.
TYPE: Any | NoneDEFAULT: {}

YIELDS	DESCRIPTION
AsyncIterator[tuple[int, Output | Exception]]	A tuple of the index of the input and the output from the Runnable.
 stream ¬∂

stream(
    input: LanguageModelInput,
    config: RunnableConfig | None = None,
    *,
    stop: list[str] | None = None,
    **kwargs: Any,
) -> Iterator[str]
Default implementation of stream, which calls invoke.

Subclasses must override this method if they support streaming output.

PARAMETER	DESCRIPTION
input	The input to the Runnable.
TYPE: Input

config	The config to use for the Runnable.
TYPE: RunnableConfig | NoneDEFAULT: None

**kwargs	Additional keyword arguments to pass to the Runnable.
TYPE: Any | NoneDEFAULT: {}

YIELDS	DESCRIPTION
Output	The output of the Runnable.
 astream async ¬∂

astream(
    input: LanguageModelInput,
    config: RunnableConfig | None = None,
    *,
    stop: list[str] | None = None,
    **kwargs: Any,
) -> AsyncIterator[str]
Default implementation of astream, which calls ainvoke.

Subclasses must override this method if they support streaming output.

PARAMETER	DESCRIPTION
input	The input to the Runnable.
TYPE: Input

config	The config to use for the Runnable.
TYPE: RunnableConfig | NoneDEFAULT: None

**kwargs	Additional keyword arguments to pass to the Runnable.
TYPE: Any | NoneDEFAULT: {}

YIELDS	DESCRIPTION
AsyncIterator[Output]	The output of the Runnable.
 astream_log async ¬∂

astream_log(
    input: Any,
    config: RunnableConfig | None = None,
    *,
    diff: bool = True,
    with_streamed_output_list: bool = True,
    include_names: Sequence[str] | None = None,
    include_types: Sequence[str] | None = None,
    include_tags: Sequence[str] | None = None,
    exclude_names: Sequence[str] | None = None,
    exclude_types: Sequence[str] | None = None,
    exclude_tags: Sequence[str] | None = None,
    **kwargs: Any,
) -> AsyncIterator[RunLogPatch] | AsyncIterator[RunLog]
Stream all output from a Runnable, as reported to the callback system.

This includes all inner runs of LLMs, Retrievers, Tools, etc.

Output is streamed as Log objects, which include a list of Jsonpatch ops that describe how the state of the run has changed in each step, and the final state of the run.

The Jsonpatch ops can be applied in order to construct state.

PARAMETER	DESCRIPTION
input	The input to the Runnable.
TYPE: Any

config	The config to use for the Runnable.
TYPE: RunnableConfig | NoneDEFAULT: None

diff	Whether to yield diffs between each step or the current state.
TYPE: boolDEFAULT: True

with_streamed_output_list	Whether to yield the streamed_output list.
TYPE: boolDEFAULT: True

include_names	Only include logs with these names.
TYPE: Sequence[str] | NoneDEFAULT: None

include_types	Only include logs with these types.
TYPE: Sequence[str] | NoneDEFAULT: None

include_tags	Only include logs with these tags.
TYPE: Sequence[str] | NoneDEFAULT: None

exclude_names	Exclude logs with these names.
TYPE: Sequence[str] | NoneDEFAULT: None

exclude_types	Exclude logs with these types.
TYPE: Sequence[str] | NoneDEFAULT: None

exclude_tags	Exclude logs with these tags.
TYPE: Sequence[str] | NoneDEFAULT: None

**kwargs	Additional keyword arguments to pass to the Runnable.
TYPE: AnyDEFAULT: {}

YIELDS	DESCRIPTION
AsyncIterator[RunLogPatch] | AsyncIterator[RunLog]	A RunLogPatch or RunLog object.
 astream_events async ¬∂

astream_events(
    input: Any,
    config: RunnableConfig | None = None,
    *,
    version: Literal["v1", "v2"] = "v2",
    include_names: Sequence[str] | None = None,
    include_types: Sequence[str] | None = None,
    include_tags: Sequence[str] | None = None,
    exclude_names: Sequence[str] | None = None,
    exclude_types: Sequence[str] | None = None,
    exclude_tags: Sequence[str] | None = None,
    **kwargs: Any,
) -> AsyncIterator[StreamEvent]
Generate a stream of events.

Use to create an iterator over StreamEvent that provide real-time information about the progress of the Runnable, including StreamEvent from intermediate results.

A StreamEvent is a dictionary with the following schema:

event: Event names are of the format: on_[runnable_type]_(start|stream|end).
name: The name of the Runnable that generated the event.
run_id: Randomly generated ID associated with the given execution of the Runnable that emitted the event. A child Runnable that gets invoked as part of the execution of a parent Runnable is assigned its own unique ID.
parent_ids: The IDs of the parent runnables that generated the event. The root Runnable will have an empty list. The order of the parent IDs is from the root to the immediate parent. Only available for v2 version of the API. The v1 version of the API will return an empty list.
tags: The tags of the Runnable that generated the event.
metadata: The metadata of the Runnable that generated the event.
data: The data associated with the event. The contents of this field depend on the type of event. See the table below for more details.
Below is a table that illustrates some events that might be emitted by various chains. Metadata fields have been omitted from the table for brevity. Chain definitions have been included after the table.

Note

This reference table is for the v2 version of the schema.

event	name	chunk	input	output
on_chat_model_start	'[model name]'		{"messages": [[SystemMessage, HumanMessage]]}	
on_chat_model_stream	'[model name]'	AIMessageChunk(content="hello")		
on_chat_model_end	'[model name]'		{"messages": [[SystemMessage, HumanMessage]]}	AIMessageChunk(content="hello world")
on_llm_start	'[model name]'		{'input': 'hello'}	
on_llm_stream	'[model name]'	'Hello'		
on_llm_end	'[model name]'		'Hello human!'	
on_chain_start	'format_docs'			
on_chain_stream	'format_docs'	'hello world!, goodbye world!'		
on_chain_end	'format_docs'		[Document(...)]	'hello world!, goodbye world!'
on_tool_start	'some_tool'		{"x": 1, "y": "2"}	
on_tool_end	'some_tool'			{"x": 1, "y": "2"}
on_retriever_start	'[retriever name]'		{"query": "hello"}	
on_retriever_end	'[retriever name]'		{"query": "hello"}	[Document(...), ..]
on_prompt_start	'[template_name]'		{"question": "hello"}	
on_prompt_end	'[template_name]'		{"question": "hello"}	ChatPromptValue(messages: [SystemMessage, ...])
In addition to the standard events, users can also dispatch custom events (see example below).

Custom events will be only be surfaced with in the v2 version of the API!

A custom event has following format:

Attribute	Type	Description
name	str	A user defined name for the event.
data	Any	The data associated with the event. This can be anything, though we suggest making it JSON serializable.
Here are declarations associated with the standard events shown above:

format_docs:


def format_docs(docs: list[Document]) -> str:
    '''Format the docs.'''
    return ", ".join([doc.page_content for doc in docs])


format_docs = RunnableLambda(format_docs)
some_tool:


@tool
def some_tool(x: int, y: str) -> dict:
    '''Some_tool.'''
    return {"x": x, "y": y}
prompt:


template = ChatPromptTemplate.from_messages(
    [
        ("system", "You are Cat Agent 007"),
        ("human", "{question}"),
    ]
).with_config({"run_name": "my_template", "tags": ["my_template"]})
Example


from langchain_core.runnables import RunnableLambda


async def reverse(s: str) -> str:
    return s[::-1]


chain = RunnableLambda(func=reverse)

events = [
    event async for event in chain.astream_events("hello", version="v2")
]

# Will produce the following events
# (run_id, and parent_ids has been omitted for brevity):
[
    {
        "data": {"input": "hello"},
        "event": "on_chain_start",
        "metadata": {},
        "name": "reverse",
        "tags": [],
    },
    {
        "data": {"chunk": "olleh"},
        "event": "on_chain_stream",
        "metadata": {},
        "name": "reverse",
        "tags": [],
    },
    {
        "data": {"output": "olleh"},
        "event": "on_chain_end",
        "metadata": {},
        "name": "reverse",
        "tags": [],
    },
]
Dispatch custom event

from langchain_core.callbacks.manager import (
    adispatch_custom_event,
)
from langchain_core.runnables import RunnableLambda, RunnableConfig
import asyncio


async def slow_thing(some_input: str, config: RunnableConfig) -> str:
    """Do something that takes a long time."""
    await asyncio.sleep(1) # Placeholder for some slow operation
    await adispatch_custom_event(
        "progress_event",
        {"message": "Finished step 1 of 3"},
        config=config # Must be included for python < 3.10
    )
    await asyncio.sleep(1) # Placeholder for some slow operation
    await adispatch_custom_event(
        "progress_event",
        {"message": "Finished step 2 of 3"},
        config=config # Must be included for python < 3.10
    )
    await asyncio.sleep(1) # Placeholder for some slow operation
    return "Done"

slow_thing = RunnableLambda(slow_thing)

async for event in slow_thing.astream_events("some_input", version="v2"):
    print(event)
PARAMETER	DESCRIPTION
input	The input to the Runnable.
TYPE: Any

config	The config to use for the Runnable.
TYPE: RunnableConfig | NoneDEFAULT: None

version	The version of the schema to use, either 'v2' or 'v1'.
Users should use 'v2'.

'v1' is for backwards compatibility and will be deprecated in 0.4.0.

No default will be assigned until the API is stabilized. custom events will only be surfaced in 'v2'.

TYPE: Literal['v1', 'v2']DEFAULT: 'v2'

include_names	Only include events from Runnable objects with matching names.
TYPE: Sequence[str] | NoneDEFAULT: None

include_types	Only include events from Runnable objects with matching types.
TYPE: Sequence[str] | NoneDEFAULT: None

include_tags	Only include events from Runnable objects with matching tags.
TYPE: Sequence[str] | NoneDEFAULT: None

exclude_names	Exclude events from Runnable objects with matching names.
TYPE: Sequence[str] | NoneDEFAULT: None

exclude_types	Exclude events from Runnable objects with matching types.
TYPE: Sequence[str] | NoneDEFAULT: None

exclude_tags	Exclude events from Runnable objects with matching tags.
TYPE: Sequence[str] | NoneDEFAULT: None

**kwargs	Additional keyword arguments to pass to the Runnable.
These will be passed to astream_log as this implementation of astream_events is built on top of astream_log.

TYPE: AnyDEFAULT: {}

YIELDS	DESCRIPTION
AsyncIterator[StreamEvent]	An async stream of StreamEvent.
RAISES	DESCRIPTION
NotImplementedError	If the version is not 'v1' or 'v2'.
 transform ¬∂

transform(
    input: Iterator[Input], config: RunnableConfig | None = None, **kwargs: Any | None
) -> Iterator[Output]
Transform inputs to outputs.

Default implementation of transform, which buffers input and calls astream.

Subclasses must override this method if they can start producing output while input is still being generated.

PARAMETER	DESCRIPTION
input	An iterator of inputs to the Runnable.
TYPE: Iterator[Input]

config	The config to use for the Runnable.
TYPE: RunnableConfig | NoneDEFAULT: None

**kwargs	Additional keyword arguments to pass to the Runnable.
TYPE: Any | NoneDEFAULT: {}

YIELDS	DESCRIPTION
Output	The output of the Runnable.
 atransform async ¬∂

atransform(
    input: AsyncIterator[Input],
    config: RunnableConfig | None = None,
    **kwargs: Any | None,
) -> AsyncIterator[Output]
Transform inputs to outputs.

Default implementation of atransform, which buffers input and calls astream.

Subclasses must override this method if they can start producing output while input is still being generated.

PARAMETER	DESCRIPTION
input	An async iterator of inputs to the Runnable.
TYPE: AsyncIterator[Input]

config	The config to use for the Runnable.
TYPE: RunnableConfig | NoneDEFAULT: None

**kwargs	Additional keyword arguments to pass to the Runnable.
TYPE: Any | NoneDEFAULT: {}

YIELDS	DESCRIPTION
AsyncIterator[Output]	The output of the Runnable.
 bind ¬∂

bind(**kwargs: Any) -> Runnable[Input, Output]
Bind arguments to a Runnable, returning a new Runnable.

Useful when a Runnable in a chain requires an argument that is not in the output of the previous Runnable or included in the user input.

PARAMETER	DESCRIPTION
**kwargs	The arguments to bind to the Runnable.
TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
Runnable[Input, Output]	A new Runnable with the arguments bound.
Example

from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser

model = ChatOllama(model="llama3.1")

# Without bind
chain = model | StrOutputParser()

chain.invoke("Repeat quoted words exactly: 'One two three four five.'")
# Output is 'One two three four five.'

# With bind
chain = model.bind(stop=["three"]) | StrOutputParser()

chain.invoke("Repeat quoted words exactly: 'One two three four five.'")
# Output is 'One two'
 with_config ¬∂

with_config(
    config: RunnableConfig | None = None, **kwargs: Any
) -> Runnable[Input, Output]
Bind config to a Runnable, returning a new Runnable.

PARAMETER	DESCRIPTION
config	The config to bind to the Runnable.
TYPE: RunnableConfig | NoneDEFAULT: None

**kwargs	Additional keyword arguments to pass to the Runnable.
TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
Runnable[Input, Output]	A new Runnable with the config bound.
 with_listeners ¬∂

with_listeners(
    *,
    on_start: Callable[[Run], None]
    | Callable[[Run, RunnableConfig], None]
    | None = None,
    on_end: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None,
    on_error: Callable[[Run], None]
    | Callable[[Run, RunnableConfig], None]
    | None = None,
) -> Runnable[Input, Output]
Bind lifecycle listeners to a Runnable, returning a new Runnable.

The Run object contains information about the run, including its id, type, input, output, error, start_time, end_time, and any tags or metadata added to the run.

PARAMETER	DESCRIPTION
on_start	Called before the Runnable starts running, with the Run object.
TYPE: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | NoneDEFAULT: None

on_end	Called after the Runnable finishes running, with the Run object.
TYPE: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | NoneDEFAULT: None

on_error	Called if the Runnable throws an error, with the Run object.
TYPE: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | NoneDEFAULT: None

RETURNS	DESCRIPTION
Runnable[Input, Output]	A new Runnable with the listeners bound.
Example

from langchain_core.runnables import RunnableLambda
from langchain_core.tracers.schemas import Run

import time


def test_runnable(time_to_sleep: int):
    time.sleep(time_to_sleep)


def fn_start(run_obj: Run):
    print("start_time:", run_obj.start_time)


def fn_end(run_obj: Run):
    print("end_time:", run_obj.end_time)


chain = RunnableLambda(test_runnable).with_listeners(
    on_start=fn_start, on_end=fn_end
)
chain.invoke(2)
 with_alisteners ¬∂

with_alisteners(
    *,
    on_start: AsyncListener | None = None,
    on_end: AsyncListener | None = None,
    on_error: AsyncListener | None = None,
) -> Runnable[Input, Output]
Bind async lifecycle listeners to a Runnable.

Returns a new Runnable.

The Run object contains information about the run, including its id, type, input, output, error, start_time, end_time, and any tags or metadata added to the run.

PARAMETER	DESCRIPTION
on_start	Called asynchronously before the Runnable starts running, with the Run object.
TYPE: AsyncListener | NoneDEFAULT: None

on_end	Called asynchronously after the Runnable finishes running, with the Run object.
TYPE: AsyncListener | NoneDEFAULT: None

on_error	Called asynchronously if the Runnable throws an error, with the Run object.
TYPE: AsyncListener | NoneDEFAULT: None

RETURNS	DESCRIPTION
Runnable[Input, Output]	A new Runnable with the listeners bound.
Example

from langchain_core.runnables import RunnableLambda, Runnable
from datetime import datetime, timezone
import time
import asyncio


def format_t(timestamp: float) -> str:
    return datetime.fromtimestamp(timestamp, tz=timezone.utc).isoformat()


async def test_runnable(time_to_sleep: int):
    print(f"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}")
    await asyncio.sleep(time_to_sleep)
    print(f"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}")


async def fn_start(run_obj: Runnable):
    print(f"on start callback starts at {format_t(time.time())}")
    await asyncio.sleep(3)
    print(f"on start callback ends at {format_t(time.time())}")


async def fn_end(run_obj: Runnable):
    print(f"on end callback starts at {format_t(time.time())}")
    await asyncio.sleep(2)
    print(f"on end callback ends at {format_t(time.time())}")


runnable = RunnableLambda(test_runnable).with_alisteners(
    on_start=fn_start, on_end=fn_end
)


async def concurrent_runs():
    await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))


asyncio.run(concurrent_runs())
# Result:
# on start callback starts at 2025-03-01T07:05:22.875378+00:00
# on start callback starts at 2025-03-01T07:05:22.875495+00:00
# on start callback ends at 2025-03-01T07:05:25.878862+00:00
# on start callback ends at 2025-03-01T07:05:25.878947+00:00
# Runnable[2s]: starts at 2025-03-01T07:05:25.879392+00:00
# Runnable[3s]: starts at 2025-03-01T07:05:25.879804+00:00
# Runnable[2s]: ends at 2025-03-01T07:05:27.881998+00:00
# on end callback starts at 2025-03-01T07:05:27.882360+00:00
# Runnable[3s]: ends at 2025-03-01T07:05:28.881737+00:00
# on end callback starts at 2025-03-01T07:05:28.882428+00:00
# on end callback ends at 2025-03-01T07:05:29.883893+00:00
# on end callback ends at 2025-03-01T07:05:30.884831+00:00
 with_types ¬∂

with_types(
    *, input_type: type[Input] | None = None, output_type: type[Output] | None = None
) -> Runnable[Input, Output]
Bind input and output types to a Runnable, returning a new Runnable.

PARAMETER	DESCRIPTION
input_type	The input type to bind to the Runnable.
TYPE: type[Input] | NoneDEFAULT: None

output_type	The output type to bind to the Runnable.
TYPE: type[Output] | NoneDEFAULT: None

RETURNS	DESCRIPTION
Runnable[Input, Output]	A new Runnable with the types bound.
 with_retry ¬∂

with_retry(
    *,
    retry_if_exception_type: tuple[type[BaseException], ...] = (Exception,),
    wait_exponential_jitter: bool = True,
    exponential_jitter_params: ExponentialJitterParams | None = None,
    stop_after_attempt: int = 3,
) -> Runnable[Input, Output]
Create a new Runnable that retries the original Runnable on exceptions.

PARAMETER	DESCRIPTION
retry_if_exception_type	A tuple of exception types to retry on.
TYPE: tuple[type[BaseException], ...]DEFAULT: (Exception,)

wait_exponential_jitter	Whether to add jitter to the wait time between retries.
TYPE: boolDEFAULT: True

stop_after_attempt	The maximum number of attempts to make before giving up.
TYPE: intDEFAULT: 3

exponential_jitter_params	Parameters for tenacity.wait_exponential_jitter. Namely: initial, max, exp_base, and jitter (all float values).
TYPE: ExponentialJitterParams | NoneDEFAULT: None

RETURNS	DESCRIPTION
Runnable[Input, Output]	A new Runnable that retries the original Runnable on exceptions.
Example

from langchain_core.runnables import RunnableLambda

count = 0


def _lambda(x: int) -> None:
    global count
    count = count + 1
    if x == 1:
        raise ValueError("x is 1")
    else:
        pass


runnable = RunnableLambda(_lambda)
try:
    runnable.with_retry(
        stop_after_attempt=2,
        retry_if_exception_type=(ValueError,),
    ).invoke(1)
except ValueError:
    pass

assert count == 2
 map ¬∂

map() -> Runnable[list[Input], list[Output]]
Return a new Runnable that maps a list of inputs to a list of outputs.

Calls invoke with each input.

RETURNS	DESCRIPTION
Runnable[list[Input], list[Output]]	A new Runnable that maps a list of inputs to a list of outputs.
Example

from langchain_core.runnables import RunnableLambda


def _lambda(x: int) -> int:
    return x + 1


runnable = RunnableLambda(_lambda)
print(runnable.map().invoke([1, 2, 3]))  # [2, 3, 4]
 with_fallbacks ¬∂

with_fallbacks(
    fallbacks: Sequence[Runnable[Input, Output]],
    *,
    exceptions_to_handle: tuple[type[BaseException], ...] = (Exception,),
    exception_key: str | None = None,
) -> RunnableWithFallbacks[Input, Output]
Add fallbacks to a Runnable, returning a new Runnable.

The new Runnable will try the original Runnable, and then each fallback in order, upon failures.

PARAMETER	DESCRIPTION
fallbacks	A sequence of runnables to try if the original Runnable fails.
TYPE: Sequence[Runnable[Input, Output]]

exceptions_to_handle	A tuple of exception types to handle.
TYPE: tuple[type[BaseException], ...]DEFAULT: (Exception,)

exception_key	If string is specified then handled exceptions will be passed to fallbacks as part of the input under the specified key.
If None, exceptions will not be passed to fallbacks.

If used, the base Runnable and its fallbacks must accept a dictionary as input.

TYPE: str | NoneDEFAULT: None

RETURNS	DESCRIPTION
RunnableWithFallbacks[Input, Output]	A new Runnable that will try the original Runnable, and then each Fallback in order, upon failures.
Example

from typing import Iterator

from langchain_core.runnables import RunnableGenerator


def _generate_immediate_error(input: Iterator) -> Iterator[str]:
    raise ValueError()
    yield ""


def _generate(input: Iterator) -> Iterator[str]:
    yield from "foo bar"


runnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(
    [RunnableGenerator(_generate)]
)
print("".join(runnable.stream({})))  # foo bar
PARAMETER	DESCRIPTION
fallbacks	A sequence of runnables to try if the original Runnable fails.
TYPE: Sequence[Runnable[Input, Output]]

exceptions_to_handle	A tuple of exception types to handle.
TYPE: tuple[type[BaseException], ...]DEFAULT: (Exception,)

exception_key	If string is specified then handled exceptions will be passed to fallbacks as part of the input under the specified key.
If None, exceptions will not be passed to fallbacks.

If used, the base Runnable and its fallbacks must accept a dictionary as input.

TYPE: str | NoneDEFAULT: None

RETURNS	DESCRIPTION
RunnableWithFallbacks[Input, Output]	A new Runnable that will try the original Runnable, and then each Fallback in order, upon failures.
 as_tool ¬∂

as_tool(
    args_schema: type[BaseModel] | None = None,
    *,
    name: str | None = None,
    description: str | None = None,
    arg_types: dict[str, type] | None = None,
) -> BaseTool
Create a BaseTool from a Runnable.

as_tool will instantiate a BaseTool with a name, description, and args_schema from a Runnable. Where possible, schemas are inferred from runnable.get_input_schema.

Alternatively (e.g., if the Runnable takes a dict as input and the specific dict keys are not typed), the schema can be specified directly with args_schema.

You can also pass arg_types to just specify the required arguments and their types.

PARAMETER	DESCRIPTION
args_schema	The schema for the tool.
TYPE: type[BaseModel] | NoneDEFAULT: None

name	The name of the tool.
TYPE: str | NoneDEFAULT: None

description	The description of the tool.
TYPE: str | NoneDEFAULT: None

arg_types	A dictionary of argument names to types.
TYPE: dict[str, type] | NoneDEFAULT: None

RETURNS	DESCRIPTION
BaseTool	A BaseTool instance.
TypedDict input


from typing_extensions import TypedDict
from langchain_core.runnables import RunnableLambda


class Args(TypedDict):
    a: int
    b: list[int]


def f(x: Args) -> str:
    return str(x["a"] * max(x["b"]))


runnable = RunnableLambda(f)
as_tool = runnable.as_tool()
as_tool.invoke({"a": 3, "b": [1, 2]})
dict input, specifying schema via args_schema


from typing import Any
from pydantic import BaseModel, Field
from langchain_core.runnables import RunnableLambda

def f(x: dict[str, Any]) -> str:
    return str(x["a"] * max(x["b"]))

class FSchema(BaseModel):
    """Apply a function to an integer and list of integers."""

    a: int = Field(..., description="Integer")
    b: list[int] = Field(..., description="List of ints")

runnable = RunnableLambda(f)
as_tool = runnable.as_tool(FSchema)
as_tool.invoke({"a": 3, "b": [1, 2]})
dict input, specifying schema via arg_types


from typing import Any
from langchain_core.runnables import RunnableLambda


def f(x: dict[str, Any]) -> str:
    return str(x["a"] * max(x["b"]))


runnable = RunnableLambda(f)
as_tool = runnable.as_tool(arg_types={"a": int, "b": list[int]})
as_tool.invoke({"a": 3, "b": [1, 2]})
str input


from langchain_core.runnables import RunnableLambda


def f(x: str) -> str:
    return x + "a"


def g(x: str) -> str:
    return x + "z"


runnable = RunnableLambda(f) | g
as_tool = runnable.as_tool()
as_tool.invoke("b")
 is_lc_serializable classmethod ¬∂

is_lc_serializable() -> bool
Is this class serializable?

By design, even if a class inherits from Serializable, it is not serializable by default. This is to prevent accidental serialization of objects that should not be serialized.

RETURNS	DESCRIPTION
bool	Whether the class is serializable. Default is False.
 get_lc_namespace classmethod ¬∂

get_lc_namespace() -> list[str]
Get the namespace of the LangChain object.

For example, if the class is langchain.llms.openai.OpenAI, then the namespace is ["langchain", "llms", "openai"]

RETURNS	DESCRIPTION
list[str]	The namespace.
 lc_id classmethod ¬∂

lc_id() -> list[str]
Return a unique identifier for this class for serialization purposes.

The unique identifier is a list of strings that describes the path to the object.

For example, for the class langchain.llms.openai.OpenAI, the id is ["langchain", "llms", "openai", "OpenAI"].

 to_json ¬∂

to_json() -> SerializedConstructor | SerializedNotImplemented
Serialize the Runnable to JSON.

RETURNS	DESCRIPTION
SerializedConstructor | SerializedNotImplemented	A JSON-serializable representation of the Runnable.
 to_json_not_implemented ¬∂

to_json_not_implemented() -> SerializedNotImplemented
Serialize a "not implemented" object.

RETURNS	DESCRIPTION
SerializedNotImplemented	SerializedNotImplemented.
 configurable_fields ¬∂

configurable_fields(
    **kwargs: AnyConfigurableField,
) -> RunnableSerializable[Input, Output]
Configure particular Runnable fields at runtime.

PARAMETER	DESCRIPTION
**kwargs	A dictionary of ConfigurableField instances to configure.
TYPE: AnyConfigurableFieldDEFAULT: {}

RAISES	DESCRIPTION
ValueError	If a configuration key is not found in the Runnable.
RETURNS	DESCRIPTION
RunnableSerializable[Input, Output]	A new Runnable with the fields configured.
Example


from langchain_core.runnables import ConfigurableField
from langchain_openai import ChatOpenAI

model = ChatOpenAI(max_tokens=20).configurable_fields(
    max_tokens=ConfigurableField(
        id="output_token_number",
        name="Max tokens in the output",
        description="The maximum number of tokens in the output",
    )
)

# max_tokens = 20
print(
    "max_tokens_20: ", model.invoke("tell me something about chess").content
)

# max_tokens = 200
print(
    "max_tokens_200: ",
    model.with_config(configurable={"output_token_number": 200})
    .invoke("tell me something about chess")
    .content,
)
 configurable_alternatives ¬∂

configurable_alternatives(
    which: ConfigurableField,
    *,
    default_key: str = "default",
    prefix_keys: bool = False,
    **kwargs: Runnable[Input, Output] | Callable[[], Runnable[Input, Output]],
) -> RunnableSerializable[Input, Output]
Configure alternatives for Runnable objects that can be set at runtime.

PARAMETER	DESCRIPTION
which	The ConfigurableField instance that will be used to select the alternative.
TYPE: ConfigurableField

default_key	The default key to use if no alternative is selected.
TYPE: strDEFAULT: 'default'

prefix_keys	Whether to prefix the keys with the ConfigurableField id.
TYPE: boolDEFAULT: False

**kwargs	A dictionary of keys to Runnable instances or callables that return Runnable instances.
TYPE: Runnable[Input, Output] | Callable[[], Runnable[Input, Output]]DEFAULT: {}

RETURNS	DESCRIPTION
RunnableSerializable[Input, Output]	A new Runnable with the alternatives configured.
Example


from langchain_anthropic import ChatAnthropic
from langchain_core.runnables.utils import ConfigurableField
from langchain_openai import ChatOpenAI

model = ChatAnthropic(
    model_name="claude-sonnet-4-5-20250929"
).configurable_alternatives(
    ConfigurableField(id="llm"),
    default_key="anthropic",
    openai=ChatOpenAI(),
)

# uses the default model ChatAnthropic
print(model.invoke("which organization created you?").content)

# uses ChatOpenAI
print(
    model.with_config(configurable={"llm": "openai"})
    .invoke("which organization created you?")
    .content
)
 set_verbose ¬∂

set_verbose(verbose: bool | None) -> bool
If verbose is None, set it.

This allows users to pass in None as verbose to access the global setting.

PARAMETER	DESCRIPTION
verbose	The verbosity setting to use.
TYPE: bool | None

RETURNS	DESCRIPTION
bool	The verbosity setting to use.
 generate_prompt ¬∂

generate_prompt(
    prompts: list[PromptValue],
    stop: list[str] | None = None,
    callbacks: Callbacks | list[Callbacks] | None = None,
    **kwargs: Any,
) -> LLMResult
Pass a sequence of prompts to the model and return model generations.

This method should make use of batched calls for models that expose a batched API.

Use this method when you want to:

Take advantage of batched calls,
Need more output from the model than just the top generated value,
Are building chains that are agnostic to the underlying language model type (e.g., pure text completion models vs chat models).
PARAMETER	DESCRIPTION
prompts	List of PromptValue objects.
A PromptValue is an object that can be converted to match the format of any language model (string for pure text generation models and BaseMessage objects for chat models).

TYPE: list[PromptValue]

stop	Stop words to use when generating.
Model output is cut off at the first occurrence of any of these substrings.

TYPE: list[str] | NoneDEFAULT: None

callbacks	Callbacks to pass through.
Used for executing additional functionality, such as logging or streaming, throughout generation.

TYPE: CallbacksDEFAULT: None

**kwargs	Arbitrary additional keyword arguments.
These are usually passed to the model provider API call.

TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
LLMResult	An LLMResult, which contains a list of candidate Generation objects for each input prompt and additional model provider-specific output.
 agenerate_prompt async ¬∂

agenerate_prompt(
    prompts: list[PromptValue],
    stop: list[str] | None = None,
    callbacks: Callbacks | list[Callbacks] | None = None,
    **kwargs: Any,
) -> LLMResult
Asynchronously pass a sequence of prompts and return model generations.

This method should make use of batched calls for models that expose a batched API.

Use this method when you want to:

Take advantage of batched calls,
Need more output from the model than just the top generated value,
Are building chains that are agnostic to the underlying language model type (e.g., pure text completion models vs chat models).
PARAMETER	DESCRIPTION
prompts	List of PromptValue objects.
A PromptValue is an object that can be converted to match the format of any language model (string for pure text generation models and BaseMessage objects for chat models).

TYPE: list[PromptValue]

stop	Stop words to use when generating.
Model output is cut off at the first occurrence of any of these substrings.

TYPE: list[str] | NoneDEFAULT: None

callbacks	Callbacks to pass through.
Used for executing additional functionality, such as logging or streaming, throughout generation.

TYPE: CallbacksDEFAULT: None

**kwargs	Arbitrary additional keyword arguments.
These are usually passed to the model provider API call.

TYPE: AnyDEFAULT: {}

RETURNS	DESCRIPTION
LLMResult	An LLMResult, which contains a list of candidate Generation objects for each input prompt and additional model provider-specific output.
 with_structured_output ¬∂

with_structured_output(
    schema: dict | type, **kwargs: Any
) -> Runnable[LanguageModelInput, dict | BaseModel]
Not implemented on this class.

 get_token_ids ¬∂

get_token_ids(text: str) -> list[int]
Return the ordered IDs of the tokens in a text.

PARAMETER	DESCRIPTION
text	The string input to tokenize.
TYPE: str

RETURNS	DESCRIPTION
list[int]	A list of IDs corresponding to the tokens in the text, in order they occur in the text.
 get_num_tokens_from_messages ¬∂

get_num_tokens_from_messages(
    messages: list[BaseMessage], tools: Sequence | None = None
) -> int
Get the number of tokens in the messages.

Useful for checking if an input fits in a model's context window.

This should be overridden by model-specific implementations to provide accurate token counts via model-specific tokenizers.

Note

The base implementation of get_num_tokens_from_messages ignores tool schemas.
The base implementation of get_num_tokens_from_messages adds additional prefixes to messages in represent user roles, which will add to the overall token count. Model-specific implementations may choose to handle this differently.
PARAMETER	DESCRIPTION
messages	The message inputs to tokenize.
TYPE: list[BaseMessage]

tools	If provided, sequence of dict, BaseModel, function, or BaseTool objects to be converted to tool schemas.
TYPE: Sequence | NoneDEFAULT: None

RETURNS	DESCRIPTION
int	The sum of the number of tokens across the messages.
 generate ¬∂

generate(
    prompts: list[str],
    stop: list[str] | None = None,
    callbacks: Callbacks | list[Callbacks] | None = None,
    *,
    tags: list[str] | list[list[str]] | None = None,
    metadata: dict[str, Any] | list[dict[str, Any]] | None = None,
    run_name: str | list[str] | None = None,
    run_id: UUID | list[UUID | None] | None = None,
    **kwargs: Any,
) -> LLMResult
Pass a sequence of prompts to a model and return generations.

This method should make use of batched calls for models that expose a batched API.

Use this method when you want to:

Take advantage of batched calls,
Need more output from the model than just the top generated value,
Are building chains that are agnostic to the underlying language model type (e.g., pure text completion models vs chat models).
PARAMETER	DESCRIPTION
prompts	List of string prompts.
TYPE: list[str]

stop	Stop words to use when generating.
Model output is cut off at the first occurrence of any of these substrings.

TYPE: list[str] | NoneDEFAULT: None

callbacks	Callbacks to pass through.
Used for executing additional functionality, such as logging or streaming, throughout generation.

TYPE: Callbacks | list[Callbacks] | NoneDEFAULT: None

tags	List of tags to associate with each prompt. If provided, the length of the list must match the length of the prompts list.
TYPE: list[str] | list[list[str]] | NoneDEFAULT: None

metadata	List of metadata dictionaries to associate with each prompt. If provided, the length of the list must match the length of the prompts list.
TYPE: dict[str, Any] | list[dict[str, Any]] | NoneDEFAULT: None

run_name	List of run names to associate with each prompt. If provided, the length of the list must match the length of the prompts list.
TYPE: str | list[str] | NoneDEFAULT: None

run_id	List of run IDs to associate with each prompt. If provided, the length of the list must match the length of the prompts list.
TYPE: UUID | list[UUID | None] | NoneDEFAULT: None

**kwargs	Arbitrary additional keyword arguments.
These are usually passed to the model provider API call.

TYPE: AnyDEFAULT: {}

RAISES	DESCRIPTION
ValueError	If prompts is not a list.
ValueError	If the length of callbacks, tags, metadata, or run_name (if provided) does not match the length of prompts.
RETURNS	DESCRIPTION
LLMResult	An LLMResult, which contains a list of candidate Generations for each input prompt and additional model provider-specific output.
 agenerate async ¬∂

agenerate(
    prompts: list[str],
    stop: list[str] | None = None,
    callbacks: Callbacks | list[Callbacks] | None = None,
    *,
    tags: list[str] | list[list[str]] | None = None,
    metadata: dict[str, Any] | list[dict[str, Any]] | None = None,
    run_name: str | list[str] | None = None,
    run_id: UUID | list[UUID | None] | None = None,
    **kwargs: Any,
) -> LLMResult
Asynchronously pass a sequence of prompts to a model and return generations.

This method should make use of batched calls for models that expose a batched API.

Use this method when you want to:

Take advantage of batched calls,
Need more output from the model than just the top generated value,
Are building chains that are agnostic to the underlying language model type (e.g., pure text completion models vs chat models).
PARAMETER	DESCRIPTION
prompts	List of string prompts.
TYPE: list[str]

stop	Stop words to use when generating.
Model output is cut off at the first occurrence of any of these substrings.

TYPE: list[str] | NoneDEFAULT: None

callbacks	Callbacks to pass through.
Used for executing additional functionality, such as logging or streaming, throughout generation.

TYPE: Callbacks | list[Callbacks] | NoneDEFAULT: None

tags	List of tags to associate with each prompt. If provided, the length of the list must match the length of the prompts list.
TYPE: list[str] | list[list[str]] | NoneDEFAULT: None

metadata	List of metadata dictionaries to associate with each prompt. If provided, the length of the list must match the length of the prompts list.
TYPE: dict[str, Any] | list[dict[str, Any]] | NoneDEFAULT: None

run_name	List of run names to associate with each prompt. If provided, the length of the list must match the length of the prompts list.
TYPE: str | list[str] | NoneDEFAULT: None

run_id	List of run IDs to associate with each prompt. If provided, the length of the list must match the length of the prompts list.
TYPE: UUID | list[UUID | None] | NoneDEFAULT: None

**kwargs	Arbitrary additional keyword arguments.
These are usually passed to the model provider API call.

TYPE: AnyDEFAULT: {}

RAISES	DESCRIPTION
ValueError	If the length of callbacks, tags, metadata, or run_name (if provided) does not match the length of prompts.
RETURNS	DESCRIPTION
LLMResult	An LLMResult, which contains a list of candidate Generations for each input prompt and additional model provider-specific output.
 __str__ ¬∂

__str__() -> str
Return a string representation of the object for printing.

 dict ¬∂

dict(**kwargs: Any) -> dict
Return a dictionary of the LLM.

 save ¬∂

save(file_path: Path | str) -> None
Save the LLM.

PARAMETER	DESCRIPTION
file_path	Path to file to save the LLM to.
TYPE: Path | str

RAISES	DESCRIPTION
ValueError	If the file path is not a string or Path object.
Example

llm.save(file_path="path/llm.yaml")
 __init__ ¬∂

__init__(**kwargs: Any) -> None
Needed for arg validation.

 validate_environment ¬∂

validate_environment() -> Self
Validates params and passes them to google-generativeai package.

 get_num_tokens ¬∂

get_num_tokens(text: str) -> int
Get the number of tokens present in the text.

Useful for checking if an input will fit in a model's context window.

PARAMETER	DESCRIPTION
text	The string input to tokenize.
TYPE: str

RETURNS	DESCRIPTION
int	The integer number of tokens in the text.
.........